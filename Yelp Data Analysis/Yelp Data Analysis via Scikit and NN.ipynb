{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini_Project_2_Palak&Pallavi",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "P5WolomW4bei",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mini Project 2\n",
        "\n",
        "## Team Members : \n",
        "## Palak Patel\n",
        "## Pallavi Yadkikar"
      ]
    },
    {
      "metadata": {
        "id": "rjqbnlHU4bek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting ready with Dataframes"
      ]
    },
    {
      "metadata": {
        "id": "BCjFa9Mh4mR3",
        "colab_type": "code",
        "outputId": "99ef22a3-7ac8-413c-bb32-234c1b94247a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N5imkmW74sHa",
        "colab_type": "code",
        "outputId": "0ebd2ddd-3e7c-478b-dc11-76b8d5bbebe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 215\n",
            " best_weights_NN.hdf5\n",
            " best_weights_NNSmooth.hdf5\n",
            " best_weights_regression1.hdf5\n",
            " best_weights_regression.hdf5\n",
            " business.json\n",
            " checkin.json\n",
            " Lab10_regularization.ipynb\n",
            " Lab5_tensor_flow.ipynb\n",
            " Lab6_evaluating_neural_networks.ipynb\n",
            " Lab7_model_visualization.ipynb\n",
            " Lab8_parameter_tuning.ipynb\n",
            " Lab9_cnn.ipynb\n",
            " Mini_project_1_Additional.ipynb\n",
            " Mini_Project_1_Palak_Pallavi.ipynb\n",
            " Mini_Project_2.ipynb\n",
            "'Mini_Project_2_Palak&Pallavi.ipynb'\n",
            " photo.json\n",
            " Project_2.ipynb\n",
            " Project_2_mardav_ls.ipynb\n",
            " review.json\n",
            "'StarRatingPrediction_TensorFlow (1).ipynb'\n",
            " tip.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rlZJD6Mh4bel",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert .json files into pandas dataframes\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "outfile = open(\"reviews.tsv\", 'w')\n",
        "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
        "sfile.writerow(['business_id','stars', 'text'])\n",
        "with open('/content/drive/My Drive/Colab Notebooks/review.json', encoding=\"utf-8\") as f:\n",
        "\n",
        "    for line in f:\n",
        "\n",
        "        row = json.loads(line)\n",
        "\n",
        "        # some special char must be encoded in 'utf-8'\n",
        "\n",
        "        sfile.writerow([row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
        "        \n",
        "outfile.close()\n",
        "df_r= pd.read_csv('reviews.tsv', delimiter =\"\\t\", encoding=\"utf-8\", nrows=500000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddg-uDem4lBy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agX6B0vu4beq",
        "colab_type": "code",
        "outputId": "eb11c163-1bb8-4a2f-d821-4dfd75b1d413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df_r.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'Total bill for this horrible service? Over $...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
              "      <td>5.0</td>\n",
              "      <td>b\"I *adore* Travis at the Hard Rock's new Kell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
              "      <td>5.0</td>\n",
              "      <td>b\"I have to say that this office really has it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
              "      <td>5.0</td>\n",
              "      <td>b\"Went in for a lunch. Steak sandwich was deli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'Today was my second out of three sessions I ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  \\\n",
              "0  ujmEBvifdJM6h6RLv4wQIg    1.0   \n",
              "1  NZnhc2sEQy3RmzKTZnqtwQ    5.0   \n",
              "2  WTqjgwHlXbSFevF32_DJVw    5.0   \n",
              "3  ikCg8xy5JIg_NGPx-MSIDA    5.0   \n",
              "4  b1b1eb3uo-w561D0ZfCEiQ    1.0   \n",
              "\n",
              "                                                text  \n",
              "0  b'Total bill for this horrible service? Over $...  \n",
              "1  b\"I *adore* Travis at the Hard Rock's new Kell...  \n",
              "2  b\"I have to say that this office really has it...  \n",
              "3  b\"Went in for a lunch. Steak sandwich was deli...  \n",
              "4  b'Today was my second out of three sessions I ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "OmeS4t1x4bey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "outfile_1 = open(\"business_1.tsv\", 'w')\n",
        "sfile_1 = csv.writer(outfile_1, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
        "sfile_1.writerow(['business_id','stars', 'review_count'])\n",
        "with open('/content/drive/My Drive/Colab Notebooks/business.json', encoding=\"utf-8\") as f:\n",
        "\n",
        "    for line in f:\n",
        "\n",
        "        row = json.loads(line)\n",
        "\n",
        "        # some special char must be encoded in 'utf-8'\n",
        "\n",
        "        sfile_1.writerow([row['business_id'], row['stars'], row['review_count']])\n",
        "        \n",
        "outfile_1.close()\n",
        "df_b = pd.read_csv('business_1.tsv', delimiter =\"\\t\", encoding=\"utf-8\", nrows=500000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9MY9s1pE4be1",
        "colab_type": "code",
        "outputId": "1b28ce8b-9132-4704-b1b1-186984761e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df_b.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  review_count\n",
              "0  1SWheh84yJXfytovILXOAQ    3.0             5\n",
              "1  QXAEGFB4oINsVuTFxEYKFQ    2.5           128\n",
              "2  gnKjwL_1w79qoiV3IC_xQQ    4.0           170\n",
              "3  xvX2CttrVhyG2z1dFg_0xw    5.0             3\n",
              "4  HhyxOkGAM07SRYtlQ4wMFQ    4.0             4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "JVXqE05J4be6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_c = df_b[~df_b.business_id.isin(['YSilq0Was9b4Q7oId_gUUw', 'Pah1N0Di1WA3wsI5TYkY7w', 'W6q7CANl3UbQa5hGCYU5OQ'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nIatqMJU4be-",
        "colab_type": "code",
        "outputId": "fe81dd32-eccf-49bf-fa3b-b28d30efa2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "x1 = set(df_r.business_id) \n",
        "y1 = set(df_c.business_id)\n",
        "\n",
        "z1 = y1 - x1\n",
        "print(z1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "R1poVKtj4bfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Group all the reviews by each business and create a new dataframe, where each line is a business with all its reviews. \n",
        "\n",
        "df_review_agg = df_r.groupby('business_id')['text'].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hoiiM-rD4bfG",
        "colab_type": "code",
        "outputId": "a5bfc4ca-6af3-4a8c-919a-715d13bba21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_review_agg.head()\n",
        "df_review_agg.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18983,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "uqzB9ygC4bfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_ready_for_sklearn = pd.DataFrame({'business_id': df_review_agg.index, 'all_reviews': df_review_agg.values})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVKc7BIa4bfO",
        "colab_type": "code",
        "outputId": "bdeae573-b125-48d2-e234-34ecad580140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df_ready_for_sklearn.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>business_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b\"Delicious! \\n\\nCame from Chicago this evenin...</td>\n",
              "      <td>--Gc998IMjLn8yr-HTzGUg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b\"Had to get my wing fix, I like dry rubs on w...</td>\n",
              "      <td>--I7YYLada0tSLkORTHb5Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b\"Let's start with the bad. REALLY slow. I din...</td>\n",
              "      <td>--U98MNlDym2cLn36BBPgQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'I love this place. Its always quiet and rela...</td>\n",
              "      <td>--j-kaNMCo1-DYzddCsA5Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b\"Awesome, fantastic, friendly---I honestly ca...</td>\n",
              "      <td>--wIGbLEhlpl_UeAIyDmZQ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         all_reviews             business_id\n",
              "0  b\"Delicious! \\n\\nCame from Chicago this evenin...  --Gc998IMjLn8yr-HTzGUg\n",
              "1  b\"Had to get my wing fix, I like dry rubs on w...  --I7YYLada0tSLkORTHb5Q\n",
              "2  b\"Let's start with the bad. REALLY slow. I din...  --U98MNlDym2cLn36BBPgQ\n",
              "3  b'I love this place. Its always quiet and rela...  --j-kaNMCo1-DYzddCsA5Q\n",
              "4  b\"Awesome, fantastic, friendly---I honestly ca...  --wIGbLEhlpl_UeAIyDmZQ"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "metadata": {
        "id": "aghBTIkA4bfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Merge Dataframes\n",
        "\n",
        "df_merge = pd.merge(df_c,df_ready_for_sklearn, on =\"business_id\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYw3Ha3L4bfX",
        "colab_type": "code",
        "outputId": "de59681a-a912-4130-dde9-1e3f0bc97d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df_merge.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "      <td>b\"Don't go here if you expect consistent or re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>b'My girlfriend and I went for dinner at Emera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>b'Husband was craving Chicken Teriyaki &amp; gyoza...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "      <td>b'The associates at this agency are absolutely...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>b\"Great customer service and great job every t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  review_count  \\\n",
              "0  1SWheh84yJXfytovILXOAQ    3.0             5   \n",
              "1  QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2  gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "3  xvX2CttrVhyG2z1dFg_0xw    5.0             3   \n",
              "4  HhyxOkGAM07SRYtlQ4wMFQ    4.0             4   \n",
              "\n",
              "                                         all_reviews  \n",
              "0  b\"Don't go here if you expect consistent or re...  \n",
              "1  b'My girlfriend and I went for dinner at Emera...  \n",
              "2  b'Husband was craving Chicken Teriyaki & gyoza...  \n",
              "3  b'The associates at this agency are absolutely...  \n",
              "4  b\"Great customer service and great job every t...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "metadata": {
        "id": "zAwJrPhD4bfe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Z Score "
      ]
    },
    {
      "metadata": {
        "id": "c_0-80tn4bfg",
        "colab_type": "code",
        "outputId": "3306eb8d-be09-433c-f8a4-311d8926aaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "#Calculate Z-Score for ‘review_count’.\n",
        "\n",
        "from scipy.stats import zscore\n",
        "\n",
        "df_merge['normal_review_count'] = zscore(df_merge['review_count'])\n",
        "df_merge[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>normal_review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "      <td>b\"Don't go here if you expect consistent or re...</td>\n",
              "      <td>-0.265740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>b'My girlfriend and I went for dinner at Emera...</td>\n",
              "      <td>0.910968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>b'Husband was craving Chicken Teriyaki &amp; gyoza...</td>\n",
              "      <td>1.312771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "      <td>b'The associates at this agency are absolutely...</td>\n",
              "      <td>-0.284874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>b\"Great customer service and great job every t...</td>\n",
              "      <td>-0.275307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>68dUKd8_8liJ7in4aWOSEA</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3</td>\n",
              "      <td>b'horrible. do not go.  staff is dishonest and...</td>\n",
              "      <td>-0.284874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5JucpCfHZltJh5r1JabjDg</td>\n",
              "      <td>3.5</td>\n",
              "      <td>7</td>\n",
              "      <td>b'OK place- I\\'ve gone for a couple of years a...</td>\n",
              "      <td>-0.246607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gbQN7vr_caG_A1ugSmGhWg</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3</td>\n",
              "      <td>b'My stylist, Maria, listened to me carefully ...</td>\n",
              "      <td>-0.284874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Y6iyemLX_oylRpnr38vgMA</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8</td>\n",
              "      <td>b'My nail girl is renting station at this plac...</td>\n",
              "      <td>-0.237040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4GBVPIYRvzGh4K4TkRQ_rw</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8</td>\n",
              "      <td>b\"I've always had an excellent experience at O...</td>\n",
              "      <td>-0.237040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  review_count  \\\n",
              "0  1SWheh84yJXfytovILXOAQ    3.0             5   \n",
              "1  QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2  gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "3  xvX2CttrVhyG2z1dFg_0xw    5.0             3   \n",
              "4  HhyxOkGAM07SRYtlQ4wMFQ    4.0             4   \n",
              "5  68dUKd8_8liJ7in4aWOSEA    2.5             3   \n",
              "6  5JucpCfHZltJh5r1JabjDg    3.5             7   \n",
              "7  gbQN7vr_caG_A1ugSmGhWg    3.5             3   \n",
              "8  Y6iyemLX_oylRpnr38vgMA    5.0             8   \n",
              "9  4GBVPIYRvzGh4K4TkRQ_rw    4.5             8   \n",
              "\n",
              "                                         all_reviews  normal_review_count  \n",
              "0  b\"Don't go here if you expect consistent or re...            -0.265740  \n",
              "1  b'My girlfriend and I went for dinner at Emera...             0.910968  \n",
              "2  b'Husband was craving Chicken Teriyaki & gyoza...             1.312771  \n",
              "3  b'The associates at this agency are absolutely...            -0.284874  \n",
              "4  b\"Great customer service and great job every t...            -0.275307  \n",
              "5  b'horrible. do not go.  staff is dishonest and...            -0.284874  \n",
              "6  b'OK place- I\\'ve gone for a couple of years a...            -0.246607  \n",
              "7  b'My stylist, Maria, listened to me carefully ...            -0.284874  \n",
              "8  b'My nail girl is renting station at this plac...            -0.237040  \n",
              "9  b\"I've always had an excellent experience at O...            -0.237040  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "metadata": {
        "id": "rX5_sTIs4bfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  MinMax for MNB"
      ]
    },
    {
      "metadata": {
        "id": "3jCJ9XK94bfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mini_count= df_merge['review_count'].min()\n",
        "maxi_count= df_merge['review_count'].max()\n",
        "df_merge['normal_min_max_review_count'] = ((df_merge['review_count']-mini_count)/(maxi_count-mini_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upZxytDg4bfr",
        "colab_type": "code",
        "outputId": "d3d63b2d-4f95-48f0-edba-d3ffbf4bfe0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "df_merge[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>normal_review_count</th>\n",
              "      <th>normal_min_max_review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "      <td>b\"Don't go here if you expect consistent or re...</td>\n",
              "      <td>-0.265740</td>\n",
              "      <td>0.000473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>b'My girlfriend and I went for dinner at Emera...</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.029593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>b'Husband was craving Chicken Teriyaki &amp; gyoza...</td>\n",
              "      <td>1.312771</td>\n",
              "      <td>0.039536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "      <td>b'The associates at this agency are absolutely...</td>\n",
              "      <td>-0.284874</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>b\"Great customer service and great job every t...</td>\n",
              "      <td>-0.275307</td>\n",
              "      <td>0.000237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>68dUKd8_8liJ7in4aWOSEA</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3</td>\n",
              "      <td>b'horrible. do not go.  staff is dishonest and...</td>\n",
              "      <td>-0.284874</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5JucpCfHZltJh5r1JabjDg</td>\n",
              "      <td>3.5</td>\n",
              "      <td>7</td>\n",
              "      <td>b'OK place- I\\'ve gone for a couple of years a...</td>\n",
              "      <td>-0.246607</td>\n",
              "      <td>0.000947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gbQN7vr_caG_A1ugSmGhWg</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3</td>\n",
              "      <td>b'My stylist, Maria, listened to me carefully ...</td>\n",
              "      <td>-0.284874</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Y6iyemLX_oylRpnr38vgMA</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8</td>\n",
              "      <td>b'My nail girl is renting station at this plac...</td>\n",
              "      <td>-0.237040</td>\n",
              "      <td>0.001184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4GBVPIYRvzGh4K4TkRQ_rw</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8</td>\n",
              "      <td>b\"I've always had an excellent experience at O...</td>\n",
              "      <td>-0.237040</td>\n",
              "      <td>0.001184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id  stars  review_count  \\\n",
              "0  1SWheh84yJXfytovILXOAQ    3.0             5   \n",
              "1  QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2  gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "3  xvX2CttrVhyG2z1dFg_0xw    5.0             3   \n",
              "4  HhyxOkGAM07SRYtlQ4wMFQ    4.0             4   \n",
              "5  68dUKd8_8liJ7in4aWOSEA    2.5             3   \n",
              "6  5JucpCfHZltJh5r1JabjDg    3.5             7   \n",
              "7  gbQN7vr_caG_A1ugSmGhWg    3.5             3   \n",
              "8  Y6iyemLX_oylRpnr38vgMA    5.0             8   \n",
              "9  4GBVPIYRvzGh4K4TkRQ_rw    4.5             8   \n",
              "\n",
              "                                         all_reviews  normal_review_count  \\\n",
              "0  b\"Don't go here if you expect consistent or re...            -0.265740   \n",
              "1  b'My girlfriend and I went for dinner at Emera...             0.910968   \n",
              "2  b'Husband was craving Chicken Teriyaki & gyoza...             1.312771   \n",
              "3  b'The associates at this agency are absolutely...            -0.284874   \n",
              "4  b\"Great customer service and great job every t...            -0.275307   \n",
              "5  b'horrible. do not go.  staff is dishonest and...            -0.284874   \n",
              "6  b'OK place- I\\'ve gone for a couple of years a...            -0.246607   \n",
              "7  b'My stylist, Maria, listened to me carefully ...            -0.284874   \n",
              "8  b'My nail girl is renting station at this plac...            -0.237040   \n",
              "9  b\"I've always had an excellent experience at O...            -0.237040   \n",
              "\n",
              "   normal_min_max_review_count  \n",
              "0                     0.000473  \n",
              "1                     0.029593  \n",
              "2                     0.039536  \n",
              "3                     0.000000  \n",
              "4                     0.000237  \n",
              "5                     0.000000  \n",
              "6                     0.000947  \n",
              "7                     0.000000  \n",
              "8                     0.001184  \n",
              "9                     0.001184  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "metadata": {
        "id": "zbFzDZ464bfx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_merge.all_reviews = df_merge.all_reviews.apply(lambda x: x.lower()) #convert text to lower case"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RemTpaUO4bf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_merge.all_reviews = df_merge.all_reviews.replace(r'\\\\n','', regex=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64g4_kBH4bf7",
        "colab_type": "code",
        "outputId": "8c552c5f-671e-41a9-de29-a119d25cc1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_merge[:10]\n",
        "df_merge.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18983, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "4G3lPF1q4bf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "df_combine = df_merge[(df_merge.review_count>50) & (df_merge.review_count<3000)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jLWXVRR4bgC",
        "colab_type": "code",
        "outputId": "56806101-3dac-45da-d95a-54faaf23e850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2583, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "metadata": {
        "id": "E4sjkeTA4bgI",
        "colab_type": "code",
        "outputId": "4517d96e-86f4-4b3d-d287-93daf361248e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine['all_reviews'] = df_combine['all_reviews'].str.replace('[^\\w\\s]','') #removing punctuation "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oboBwBQg4bgM",
        "colab_type": "code",
        "outputId": "5f5bf604-4abe-4afb-d222-773b9fa8688e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>normal_review_count</th>\n",
              "      <th>normal_min_max_review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>bmy girlfriend and i went for dinner at emeral...</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.029593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>bhusband was craving chicken teriyaki  gyoza s...</td>\n",
              "      <td>1.312771</td>\n",
              "      <td>0.039536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>KWywu2tTEPWmR9JnBc0WyQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>107</td>\n",
              "      <td>bits awesome herethe guys are soooo hot and ni...</td>\n",
              "      <td>0.710067</td>\n",
              "      <td>0.024621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>4.5</td>\n",
              "      <td>184</td>\n",
              "      <td>bwe found out about this gem from the mans cow...</td>\n",
              "      <td>1.446705</td>\n",
              "      <td>0.042850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>iojTeSaoPuxm4WeCzDUA6w</td>\n",
              "      <td>4.5</td>\n",
              "      <td>52</td>\n",
              "      <td>bi got into a parking lot accident with my 201...</td>\n",
              "      <td>0.183896</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NDuUMJfrWk52RA-H-OtrpA</td>\n",
              "      <td>3.0</td>\n",
              "      <td>57</td>\n",
              "      <td>bpretty solid vegan spot had a rice bowl there...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>GWO87Y-IqL54_Ijx6hTYAQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>57</td>\n",
              "      <td>bim not sure how you can rate a classic ice cr...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>viivlh_KJkiDVxhIdZnV8Q</td>\n",
              "      <td>2.5</td>\n",
              "      <td>64</td>\n",
              "      <td>bfirst time here 1152017 to see entombed ad pl...</td>\n",
              "      <td>0.298697</td>\n",
              "      <td>0.014441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>q2KtxnPa6rPSpAlMbg9l8g</td>\n",
              "      <td>4.0</td>\n",
              "      <td>174</td>\n",
              "      <td>bi loved my little cake but the owner is her o...</td>\n",
              "      <td>1.351038</td>\n",
              "      <td>0.040483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>dQj5DLZjeDK3KFysh1SYOQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>242</td>\n",
              "      <td>bit doesnt get more kooky than meatless easter...</td>\n",
              "      <td>2.001576</td>\n",
              "      <td>0.056581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               business_id  stars  review_count  \\\n",
              "1   QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2   gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "22  KWywu2tTEPWmR9JnBc0WyQ    4.0           107   \n",
              "25  tstimHoMcYbkSC4eBA1wEg    4.5           184   \n",
              "28  iojTeSaoPuxm4WeCzDUA6w    4.5            52   \n",
              "29  NDuUMJfrWk52RA-H-OtrpA    3.0            57   \n",
              "47  GWO87Y-IqL54_Ijx6hTYAQ    4.5            57   \n",
              "52  viivlh_KJkiDVxhIdZnV8Q    2.5            64   \n",
              "53  q2KtxnPa6rPSpAlMbg9l8g    4.0           174   \n",
              "66  dQj5DLZjeDK3KFysh1SYOQ    4.5           242   \n",
              "\n",
              "                                          all_reviews  normal_review_count  \\\n",
              "1   bmy girlfriend and i went for dinner at emeral...             0.910968   \n",
              "2   bhusband was craving chicken teriyaki  gyoza s...             1.312771   \n",
              "22  bits awesome herethe guys are soooo hot and ni...             0.710067   \n",
              "25  bwe found out about this gem from the mans cow...             1.446705   \n",
              "28  bi got into a parking lot accident with my 201...             0.183896   \n",
              "29  bpretty solid vegan spot had a rice bowl there...             0.231730   \n",
              "47  bim not sure how you can rate a classic ice cr...             0.231730   \n",
              "52  bfirst time here 1152017 to see entombed ad pl...             0.298697   \n",
              "53  bi loved my little cake but the owner is her o...             1.351038   \n",
              "66  bit doesnt get more kooky than meatless easter...             2.001576   \n",
              "\n",
              "    normal_min_max_review_count  \n",
              "1                      0.029593  \n",
              "2                      0.039536  \n",
              "22                     0.024621  \n",
              "25                     0.042850  \n",
              "28                     0.011600  \n",
              "29                     0.012784  \n",
              "47                     0.012784  \n",
              "52                     0.014441  \n",
              "53                     0.040483  \n",
              "66                     0.056581  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "metadata": {
        "id": "C76SDY_m4bgR",
        "colab_type": "code",
        "outputId": "88666db9-aab3-4e84-9bca-3cba9d008dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine['all_reviews'] = df_combine['all_reviews'].str[1:]  #removing first char \"b\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "KNksD1C24bgY",
        "colab_type": "code",
        "outputId": "5d69ea10-7bd1-4d94-f840-0306999d9f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>normal_review_count</th>\n",
              "      <th>normal_min_max_review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>my girlfriend and i went for dinner at emerald...</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.029593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>husband was craving chicken teriyaki  gyoza so...</td>\n",
              "      <td>1.312771</td>\n",
              "      <td>0.039536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>KWywu2tTEPWmR9JnBc0WyQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>107</td>\n",
              "      <td>its awesome herethe guys are soooo hot and nic...</td>\n",
              "      <td>0.710067</td>\n",
              "      <td>0.024621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>4.5</td>\n",
              "      <td>184</td>\n",
              "      <td>we found out about this gem from the mans cowo...</td>\n",
              "      <td>1.446705</td>\n",
              "      <td>0.042850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>iojTeSaoPuxm4WeCzDUA6w</td>\n",
              "      <td>4.5</td>\n",
              "      <td>52</td>\n",
              "      <td>i got into a parking lot accident with my 2011...</td>\n",
              "      <td>0.183896</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NDuUMJfrWk52RA-H-OtrpA</td>\n",
              "      <td>3.0</td>\n",
              "      <td>57</td>\n",
              "      <td>pretty solid vegan spot had a rice bowl there ...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>GWO87Y-IqL54_Ijx6hTYAQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>57</td>\n",
              "      <td>im not sure how you can rate a classic ice cre...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>viivlh_KJkiDVxhIdZnV8Q</td>\n",
              "      <td>2.5</td>\n",
              "      <td>64</td>\n",
              "      <td>first time here 1152017 to see entombed ad ple...</td>\n",
              "      <td>0.298697</td>\n",
              "      <td>0.014441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>q2KtxnPa6rPSpAlMbg9l8g</td>\n",
              "      <td>4.0</td>\n",
              "      <td>174</td>\n",
              "      <td>i loved my little cake but the owner is her ow...</td>\n",
              "      <td>1.351038</td>\n",
              "      <td>0.040483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>dQj5DLZjeDK3KFysh1SYOQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>242</td>\n",
              "      <td>it doesnt get more kooky than meatless eastern...</td>\n",
              "      <td>2.001576</td>\n",
              "      <td>0.056581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               business_id  stars  review_count  \\\n",
              "1   QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2   gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "22  KWywu2tTEPWmR9JnBc0WyQ    4.0           107   \n",
              "25  tstimHoMcYbkSC4eBA1wEg    4.5           184   \n",
              "28  iojTeSaoPuxm4WeCzDUA6w    4.5            52   \n",
              "29  NDuUMJfrWk52RA-H-OtrpA    3.0            57   \n",
              "47  GWO87Y-IqL54_Ijx6hTYAQ    4.5            57   \n",
              "52  viivlh_KJkiDVxhIdZnV8Q    2.5            64   \n",
              "53  q2KtxnPa6rPSpAlMbg9l8g    4.0           174   \n",
              "66  dQj5DLZjeDK3KFysh1SYOQ    4.5           242   \n",
              "\n",
              "                                          all_reviews  normal_review_count  \\\n",
              "1   my girlfriend and i went for dinner at emerald...             0.910968   \n",
              "2   husband was craving chicken teriyaki  gyoza so...             1.312771   \n",
              "22  its awesome herethe guys are soooo hot and nic...             0.710067   \n",
              "25  we found out about this gem from the mans cowo...             1.446705   \n",
              "28  i got into a parking lot accident with my 2011...             0.183896   \n",
              "29  pretty solid vegan spot had a rice bowl there ...             0.231730   \n",
              "47  im not sure how you can rate a classic ice cre...             0.231730   \n",
              "52  first time here 1152017 to see entombed ad ple...             0.298697   \n",
              "53  i loved my little cake but the owner is her ow...             1.351038   \n",
              "66  it doesnt get more kooky than meatless eastern...             2.001576   \n",
              "\n",
              "    normal_min_max_review_count  \n",
              "1                      0.029593  \n",
              "2                      0.039536  \n",
              "22                     0.024621  \n",
              "25                     0.042850  \n",
              "28                     0.011600  \n",
              "29                     0.012784  \n",
              "47                     0.012784  \n",
              "52                     0.014441  \n",
              "53                     0.040483  \n",
              "66                     0.056581  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "ELv8iM3q4bgb",
        "colab_type": "code",
        "outputId": "f29f2a0c-7ccb-45d9-c9a3-763275fa25eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine['all_reviews'] = df_combine['all_reviews'].str.replace('\\d+', '') #removing digits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2LB1jzXQ4bgh",
        "colab_type": "code",
        "outputId": "f52bbec7-d384-4fef-a7f4-e3953e29c311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "df_combine[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>review_count</th>\n",
              "      <th>all_reviews</th>\n",
              "      <th>normal_review_count</th>\n",
              "      <th>normal_min_max_review_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
              "      <td>2.5</td>\n",
              "      <td>128</td>\n",
              "      <td>my girlfriend and i went for dinner at emerald...</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.029593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>170</td>\n",
              "      <td>husband was craving chicken teriyaki  gyoza so...</td>\n",
              "      <td>1.312771</td>\n",
              "      <td>0.039536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>KWywu2tTEPWmR9JnBc0WyQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>107</td>\n",
              "      <td>its awesome herethe guys are soooo hot and nic...</td>\n",
              "      <td>0.710067</td>\n",
              "      <td>0.024621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>4.5</td>\n",
              "      <td>184</td>\n",
              "      <td>we found out about this gem from the mans cowo...</td>\n",
              "      <td>1.446705</td>\n",
              "      <td>0.042850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>iojTeSaoPuxm4WeCzDUA6w</td>\n",
              "      <td>4.5</td>\n",
              "      <td>52</td>\n",
              "      <td>i got into a parking lot accident with my  cad...</td>\n",
              "      <td>0.183896</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NDuUMJfrWk52RA-H-OtrpA</td>\n",
              "      <td>3.0</td>\n",
              "      <td>57</td>\n",
              "      <td>pretty solid vegan spot had a rice bowl there ...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>GWO87Y-IqL54_Ijx6hTYAQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>57</td>\n",
              "      <td>im not sure how you can rate a classic ice cre...</td>\n",
              "      <td>0.231730</td>\n",
              "      <td>0.012784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>viivlh_KJkiDVxhIdZnV8Q</td>\n",
              "      <td>2.5</td>\n",
              "      <td>64</td>\n",
              "      <td>first time here  to see entombed ad plenty of ...</td>\n",
              "      <td>0.298697</td>\n",
              "      <td>0.014441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>q2KtxnPa6rPSpAlMbg9l8g</td>\n",
              "      <td>4.0</td>\n",
              "      <td>174</td>\n",
              "      <td>i loved my little cake but the owner is her ow...</td>\n",
              "      <td>1.351038</td>\n",
              "      <td>0.040483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>dQj5DLZjeDK3KFysh1SYOQ</td>\n",
              "      <td>4.5</td>\n",
              "      <td>242</td>\n",
              "      <td>it doesnt get more kooky than meatless eastern...</td>\n",
              "      <td>2.001576</td>\n",
              "      <td>0.056581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>9UTpmQ4OhX5jNFUIu7dPPQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>103</td>\n",
              "      <td>the quantity is good and price is better lolll...</td>\n",
              "      <td>0.671800</td>\n",
              "      <td>0.023674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>v-scZMU6jhnmV955RSzGJw</td>\n",
              "      <td>4.5</td>\n",
              "      <td>106</td>\n",
              "      <td>this place was awful i had a salmon roll and a...</td>\n",
              "      <td>0.700500</td>\n",
              "      <td>0.024384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>44YFU284Z3KDEy25QyVoUw</td>\n",
              "      <td>3.5</td>\n",
              "      <td>269</td>\n",
              "      <td>enjoyed nee house immensely no service issues ...</td>\n",
              "      <td>2.259877</td>\n",
              "      <td>0.062973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>f2ZWZPENViL92BrFsIgR6w</td>\n",
              "      <td>4.5</td>\n",
              "      <td>116</td>\n",
              "      <td>it used to be  for a haircut its now  and that...</td>\n",
              "      <td>0.796167</td>\n",
              "      <td>0.026752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>KjicU7uxRt2KDEnO5cgxDQ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>90</td>\n",
              "      <td>this company tried deliver flowers to our home...</td>\n",
              "      <td>0.547432</td>\n",
              "      <td>0.020597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>wUsjANxpknTpsaTjYoo9iA</td>\n",
              "      <td>2.5</td>\n",
              "      <td>52</td>\n",
              "      <td>i needed to update my nail color  i went to ve...</td>\n",
              "      <td>0.183896</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>sKhDrZFCJqfRNylkHrIDsQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>76</td>\n",
              "      <td>this is a cool starbucks  great atmosphere  to...</td>\n",
              "      <td>0.413498</td>\n",
              "      <td>0.017282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>r8764MtYyt8JhxMvrfM_xQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>320</td>\n",
              "      <td>i was really disappointed with my most recent ...</td>\n",
              "      <td>2.747781</td>\n",
              "      <td>0.075047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>L0aSDVHNXCl6sY4cfZQ-5Q</td>\n",
              "      <td>4.0</td>\n",
              "      <td>108</td>\n",
              "      <td>after spending all the money and energy in con...</td>\n",
              "      <td>0.719633</td>\n",
              "      <td>0.024858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>LcIHB4sWUKTnpBtvVx_W5A</td>\n",
              "      <td>4.0</td>\n",
              "      <td>69</td>\n",
              "      <td>these guys are great   if you need reliable tr...</td>\n",
              "      <td>0.346531</td>\n",
              "      <td>0.015625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                business_id  stars  review_count  \\\n",
              "1    QXAEGFB4oINsVuTFxEYKFQ    2.5           128   \n",
              "2    gnKjwL_1w79qoiV3IC_xQQ    4.0           170   \n",
              "22   KWywu2tTEPWmR9JnBc0WyQ    4.0           107   \n",
              "25   tstimHoMcYbkSC4eBA1wEg    4.5           184   \n",
              "28   iojTeSaoPuxm4WeCzDUA6w    4.5            52   \n",
              "29   NDuUMJfrWk52RA-H-OtrpA    3.0            57   \n",
              "47   GWO87Y-IqL54_Ijx6hTYAQ    4.5            57   \n",
              "52   viivlh_KJkiDVxhIdZnV8Q    2.5            64   \n",
              "53   q2KtxnPa6rPSpAlMbg9l8g    4.0           174   \n",
              "66   dQj5DLZjeDK3KFysh1SYOQ    4.5           242   \n",
              "71   9UTpmQ4OhX5jNFUIu7dPPQ    4.0           103   \n",
              "73   v-scZMU6jhnmV955RSzGJw    4.5           106   \n",
              "78   44YFU284Z3KDEy25QyVoUw    3.5           269   \n",
              "99   f2ZWZPENViL92BrFsIgR6w    4.5           116   \n",
              "109  KjicU7uxRt2KDEnO5cgxDQ    3.0            90   \n",
              "123  wUsjANxpknTpsaTjYoo9iA    2.5            52   \n",
              "127  sKhDrZFCJqfRNylkHrIDsQ    4.0            76   \n",
              "139  r8764MtYyt8JhxMvrfM_xQ    4.0           320   \n",
              "140  L0aSDVHNXCl6sY4cfZQ-5Q    4.0           108   \n",
              "144  LcIHB4sWUKTnpBtvVx_W5A    4.0            69   \n",
              "\n",
              "                                           all_reviews  normal_review_count  \\\n",
              "1    my girlfriend and i went for dinner at emerald...             0.910968   \n",
              "2    husband was craving chicken teriyaki  gyoza so...             1.312771   \n",
              "22   its awesome herethe guys are soooo hot and nic...             0.710067   \n",
              "25   we found out about this gem from the mans cowo...             1.446705   \n",
              "28   i got into a parking lot accident with my  cad...             0.183896   \n",
              "29   pretty solid vegan spot had a rice bowl there ...             0.231730   \n",
              "47   im not sure how you can rate a classic ice cre...             0.231730   \n",
              "52   first time here  to see entombed ad plenty of ...             0.298697   \n",
              "53   i loved my little cake but the owner is her ow...             1.351038   \n",
              "66   it doesnt get more kooky than meatless eastern...             2.001576   \n",
              "71   the quantity is good and price is better lolll...             0.671800   \n",
              "73   this place was awful i had a salmon roll and a...             0.700500   \n",
              "78   enjoyed nee house immensely no service issues ...             2.259877   \n",
              "99   it used to be  for a haircut its now  and that...             0.796167   \n",
              "109  this company tried deliver flowers to our home...             0.547432   \n",
              "123  i needed to update my nail color  i went to ve...             0.183896   \n",
              "127  this is a cool starbucks  great atmosphere  to...             0.413498   \n",
              "139  i was really disappointed with my most recent ...             2.747781   \n",
              "140  after spending all the money and energy in con...             0.719633   \n",
              "144  these guys are great   if you need reliable tr...             0.346531   \n",
              "\n",
              "     normal_min_max_review_count  \n",
              "1                       0.029593  \n",
              "2                       0.039536  \n",
              "22                      0.024621  \n",
              "25                      0.042850  \n",
              "28                      0.011600  \n",
              "29                      0.012784  \n",
              "47                      0.012784  \n",
              "52                      0.014441  \n",
              "53                      0.040483  \n",
              "66                      0.056581  \n",
              "71                      0.023674  \n",
              "73                      0.024384  \n",
              "78                      0.062973  \n",
              "99                      0.026752  \n",
              "109                     0.020597  \n",
              "123                     0.011600  \n",
              "127                     0.017282  \n",
              "139                     0.075047  \n",
              "140                     0.024858  \n",
              "144                     0.015625  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "metadata": {
        "id": "YQqc8hVL4bgo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "gxi_caYi4bgp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TF-IDF to do feature extraction from review contents for models.\n",
        "\n",
        "import sklearn.feature_extraction.text as sk_text\n",
        "\n",
        "vectorizer = sk_text.TfidfVectorizer(stop_words = 'english', max_features = 1000, min_df=1)\n",
        "\n",
        "matrix = vectorizer.fit_transform(df_combine.all_reviews)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9s2aNHlW4bgt",
        "colab_type": "code",
        "outputId": "0cd51149-ecb5-44d4-df25-5d9e0589b978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['able', 'absolutely', 'accommodating', 'actual', 'actually', 'add', 'added', 'addition', 'affordable', 'afternoon', 'ago', 'ahead', 'air', 'airport', 'amazing', 'ambiance', 'apparently', 'appetizer', 'appetizers', 'apple', 'appointment', 'appreciate', 'area', 'arent', 'arrived', 'asian', 'ask', 'asked', 'asking', 'ate', 'atmosphere', 'attention', 'attentive', 'attitude', 'authentic', 'available', 'average', 'avocado', 'avoid', 'away', 'awesome', 'awful', 'bacon', 'bad', 'bag', 'bagel', 'baked', 'bar', 'barely', 'bartender', 'based', 'basic', 'basically', 'bathroom', 'bbq', 'bean', 'beans', 'beautiful', 'bed', 'beef', 'beer', 'beers', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'black', 'bland', 'blue', 'book', 'bottle', 'bought', 'bowl', 'box', 'boyfriend', 'bread', 'breakfast', 'bring', 'brisket', 'broth', 'brought', 'brunch', 'bucks', 'buffet', 'building', 'bun', 'bunch', 'burger', 'burgers', 'burrito', 'business', 'busy', 'butter', 'buy', 'cafe', 'cake', 'cakes', 'calamari', 'called', 'came', 'car', 'card', 'care', 'case', 'cash', 'cashier', 'casino', 'casual', 'cause', 'center', 'certainly', 'chain', 'chairs', 'chance', 'change', 'changed', 'charge', 'charged', 'charlotte', 'cheap', 'check', 'checked', 'checking', 'cheese', 'cheesecake', 'chef', 'chicken', 'chili', 'chinese', 'chips', 'chocolate', 'choice', 'choices', 'choose', 'chose', 'city', 'classic', 'clean', 'clear', 'clearly', 'close', 'closed', 'club', 'cocktail', 'cocktails', 'coconut', 'coffee', 'cold', 'color', 'combo', 'come', 'comes', 'comfortable', 'coming', 'company', 'compared', 'complaint', 'complete', 'completely', 'considering', 'conversation', 'cook', 'cooked', 'cookie', 'cookies', 'cool', 'corn', 'corner', 'cost', 'counter', 'couple', 'course', 'covered', 'cozy', 'crab', 'craving', 'crazy', 'cream', 'creamy', 'credit', 'crepe', 'crispy', 'crowd', 'crowded', 'crust', 'cup', 'curry', 'customer', 'customers', 'cut', 'cute', 'dark', 'date', 'daughter', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'deep', 'definitely', 'delicious', 'delivered', 'delivery', 'desk', 'despite', 'dessert', 'desserts', 'did', 'didnt', 'die', 'different', 'dim', 'dining', 'dinner', 'dip', 'dirty', 'disappointed', 'disappointing', 'dish', 'dishes', 'does', 'doesnt', 'dog', 'dogs', 'doing', 'dollars', 'dont', 'door', 'double', 'downtown', 'dr', 'dressing', 'drink', 'drinks', 'drive', 'dry', 'duck', 'early', 'easily', 'easy', 'eat', 'eaten', 'eating', 'efficient', 'egg', 'eggs', 'employee', 'employees', 'end', 'ended', 'enjoy', 'enjoyed', 'entire', 'entree', 'entrees', 'environment', 'especially', 'establishment', 'evening', 'event', 'exactly', 'excellent', 'excited', 'expect', 'expectations', 'expected', 'expecting', 'expensive', 'experience', 'explained', 'extra', 'extremely', 'eye', 'fabulous', 'face', 'fact', 'fair', 'fairly', 'family', 'fan', 'fancy', 'fantastic', 'far', 'fast', 'fat', 'favorite', 'favorites', 'feel', 'feeling', 'felt', 'filet', 'filled', 'filling', 'finally', 'fine', 'finish', 'finished', 'fish', 'fix', 'fixed', 'flavor', 'flavorful', 'flavors', 'flight', 'floor', 'food', 'foods', 'forever', 'forget', 'forgot', 'forward', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'frozen', 'fruit', 'fun', 'future', 'game', 'games', 'garlic', 'gave', 'gem', 'general', 'generous', 'gets', 'getting', 'girl', 'girls', 'given', 'giving', 'glad', 'glass', 'glasses', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'grab', 'greasy', 'great', 'greek', 'green', 'greeted', 'grill', 'grilled', 'group', 'groupon', 'guacamole', 'guess', 'guests', 'guy', 'guys', 'hair', 'half', 'hand', 'hands', 'happened', 'happy', 'hard', 'hate', 'havent', 'having', 'head', 'healthy', 'hear', 'heard', 'heat', 'help', 'helped', 'helpful', 'hes', 'hidden', 'high', 'higher', 'highly', 'hit', 'hold', 'home', 'homemade', 'honest', 'honestly', 'honey', 'hope', 'horrible', 'hostess', 'hot', 'hotel', 'hour', 'hours', 'house', 'huge', 'hummus', 'hungry', 'husband', 'ice', 'iced', 'id', 'idea', 'ill', 'im', 'immediately', 'impressed', 'included', 'including', 'incredible', 'incredibly', 'indian', 'ingredients', 'inside', 'instead', 'interesting', 'interior', 'isnt', 'issue', 'issues', 'italian', 'item', 'items', 'ive', 'japanese', 'job', 'joint', 'juice', 'juicy', 'just', 'kept', 'kids', 'kind', 'kinda', 'kitchen', 'knew', 'know', 'knowledgeable', 'knows', 'korean', 'la', 'lack', 'lady', 'lamb', 'large', 'las', 'late', 'later', 'leave', 'leaving', 'left', 'lemon', 'let', 'lets', 'lettuce', 'level', 'life', 'light', 'like', 'liked', 'limited', 'line', 'list', 'literally', 'little', 'live', 'lived', 'lobster', 'local', 'located', 'location', 'locations', 'lol', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'loud', 'love', 'loved', 'lovely', 'low', 'lunch', 'mac', 'main', 'make', 'makes', 'making', 'mall', 'man', 'management', 'manager', 'mango', 'massage', 'matter', 'maybe', 'meal', 'meals', 'mean', 'means', 'meat', 'meats', 'mediocre', 'medium', 'mention', 'mentioned', 'menu', 'met', 'mexican', 'middle', 'milk', 'min', 'mind', 'mins', 'minute', 'minutes', 'miss', 'missing', 'mix', 'mixed', 'modern', 'mom', 'money', 'month', 'months', 'morning', 'mouth', 'moved', 'movie', 'multiple', 'mushroom', 'mushrooms', 'music', 'nachos', 'nail', 'nails', 'near', 'need', 'needed', 'needs', 'negative', 'neighborhood', 'new', 'nice', 'nicely', 'night', 'nights', 'noodle', 'noodles', 'normal', 'normally', 'note', 'noticed', 'number', 'obviously', 'offer', 'offered', 'office', 'oh', 'oil', 'ok', 'okay', 'old', 'ones', 'onion', 'onions', 'online', 'open', 'opened', 'opinion', 'option', 'options', 'orange', 'order', 'ordered', 'ordering', 'orders', 'original', 'outdoor', 'outside', 'outstanding', 'overall', 'overpriced', 'owner', 'owners', 'packed', 'pad', 'paid', 'pancakes', 'park', 'parking', 'party', 'pass', 'past', 'pasta', 'patio', 'pay', 'paying', 'people', 'pepper', 'peppers', 'perfect', 'perfection', 'perfectly', 'person', 'personal', 'pho', 'phoenix', 'phone', 'pick', 'picked', 'pictures', 'pie', 'piece', 'pieces', 'pita', 'pizza', 'pizzas', 'place', 'placed', 'places', 'plain', 'plan', 'plate', 'plates', 'play', 'playing', 'pleasant', 'pleased', 'plenty', 'plus', 'pm', 'point', 'polite', 'pool', 'poor', 'pork', 'portion', 'portions', 'possible', 'potato', 'potatoes', 'prefer', 'prepared', 'presentation', 'pretty', 'price', 'priced', 'prices', 'pricey', 'prime', 'probably', 'problem', 'process', 'professional', 'pulled', 'purchase', 'quality', 'questions', 'quick', 'quickly', 'quiet', 'quite', 'ramen', 'rare', 'rating', 'read', 'reading', 'ready', 'real', 'really', 'reason', 'reasonable', 'reasonably', 'received', 'recently', 'recommend', 'recommended', 'red', 'regular', 'remember', 'rental', 'reservation', 'reservations', 'rest', 'restaurant', 'restaurants', 'return', 'returning', 'review', 'reviews', 'rib', 'ribs', 'rice', 'rich', 'right', 'roasted', 'rock', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'running', 'rush', 'sad', 'said', 'salad', 'salads', 'salmon', 'salon', 'salsa', 'salt', 'salty', 'sandwich', 'sandwiches', 'sashimi', 'sat', 'satisfied', 'saturday', 'sauce', 'sauces', 'sausage', 'save', 'saw', 'say', 'saying', 'says', 'scallops', 'school', 'scottsdale', 'seafood', 'seasoned', 'seat', 'seated', 'seating', 'seats', 'second', 'section', 'seeing', 'seen', 'selection', 'sent', 'seriously', 'serve', 'served', 'server', 'servers', 'service', 'services', 'serving', 'set', 'share', 'shared', 'shop', 'shopping', 'short', 'shot', 'showed', 'shows', 'shrimp', 'sides', 'sign', 'similar', 'simple', 'simply', 'single', 'sit', 'sitting', 'size', 'slice', 'slices', 'slightly', 'slow', 'small', 'smaller', 'smell', 'smile', 'soft', 'soggy', 'solid', 'son', 'soon', 'sorry', 'sort', 'soup', 'sour', 'space', 'speak', 'special', 'specials', 'spend', 'spent', 'spice', 'spicy', 'spinach', 'split', 'spot', 'spring', 'st', 'staff', 'stand', 'standard', 'star', 'stars', 'start', 'started', 'station', 'stay', 'stayed', 'staying', 'steak', 'stick', 'stop', 'stopped', 'store', 'street', 'strip', 'strong', 'stuff', 'stuffed', 'style', 'sugar', 'suggest', 'sum', 'summer', 'sunday', 'super', 'supposed', 'sure', 'surprise', 'surprised', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'takeout', 'takes', 'taking', 'talk', 'talking', 'taste', 'tasted', 'tastes', 'tasting', 'tasty', 'tea', 'team', 'tell', 'tender', 'terrible', 'texture', 'th', 'thai', 'thank', 'thanks', 'thats', 'theres', 'theyre', 'thing', 'things', 'think', 'thinking', 'thought', 'tickets', 'time', 'times', 'tiny', 'tip', 'toast', 'today', 'tofu', 'told', 'tomato', 'tomatoes', 'tonight', 'took', 'topped', 'toppings', 'toronto', 'total', 'totally', 'touch', 'town', 'traditional', 'treat', 'treated', 'tried', 'trip', 'true', 'truly', 'try', 'trying', 'tuna', 'turkey', 'turn', 'turned', 'twice', 'type', 'typical', 'understand', 'unfortunately', 'unique', 'unless', 'use', 'used', 'using', 'usual', 'usually', 'valley', 'value', 'variety', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggie', 'veggies', 'vibe', 'view', 'visit', 'visited', 'visiting', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'walking', 'wall', 'want', 'wanted', 'warm', 'wash', 'wasnt', 'waste', 'watch', 'watching', 'water', 'way', 'wedding', 'week', 'weekend', 'weeks', 'weird', 'welcoming', 'went', 'werent', 'weve', 'white', 'wife', 'wine', 'wings', 'wish', 'woman', 'wonderful', 'wont', 'word', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worst', 'worth', 'wouldnt', 'wow', 'wrap', 'write', 'wrong', 'year', 'years', 'yelp', 'yes', 'youll', 'young', 'youre', 'yum', 'yummy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v-r-EXPp4bgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting X ready for Modeling"
      ]
    },
    {
      "metadata": {
        "id": "3WuU3F6e4bg0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#After feature normalization with Z-Score, X is ready for all other models\n",
        "\n",
        "a_1 = matrix.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z5TRBOe4bg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "normal_review_count = df_combine['normal_review_count'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-k_akD24bg_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a_2 = np.asarray([ np.asarray(normal_review_count)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cxH3dwWy4bhC",
        "colab_type": "code",
        "outputId": "364eda43-4c24-41fb-a6da-dc7bb294b326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a_1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2583, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "D0B9UDhJ4bhH",
        "colab_type": "code",
        "outputId": "495ae38a-264a-4eae-abe1-bddd044bf9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(a_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.91096793 1.31277074 0.71006653 ... 0.41349778 0.68136632 0.23172984]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "roBdt79d4bhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a_3 = np.concatenate((a_1,a_2.T), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DrRwCf3f4bhO",
        "colab_type": "code",
        "outputId": "2fb8e264-4990-4f1d-ec9f-5de3265653ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(a_3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00171747 0.01021657 0.         ... 0.         0.00227365 0.91096793]\n",
            " [0.00263981 0.00523441 0.00188756 ... 0.         0.00524202 1.31277074]\n",
            " [0.0279295  0.         0.         ... 0.         0.         0.71006653]\n",
            " ...\n",
            " [0.04920593 0.00813077 0.01172802 ... 0.         0.         0.41349778]\n",
            " [0.0068188  0.01014063 0.0048757  ... 0.0060089  0.0135405  0.68136632]\n",
            " [0.03779123 0.         0.         ... 0.         0.         0.23172984]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3rq7Fc7e4bhT",
        "colab_type": "code",
        "outputId": "c051effc-306a-4aa7-8094-6ab4ee86c491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "x = a_3\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00171747 0.01021657 0.         ... 0.         0.00227365 0.91096793]\n",
            " [0.00263981 0.00523441 0.00188756 ... 0.         0.00524202 1.31277074]\n",
            " [0.0279295  0.         0.         ... 0.         0.         0.71006653]\n",
            " ...\n",
            " [0.04920593 0.00813077 0.01172802 ... 0.         0.         0.41349778]\n",
            " [0.0068188  0.01014063 0.0048757  ... 0.0060089  0.0135405  0.68136632]\n",
            " [0.03779123 0.         0.         ... 0.         0.         0.23172984]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3_nRwDO4bhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting x ready for mnb"
      ]
    },
    {
      "metadata": {
        "id": "dxnadbdB4bhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#After feature normalization with Min-Max, X is ready for MNB model\n",
        "\n",
        "normal_min_max_review_count = df_combine['normal_min_max_review_count'].tolist()\n",
        "\n",
        "import numpy as np\n",
        "a_4 = np.asarray([ np.asarray(normal_min_max_review_count)])\n",
        "x_mnb = np.concatenate((a_1,a_4.T), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_jqr5_m4bho",
        "colab_type": "code",
        "outputId": "71eadc0b-454f-4f1b-a2ad-4fcd442b7228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(x_mnb.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2583, 1001)\n",
            "(2583, 1001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D0W-a1Vb4bhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Making Y ready for Modeling"
      ]
    },
    {
      "metadata": {
        "id": "yE2tKImj4bhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_list = df_combine['stars'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PgvYfkEw4bh4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = y_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kF0rn7w4bh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y = np.asarray(y_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-yDAZw4I4biC",
        "colab_type": "code",
        "outputId": "0e1773e5-a004-4ff9-b96b-199352e464c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2583,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Ni3FrfE4biN",
        "colab_type": "code",
        "outputId": "327fad61-bf28-4a9c-bd85-7f2242174224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_dc5yY5_4biQ",
        "colab_type": "code",
        "outputId": "2f9fcf6c-7558-48dd-ceea-c6bf4f2d5caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5, 4. , 4. , 4.5, 4.5, 3. , 4.5, 2.5, 4. , 4.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "metadata": {
        "id": "i0ZRtfLd4biT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Splitting Data for Training & Testing"
      ]
    },
    {
      "metadata": {
        "id": "9M_vkyQ74biX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Making Y ready for Modeling"
      ]
    },
    {
      "metadata": {
        "id": "psKcbwYR4biY",
        "colab_type": "code",
        "outputId": "85c4d778-4c40-4453-8656-aad97f794679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Label Encoding for linear regression,For linear regression Y is label encoded\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "#le.fit(y)\n",
        "\n",
        "normal_y = le.fit_transform(y)\n",
        "le.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "metadata": {
        "id": "w-8NnJom4big",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting data for linear regression"
      ]
    },
    {
      "metadata": {
        "id": "kc7eW0624bih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_lin, x_test_lin, y_train_lin, y_test_lin = train_test_split(x, y, test_size = 0.15, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlqBAGYO4bio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting Data for mnb"
      ]
    },
    {
      "metadata": {
        "id": "5vhujPes4bip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_mnb, x_test_mnb, y_train_mnb, y_test_mnb = train_test_split(x_mnb, normal_y, test_size = 0.15, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ-xr7Vt4biu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting Data for other models"
      ]
    },
    {
      "metadata": {
        "id": "OBH3Z-8T4biw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, normal_y, test_size = 0.15, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ThLrH1NF4biz",
        "colab_type": "code",
        "outputId": "ca1d5a53-383d-4084-ba9d-aa2802fbf0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2195,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "metadata": {
        "id": "fWqT9YNv4bi3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "metadata": {
        "id": "V9IJ3eRD4bi5",
        "colab_type": "code",
        "outputId": "09b33f5a-9933-4e37-92d2-23fcaf46f985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg.fit(x_train_lin, y_train_lin)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
              "         normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "metadata": {
        "id": "izKzAJ-z4bjC",
        "colab_type": "code",
        "outputId": "8cd5a4d6-aa77-4432-f784-1e52f3531ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred_lir = lin_reg.predict(x_test_lin)\n",
        "\n",
        "print(\"test\", y_test_lin[:20])\n",
        "print(\"pred\", y_pred_lir[:20])\n",
        "\n",
        "print(\"R2 score:\",metrics.r2_score(y_test_lin,y_pred_lir))\n",
        "print(\"RMSE::   \",np.sqrt(metrics.mean_squared_error(y_test_lin, y_pred_lir)))\n",
        "print(\"MSE:     \", metrics.mean_squared_error(y_test_lin, y_pred_lir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [3.5 4.  4.  4.  4.  4.  3.5 3.  2.5 4.  4.  3.5 3.5 3.5 4.  4.  3.  3.5\n",
            " 4.  3.5]\n",
            "pred [3.4899191  4.50240916 3.98507549 3.83589539 4.03957448 4.12322232\n",
            " 4.37257958 3.02215727 2.00980139 3.86575136 4.03244476 3.16957318\n",
            " 3.03863577 3.56206458 3.81909409 3.65518514 3.01913041 3.59092158\n",
            " 3.97158446 4.39252653]\n",
            "R2 score: 0.699275689973706\n",
            "RMSE::    0.3693440434022799\n",
            "MSE:      0.13641502239674522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0gB2j0pL4bjF",
        "colab_type": "code",
        "outputId": "3f80e617-3c08-41e5-f959-625515f7882e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "chart_regression(y_pred_lir[:50], y_test_lin[:50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/Q/ifoS+OQiubwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tz6TrB6J4bjI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "gW5Ynpnb4bjM",
        "colab_type": "code",
        "outputId": "30503333-4c70-4b0e-ff95-b6358cacdbd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "log_reg.fit(x_train,y_train)\n",
        "\n",
        "y_pred_log = log_reg.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKk7fMT94bjQ",
        "colab_type": "code",
        "outputId": "df424788-ec36-4b10-f4d8-f4d1524004ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred_log = log_reg.predict(x_test)\n",
        "\n",
        "print(\"test\", y_test[:20])\n",
        "print(\"pred\", y_pred_log[:20])\n",
        "\n",
        "\n",
        "print(metrics.confusion_matrix(y_test,y_pred_log))\n",
        "print(\"Precision Score:: \",metrics.precision_score(y_test,y_pred_log,average='weighted'))   \n",
        "print(\"Recall Score:: \",metrics.recall_score(y_test,y_pred_log,average='weighted'))      \n",
        "print(\"F1 Score:: \",metrics.f1_score(y_test,y_pred_log,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [5 6 6 6 6 6 5 4 3 6 6 5 5 5 6 6 4 5 6 5]\n",
            "pred [5 6 6 5 6 6 6 5 4 6 5 5 7 5 5 6 4 4 6 6]\n",
            "[[ 0  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  2  3  1  0  0]\n",
            " [ 0  0  0  2 16  4  1  0  0]\n",
            " [ 0  0  0  2 10 33 10  1  0]\n",
            " [ 0  0  0  0  9 38 46 10  0]\n",
            " [ 0  0  0  0  0 16 91 12  1]\n",
            " [ 0  0  0  0  0  2 31 25  1]\n",
            " [ 0  0  0  0  0  0  3 12  3]]\n",
            "Precision Score::  0.41064271939745756\n",
            "Recall Score::  0.43556701030927836\n",
            "F1 Score::  0.4016463266271519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpAAOyRa4bjU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNB"
      ]
    },
    {
      "metadata": {
        "id": "yZIQeuzZ4bjV",
        "colab_type": "code",
        "outputId": "4bc724ef-bdb8-44d3-948f-4370d8e81eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train_mnb, y_train_mnb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "StcnV3ZM4bja",
        "colab_type": "code",
        "outputId": "1ac7e407-96b6-460d-f63c-26b4997f1277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = mnb.predict(x_test_mnb)\n",
        "\n",
        "print(\"test\", y_test_mnb[:10])\n",
        "print(\"pred\", y_pred[:10])\n",
        "\n",
        "print(metrics.confusion_matrix(y_test_mnb,y_pred))\n",
        "print(\"Precision Score:: \",metrics.precision_score(y_test_mnb,y_pred,average='weighted'))   \n",
        "print(\"Recall Score:: \",metrics.recall_score(y_test_mnb,y_pred,average='weighted'))      \n",
        "print(\"F1 Score:: \",metrics.f1_score(y_test_mnb,y_pred,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [5 6 6 6 6 6 5 4 3 6]\n",
            "pred [5 6 6 5 6 6 6 5 5 6]\n",
            "[[ 0  0  0  0  1  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  0]\n",
            " [ 0  0  0  1  3  1  1  0  1]\n",
            " [ 0  0  1  0  4 11  5  2  0]\n",
            " [ 0  0  0  0  5 21 23  7  0]\n",
            " [ 0  0  0  0  1 26 53 20  3]\n",
            " [ 0  0  0  0  1  8 92 18  1]\n",
            " [ 0  0  0  0  0  1 38 19  1]\n",
            " [ 0  0  0  0  0  0  1  9  8]]\n",
            "Precision Score::  0.3477209273429134\n",
            "Recall Score::  0.3865979381443299\n",
            "F1 Score::  0.33794503887700067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "e4yaU23j4bje",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ]
    },
    {
      "metadata": {
        "id": "6LG1akOB4bjf",
        "colab_type": "code",
        "outputId": "3d65e4b5-4996-4732-f185-9e41421fd587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "           weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "Mxb92bE_4bji",
        "colab_type": "code",
        "outputId": "235c5755-294d-4afb-87e7-5f1e92f06c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred_knn = knn.predict(x_test)\n",
        "\n",
        "print(\"test\", y_test[:10])\n",
        "print(\"pred\", y_pred_knn[:10])\n",
        "\n",
        "print(metrics.confusion_matrix(y_test,y_pred_knn))\n",
        "print(\"Precision Score:: \",metrics.precision_score(y_test,y_pred_knn,average='weighted'))   \n",
        "print(\"Recall Score:: \",metrics.recall_score(y_test,y_pred_knn,average='weighted'))      \n",
        "print(\"F1 Score:: \",metrics.f1_score(y_test,y_pred_knn,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [5 6 6 6 6 6 5 4 3 6]\n",
            "pred [5 4 6 5 5 6 5 6 4 6]\n",
            "[[ 1  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  0  0]\n",
            " [ 0  1  1  4  0  1  0  0  0]\n",
            " [ 0  2  3  5  7  4  2  0  0]\n",
            " [ 0  1  2 12 16 14 10  1  0]\n",
            " [ 0  2  1 13 29 33 20  5  0]\n",
            " [ 0  2  0  3 24 30 49 12  0]\n",
            " [ 0  0  1  4  7 11 26  9  1]\n",
            " [ 0  0  0  1  2  0  7  5  3]]\n",
            "Precision Score::  0.3435868029392645\n",
            "Recall Score::  0.3015463917525773\n",
            "F1 Score::  0.30836831438545476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CSSik1Ei4bjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SVM "
      ]
    },
    {
      "metadata": {
        "id": "x_l4SGyN4bjm",
        "colab_type": "code",
        "outputId": "2c44b1cd-53bf-4311-f574-e18cd9270b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(C=10.0,kernel='linear')\n",
        "svm.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
              "  shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "DG5sBJof4bju",
        "colab_type": "code",
        "outputId": "01c59ecd-8492-4c1d-f880-76719955ceaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred_svm = svm.predict(x_test)\n",
        "\n",
        "print(\"test\", y_test[:20])\n",
        "print(\"pred\", y_pred_svm[:20])\n",
        "\n",
        "\n",
        "print(metrics.confusion_matrix(y_test,y_pred_svm))\n",
        "print(\"Precision Score:: \",metrics.precision_score(y_test,y_pred_svm,average='weighted'))   \n",
        "print(\"Recall Score:: \",metrics.recall_score(y_test,y_pred_svm,average='weighted'))      \n",
        "print(\"F1 Score:: \",metrics.f1_score(y_test,y_pred_svm,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test [5 6 6 6 6 6 5 4 3 6 6 5 5 5 6 6 4 5 6 5]\n",
            "pred [5 6 6 6 6 6 6 3 4 6 6 4 5 5 6 5 4 5 6 6]\n",
            "[[ 0  0  0  1  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  0  0]\n",
            " [ 0  1  1  4  1  0  0  0  0]\n",
            " [ 0  1  2 13  7  0  0  0  0]\n",
            " [ 0  0  3  7 27 18  1  0  0]\n",
            " [ 0  0  0  1 16 62 22  2  0]\n",
            " [ 0  0  0  0  2 19 87 12  0]\n",
            " [ 0  0  0  0  0  3 14 37  5]\n",
            " [ 0  0  0  0  0  0  1  7 10]]\n",
            "Precision Score::  0.6102940104601291\n",
            "Recall Score::  0.6108247422680413\n",
            "F1 Score::  0.6100501544957396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JPX3pWmV4bjy",
        "colab_type": "code",
        "outputId": "45a791a2-31e3-4229-a099-fa48e91c1a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a_3.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2583, 1001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "metadata": {
        "id": "c6_KlaafVfpX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Using Tensorflow**"
      ]
    },
    {
      "metadata": {
        "id": "2aDUXn7tMiU1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0JjbFZTVtM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Rgression**"
      ]
    },
    {
      "metadata": {
        "id": "wY5ITFTIWCr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model Fitting**"
      ]
    },
    {
      "metadata": {
        "id": "_E9paooxQBT-",
        "colab_type": "code",
        "outputId": "fd9c9e95-0b53-463f-a6c3-ba7677690da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(25, input_dim=x_train_lin.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "model.fit(x_train_lin,y_train_lin,verbose=2,epochs=100)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 5s - loss: 7.9652\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.1614\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.8372\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.6140\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4341\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.3061\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.2304\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1826\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1527\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1320\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1169\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1074\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0998\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0935\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0867\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0825\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0780\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0741\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0704\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0689\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0651\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0633\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0608\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0589\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0567\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0557\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0560\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0533\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0506\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0496\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0485\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0491\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0482\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0456\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0426\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0420\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0413\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0402\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0393\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0393\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0379\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0370\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0349\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0342\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.0337\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.0328\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.0314\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.0308\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.0308\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.0293\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.0293\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.0280\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.0270\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.0265\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.0274\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0254\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.0254\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.0246\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.0233\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.0236\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.0234\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.0216\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0215\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0207\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0212\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0209\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0197\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0200\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0188\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0197\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0180\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0179\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0169\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0176\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0163\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0160\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0155\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0155\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0154\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0162\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0142\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0147\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0138\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0144\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0140\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0131\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0123\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0126\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0122\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0120\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0129\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0113\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0109\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0109\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0111\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0109\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0108\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0104\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0098\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f97fd7725f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "metadata": {
        "id": "LtFnMJ_zV-Gr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Prediction & Accuracy**"
      ]
    },
    {
      "metadata": {
        "id": "IZ2jdf1bV8Zx",
        "colab_type": "code",
        "outputId": "cde6c25e-25a9-487b-e4f6-4581173e7311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6647
        }
      },
      "cell_type": "code",
      "source": [
        "pred = model.predict(x_test_lin)\n",
        "print(\"Shape: {}\".format(pred.shape))\n",
        "print(pred)\n",
        "\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test_lin))\n",
        "print(\"Final score (RMSE): {}\".format(score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (388, 1)\n",
            "[[3.376116 ]\n",
            " [4.2072926]\n",
            " [3.4434793]\n",
            " [3.788902 ]\n",
            " [4.003944 ]\n",
            " [4.2725196]\n",
            " [3.803955 ]\n",
            " [3.1586812]\n",
            " [2.138804 ]\n",
            " [3.9341507]\n",
            " [4.0515165]\n",
            " [3.2709012]\n",
            " [3.5347283]\n",
            " [3.4276526]\n",
            " [4.0438848]\n",
            " [3.5274017]\n",
            " [2.9404857]\n",
            " [3.4626534]\n",
            " [3.8306613]\n",
            " [4.2123737]\n",
            " [3.7127788]\n",
            " [3.053587 ]\n",
            " [3.321624 ]\n",
            " [3.5385852]\n",
            " [3.1069276]\n",
            " [3.4388087]\n",
            " [2.8171935]\n",
            " [3.9255412]\n",
            " [4.399789 ]\n",
            " [3.9394374]\n",
            " [4.337239 ]\n",
            " [2.4378157]\n",
            " [3.5468585]\n",
            " [4.06271  ]\n",
            " [2.9551537]\n",
            " [3.0606585]\n",
            " [3.5476565]\n",
            " [3.2327397]\n",
            " [4.1967316]\n",
            " [3.1047468]\n",
            " [4.4926534]\n",
            " [3.768099 ]\n",
            " [4.0322366]\n",
            " [3.6997354]\n",
            " [4.9182115]\n",
            " [3.8268623]\n",
            " [4.164805 ]\n",
            " [4.380322 ]\n",
            " [2.6441581]\n",
            " [4.497718 ]\n",
            " [3.35448  ]\n",
            " [4.332864 ]\n",
            " [2.8782337]\n",
            " [4.4941006]\n",
            " [3.7666843]\n",
            " [4.425686 ]\n",
            " [3.3087125]\n",
            " [3.308258 ]\n",
            " [4.048169 ]\n",
            " [4.3782406]\n",
            " [3.1293387]\n",
            " [3.1070266]\n",
            " [2.943472 ]\n",
            " [4.94181  ]\n",
            " [2.3463676]\n",
            " [3.9398336]\n",
            " [3.8594303]\n",
            " [3.7163706]\n",
            " [4.576646 ]\n",
            " [3.1519063]\n",
            " [3.5483346]\n",
            " [3.489687 ]\n",
            " [4.2832694]\n",
            " [3.5093617]\n",
            " [3.482973 ]\n",
            " [3.2092931]\n",
            " [3.509183 ]\n",
            " [2.673147 ]\n",
            " [3.4740658]\n",
            " [2.0015159]\n",
            " [3.6933923]\n",
            " [4.035256 ]\n",
            " [3.7504425]\n",
            " [4.8747563]\n",
            " [4.252254 ]\n",
            " [3.7126372]\n",
            " [3.2999744]\n",
            " [1.9761031]\n",
            " [3.998337 ]\n",
            " [3.283448 ]\n",
            " [4.7742257]\n",
            " [3.1077254]\n",
            " [2.9646583]\n",
            " [3.2815788]\n",
            " [2.1583188]\n",
            " [3.4607823]\n",
            " [4.0044208]\n",
            " [3.5992417]\n",
            " [3.843493 ]\n",
            " [2.9783392]\n",
            " [2.8691368]\n",
            " [5.6065035]\n",
            " [3.6579335]\n",
            " [5.034997 ]\n",
            " [3.5925841]\n",
            " [2.6623063]\n",
            " [4.2226653]\n",
            " [3.1560192]\n",
            " [4.201136 ]\n",
            " [4.686768 ]\n",
            " [3.5897193]\n",
            " [3.2034078]\n",
            " [3.3083296]\n",
            " [3.0365193]\n",
            " [3.0371673]\n",
            " [2.8233387]\n",
            " [3.8085577]\n",
            " [3.3298612]\n",
            " [4.764714 ]\n",
            " [4.0950136]\n",
            " [3.9160762]\n",
            " [3.4826984]\n",
            " [4.116059 ]\n",
            " [3.7585554]\n",
            " [5.029938 ]\n",
            " [3.6255677]\n",
            " [2.977915 ]\n",
            " [4.077378 ]\n",
            " [2.2749617]\n",
            " [1.9984963]\n",
            " [4.176285 ]\n",
            " [3.4909432]\n",
            " [4.5124283]\n",
            " [3.9077225]\n",
            " [3.7939618]\n",
            " [3.392398 ]\n",
            " [4.0206113]\n",
            " [3.8772807]\n",
            " [3.9821134]\n",
            " [2.4300766]\n",
            " [3.773473 ]\n",
            " [4.7918735]\n",
            " [3.0796752]\n",
            " [3.1145566]\n",
            " [2.243946 ]\n",
            " [4.661493 ]\n",
            " [3.9580338]\n",
            " [4.43282  ]\n",
            " [4.21713  ]\n",
            " [4.8485775]\n",
            " [3.3087592]\n",
            " [4.3260455]\n",
            " [3.871863 ]\n",
            " [3.7012646]\n",
            " [5.0436893]\n",
            " [5.6463304]\n",
            " [3.601214 ]\n",
            " [3.2929928]\n",
            " [2.7387807]\n",
            " [3.9209085]\n",
            " [4.01057  ]\n",
            " [3.8824127]\n",
            " [4.377914 ]\n",
            " [3.9668298]\n",
            " [4.4427066]\n",
            " [3.9185677]\n",
            " [3.9531891]\n",
            " [3.5257356]\n",
            " [3.6316533]\n",
            " [3.6288407]\n",
            " [3.7011433]\n",
            " [4.988837 ]\n",
            " [3.329554 ]\n",
            " [1.7989235]\n",
            " [3.9586177]\n",
            " [4.0589523]\n",
            " [4.6269555]\n",
            " [3.018221 ]\n",
            " [4.9890585]\n",
            " [3.9457722]\n",
            " [4.2089133]\n",
            " [3.7593644]\n",
            " [3.8223372]\n",
            " [3.899669 ]\n",
            " [4.1858478]\n",
            " [3.8268578]\n",
            " [4.4432926]\n",
            " [3.8821893]\n",
            " [3.905865 ]\n",
            " [3.6905475]\n",
            " [3.8208947]\n",
            " [3.3792915]\n",
            " [3.303418 ]\n",
            " [4.0659485]\n",
            " [4.2106376]\n",
            " [3.845166 ]\n",
            " [2.3972504]\n",
            " [4.6918173]\n",
            " [2.8286893]\n",
            " [4.0820427]\n",
            " [3.69933  ]\n",
            " [4.3945255]\n",
            " [4.0246325]\n",
            " [4.072658 ]\n",
            " [3.522988 ]\n",
            " [3.0317748]\n",
            " [2.5122826]\n",
            " [2.7244513]\n",
            " [2.9089491]\n",
            " [4.0929275]\n",
            " [3.2686167]\n",
            " [3.1797667]\n",
            " [3.2495909]\n",
            " [2.249859 ]\n",
            " [4.60354  ]\n",
            " [2.821604 ]\n",
            " [3.490041 ]\n",
            " [2.7254884]\n",
            " [4.473635 ]\n",
            " [4.452966 ]\n",
            " [4.452742 ]\n",
            " [4.2046485]\n",
            " [3.7749429]\n",
            " [4.438787 ]\n",
            " [3.7971566]\n",
            " [3.5307446]\n",
            " [5.099057 ]\n",
            " [1.1169873]\n",
            " [3.856238 ]\n",
            " [4.5004354]\n",
            " [2.0924103]\n",
            " [3.5953996]\n",
            " [3.4052544]\n",
            " [3.6913805]\n",
            " [2.6396894]\n",
            " [3.6093748]\n",
            " [3.467949 ]\n",
            " [4.562193 ]\n",
            " [3.5604699]\n",
            " [3.7823353]\n",
            " [3.2876716]\n",
            " [3.3205047]\n",
            " [3.5031536]\n",
            " [3.841684 ]\n",
            " [3.5970652]\n",
            " [3.3311467]\n",
            " [3.2260373]\n",
            " [4.769285 ]\n",
            " [4.049056 ]\n",
            " [4.7557054]\n",
            " [4.293903 ]\n",
            " [3.5196354]\n",
            " [3.0668745]\n",
            " [4.2431397]\n",
            " [4.3642344]\n",
            " [2.7418656]\n",
            " [3.213043 ]\n",
            " [3.2212858]\n",
            " [3.5279105]\n",
            " [4.2569776]\n",
            " [2.6037538]\n",
            " [3.8274605]\n",
            " [3.9508352]\n",
            " [5.1190977]\n",
            " [2.8098056]\n",
            " [3.8025236]\n",
            " [3.9328296]\n",
            " [3.978658 ]\n",
            " [5.214314 ]\n",
            " [3.9096317]\n",
            " [3.6228578]\n",
            " [4.8245344]\n",
            " [3.9576418]\n",
            " [2.3306592]\n",
            " [3.7723808]\n",
            " [3.956456 ]\n",
            " [3.3860776]\n",
            " [4.584452 ]\n",
            " [4.0743737]\n",
            " [4.515247 ]\n",
            " [3.9043171]\n",
            " [3.7940729]\n",
            " [3.669007 ]\n",
            " [4.035514 ]\n",
            " [3.225186 ]\n",
            " [4.6661396]\n",
            " [3.8562944]\n",
            " [3.8863149]\n",
            " [4.6537094]\n",
            " [3.4386077]\n",
            " [3.125881 ]\n",
            " [3.513191 ]\n",
            " [3.7909591]\n",
            " [4.097545 ]\n",
            " [3.8660676]\n",
            " [3.5305502]\n",
            " [4.071892 ]\n",
            " [3.604934 ]\n",
            " [3.4308138]\n",
            " [3.8474548]\n",
            " [2.954891 ]\n",
            " [3.7413127]\n",
            " [4.0804086]\n",
            " [4.58896  ]\n",
            " [2.244369 ]\n",
            " [3.1094434]\n",
            " [4.307273 ]\n",
            " [1.7248973]\n",
            " [3.83482  ]\n",
            " [4.454297 ]\n",
            " [3.8792956]\n",
            " [3.7811472]\n",
            " [4.5263796]\n",
            " [2.6374235]\n",
            " [3.5563936]\n",
            " [3.198968 ]\n",
            " [2.8578455]\n",
            " [3.0334527]\n",
            " [4.001317 ]\n",
            " [4.083092 ]\n",
            " [2.6641421]\n",
            " [3.673    ]\n",
            " [3.9009607]\n",
            " [3.399502 ]\n",
            " [3.3403738]\n",
            " [4.1189737]\n",
            " [3.1787496]\n",
            " [3.8753498]\n",
            " [3.9138246]\n",
            " [4.3233194]\n",
            " [3.9744706]\n",
            " [3.2336519]\n",
            " [2.6922317]\n",
            " [1.909662 ]\n",
            " [3.3850713]\n",
            " [3.8709483]\n",
            " [2.448325 ]\n",
            " [4.112991 ]\n",
            " [4.584027 ]\n",
            " [2.5945942]\n",
            " [4.2320175]\n",
            " [4.231656 ]\n",
            " [2.788355 ]\n",
            " [2.7786045]\n",
            " [4.174355 ]\n",
            " [3.6355245]\n",
            " [3.5758674]\n",
            " [3.0714276]\n",
            " [4.299439 ]\n",
            " [3.9014592]\n",
            " [3.8535764]\n",
            " [3.4590385]\n",
            " [2.9248095]\n",
            " [4.0414686]\n",
            " [3.0569575]\n",
            " [3.3840165]\n",
            " [3.467037 ]\n",
            " [4.714801 ]\n",
            " [4.570035 ]\n",
            " [5.1378884]\n",
            " [3.0376067]\n",
            " [2.4086063]\n",
            " [3.4374423]\n",
            " [5.4865184]\n",
            " [4.033235 ]\n",
            " [4.2756596]\n",
            " [3.7590642]\n",
            " [3.2371137]\n",
            " [3.0077326]\n",
            " [3.5719233]\n",
            " [4.3826356]\n",
            " [4.4305105]\n",
            " [3.8947427]\n",
            " [4.2336087]\n",
            " [4.230597 ]\n",
            " [4.9836516]\n",
            " [4.538907 ]\n",
            " [5.7128296]\n",
            " [3.8426418]\n",
            " [3.2563667]\n",
            " [3.5885482]\n",
            " [3.7894745]\n",
            " [3.1230175]\n",
            " [3.600155 ]\n",
            " [3.9912994]\n",
            " [3.011101 ]\n",
            " [4.3262024]\n",
            " [1.943971 ]]\n",
            "Final score (RMSE): 0.33881235783064645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FpUfPCPISx1u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fntykWQsVa9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **With Early Stopping**"
      ]
    },
    {
      "metadata": {
        "id": "mSXG8r71XIU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Improved Score with early stopping**"
      ]
    },
    {
      "metadata": {
        "id": "M0BIezyOOcyZ",
        "colab_type": "code",
        "outputId": "edfbc66e-c974-4d88-93b0-26dcdf2918dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10387
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_regression.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    #build network\n",
        "    model_regression = Sequential()\n",
        "\n",
        "    model_regression.add(Dense(25, input_dim=x_train_lin.shape[1], activation='relu')) # Hidden 1   \n",
        "    model_regression.add(Dense(10, activation='relu')) # Hidden 2\n",
        "    model_regression.add(Dense(1)) # Output\n",
        "\n",
        "    model_regression.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model_regression.fit(x_train_lin,y_train_lin,validation_data=(x_test_lin,y_test_lin),callbacks=[monitor,checkpointer],verbose=2,epochs=100)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 5s - loss: 8.0885 - val_loss: 1.6278\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.2169 - val_loss: 0.9430\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.8808 - val_loss: 0.7039\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.6230 - val_loss: 0.4990\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4149 - val_loss: 0.3581\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.2891 - val_loss: 0.2804\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.2242 - val_loss: 0.2379\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1868 - val_loss: 0.2080\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1610 - val_loss: 0.1883\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1436 - val_loss: 0.1746\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1283 - val_loss: 0.1632\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1178 - val_loss: 0.1538\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.1106 - val_loss: 0.1468\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.1038 - val_loss: 0.1464\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0979 - val_loss: 0.1389\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0927 - val_loss: 0.1342\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0888 - val_loss: 0.1320\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0847 - val_loss: 0.1285\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0818 - val_loss: 0.1242\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0784 - val_loss: 0.1246\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0775 - val_loss: 0.1201\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0743 - val_loss: 0.1186\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0708 - val_loss: 0.1177\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0686 - val_loss: 0.1155\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0666 - val_loss: 0.1153\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0660 - val_loss: 0.1155\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0659 - val_loss: 0.1122\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0610 - val_loss: 0.1104\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0587 - val_loss: 0.1119\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0572 - val_loss: 0.1078\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0552 - val_loss: 0.1064\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0549 - val_loss: 0.1080\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0535 - val_loss: 0.1064\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0525 - val_loss: 0.1054\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0505 - val_loss: 0.1067\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0492 - val_loss: 0.1076\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0497 - val_loss: 0.1057\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0469 - val_loss: 0.1046\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0474 - val_loss: 0.1052\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0451 - val_loss: 0.1042\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0457 - val_loss: 0.1043\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0432 - val_loss: 0.1028\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0424 - val_loss: 0.1046\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0416 - val_loss: 0.1049\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.0414 - val_loss: 0.1037\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.0399 - val_loss: 0.1036\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.0402 - val_loss: 0.1032\n",
            "Epoch 00047: early stopping\n",
            "1\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 4.7441 - val_loss: 1.0927\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.9506 - val_loss: 0.7047\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.6086 - val_loss: 0.4765\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.3900 - val_loss: 0.3432\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2685 - val_loss: 0.2685\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.2011 - val_loss: 0.2223\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1655 - val_loss: 0.1894\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1378 - val_loss: 0.1677\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1207 - val_loss: 0.1556\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1089 - val_loss: 0.1434\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.0980 - val_loss: 0.1380\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0930 - val_loss: 0.1331\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0857 - val_loss: 0.1280\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0827 - val_loss: 0.1242\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0765 - val_loss: 0.1211\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0729 - val_loss: 0.1175\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0693 - val_loss: 0.1146\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0662 - val_loss: 0.1186\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0644 - val_loss: 0.1125\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0617 - val_loss: 0.1144\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0584 - val_loss: 0.1065\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0580 - val_loss: 0.1053\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0557 - val_loss: 0.1101\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0541 - val_loss: 0.1053\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0513 - val_loss: 0.1063\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0502 - val_loss: 0.1050\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0491 - val_loss: 0.1046\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0483 - val_loss: 0.1076\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0482 - val_loss: 0.1069\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0464 - val_loss: 0.1058\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0445 - val_loss: 0.1050\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0439 - val_loss: 0.1020\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0430 - val_loss: 0.1071\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0412 - val_loss: 0.1051\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0410 - val_loss: 0.1044\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0405 - val_loss: 0.1034\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0398 - val_loss: 0.1050\n",
            "Epoch 00037: early stopping\n",
            "2\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 7.4087 - val_loss: 1.2303\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.0942 - val_loss: 0.8569\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.8124 - val_loss: 0.6617\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.6066 - val_loss: 0.5082\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4392 - val_loss: 0.3778\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.3002 - val_loss: 0.2765\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.2110 - val_loss: 0.2155\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1586 - val_loss: 0.1837\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1301 - val_loss: 0.1641\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1136 - val_loss: 0.1402\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1021 - val_loss: 0.1367\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0951 - val_loss: 0.1259\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0875 - val_loss: 0.1232\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0837 - val_loss: 0.1209\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0796 - val_loss: 0.1151\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0760 - val_loss: 0.1150\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0728 - val_loss: 0.1131\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0705 - val_loss: 0.1117\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0687 - val_loss: 0.1092\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0662 - val_loss: 0.1091\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0638 - val_loss: 0.1078\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0628 - val_loss: 0.1055\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0609 - val_loss: 0.1067\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0600 - val_loss: 0.1058\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0581 - val_loss: 0.1085\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0572 - val_loss: 0.1090\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0583 - val_loss: 0.1119\n",
            "Epoch 00027: early stopping\n",
            "3\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 6.1985 - val_loss: 1.2094\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.0400 - val_loss: 0.7585\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.6545 - val_loss: 0.4917\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4016 - val_loss: 0.3279\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2641 - val_loss: 0.2513\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1986 - val_loss: 0.2132\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1615 - val_loss: 0.1793\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1358 - val_loss: 0.1624\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1192 - val_loss: 0.1533\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1068 - val_loss: 0.1397\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.0993 - val_loss: 0.1357\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0911 - val_loss: 0.1285\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0865 - val_loss: 0.1251\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0841 - val_loss: 0.1221\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0773 - val_loss: 0.1183\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0735 - val_loss: 0.1199\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0704 - val_loss: 0.1129\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0669 - val_loss: 0.1116\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0651 - val_loss: 0.1107\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0613 - val_loss: 0.1094\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0596 - val_loss: 0.1062\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0575 - val_loss: 0.1054\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0555 - val_loss: 0.1071\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0551 - val_loss: 0.1041\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0524 - val_loss: 0.1041\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0515 - val_loss: 0.1031\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0488 - val_loss: 0.1018\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0481 - val_loss: 0.1062\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0494 - val_loss: 0.1020\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0455 - val_loss: 0.1026\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0442 - val_loss: 0.1088\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0437 - val_loss: 0.0980\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0412 - val_loss: 0.0975\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0415 - val_loss: 0.1004\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0387 - val_loss: 0.0976\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0382 - val_loss: 0.0981\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0367 - val_loss: 0.0982\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0362 - val_loss: 0.0991\n",
            "Epoch 00038: early stopping\n",
            "4\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 5.8188 - val_loss: 1.0953\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.9520 - val_loss: 0.7235\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.6484 - val_loss: 0.5227\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4468 - val_loss: 0.3774\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.3072 - val_loss: 0.2896\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.2299 - val_loss: 0.2404\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1880 - val_loss: 0.2080\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1596 - val_loss: 0.1863\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1392 - val_loss: 0.1740\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1249 - val_loss: 0.1595\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1133 - val_loss: 0.1518\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1063 - val_loss: 0.1480\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0983 - val_loss: 0.1391\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0917 - val_loss: 0.1320\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0871 - val_loss: 0.1314\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0846 - val_loss: 0.1265\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0772 - val_loss: 0.1232\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0723 - val_loss: 0.1207\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0698 - val_loss: 0.1223\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0664 - val_loss: 0.1185\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0642 - val_loss: 0.1131\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0614 - val_loss: 0.1105\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0587 - val_loss: 0.1112\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0570 - val_loss: 0.1127\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0547 - val_loss: 0.1069\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0524 - val_loss: 0.1077\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0502 - val_loss: 0.1062\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0497 - val_loss: 0.1088\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0477 - val_loss: 0.1054\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0457 - val_loss: 0.1049\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0442 - val_loss: 0.1040\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0431 - val_loss: 0.1053\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0415 - val_loss: 0.1075\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0410 - val_loss: 0.1044\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0397 - val_loss: 0.1059\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0384 - val_loss: 0.1033\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0377 - val_loss: 0.1155\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0393 - val_loss: 0.1081\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0359 - val_loss: 0.1021\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0341 - val_loss: 0.1024\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0334 - val_loss: 0.1037\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0339 - val_loss: 0.1031\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0321 - val_loss: 0.1031\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0313 - val_loss: 0.1156\n",
            "Epoch 00044: early stopping\n",
            "5\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 5.5782 - val_loss: 1.1589\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.0143 - val_loss: 0.7340\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.6494 - val_loss: 0.4863\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4016 - val_loss: 0.3251\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2576 - val_loss: 0.2376\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1911 - val_loss: 0.1957\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1542 - val_loss: 0.1708\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1298 - val_loss: 0.1542\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1158 - val_loss: 0.1451\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1043 - val_loss: 0.1367\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.0962 - val_loss: 0.1319\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0904 - val_loss: 0.1266\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0846 - val_loss: 0.1233\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0803 - val_loss: 0.1224\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0760 - val_loss: 0.1163\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0718 - val_loss: 0.1140\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0699 - val_loss: 0.1152\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0670 - val_loss: 0.1110\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0644 - val_loss: 0.1094\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0624 - val_loss: 0.1088\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0596 - val_loss: 0.1072\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0578 - val_loss: 0.1054\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0565 - val_loss: 0.1041\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0554 - val_loss: 0.1027\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0527 - val_loss: 0.1053\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0508 - val_loss: 0.1014\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0503 - val_loss: 0.1015\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0496 - val_loss: 0.1016\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0481 - val_loss: 0.1028\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0469 - val_loss: 0.1007\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0457 - val_loss: 0.1031\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0450 - val_loss: 0.1012\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0439 - val_loss: 0.1006\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0419 - val_loss: 0.1038\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0398 - val_loss: 0.0998\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0389 - val_loss: 0.1002\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0373 - val_loss: 0.1038\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0357 - val_loss: 0.1066\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0354 - val_loss: 0.1041\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0361 - val_loss: 0.1038\n",
            "Epoch 00040: early stopping\n",
            "6\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 7.4584 - val_loss: 1.2804\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.1376 - val_loss: 0.8594\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.7820 - val_loss: 0.5966\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4675 - val_loss: 0.3479\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2723 - val_loss: 0.2436\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1871 - val_loss: 0.1908\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1430 - val_loss: 0.1582\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1171 - val_loss: 0.1523\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1032 - val_loss: 0.1330\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.0922 - val_loss: 0.1216\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.0824 - val_loss: 0.1157\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0769 - val_loss: 0.1145\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0718 - val_loss: 0.1102\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0672 - val_loss: 0.1089\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0654 - val_loss: 0.1044\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0604 - val_loss: 0.1095\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0580 - val_loss: 0.1019\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0543 - val_loss: 0.1009\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0542 - val_loss: 0.1081\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0498 - val_loss: 0.0992\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0478 - val_loss: 0.0960\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0458 - val_loss: 0.0974\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0453 - val_loss: 0.0979\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0454 - val_loss: 0.0956\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0416 - val_loss: 0.0954\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0404 - val_loss: 0.0934\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0387 - val_loss: 0.0991\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0383 - val_loss: 0.0981\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0370 - val_loss: 0.0948\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0357 - val_loss: 0.0993\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0342 - val_loss: 0.0941\n",
            "Epoch 00031: early stopping\n",
            "7\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 12.6114 - val_loss: 8.6865\n",
            "Epoch 2/100\n",
            " - 0s - loss: 3.8568 - val_loss: 1.1713\n",
            "Epoch 3/100\n",
            " - 0s - loss: 1.1841 - val_loss: 0.9620\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.9310 - val_loss: 0.7043\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.5877 - val_loss: 0.4226\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.3398 - val_loss: 0.2818\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.2189 - val_loss: 0.2080\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1615 - val_loss: 0.1727\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1309 - val_loss: 0.1538\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1117 - val_loss: 0.1426\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1015 - val_loss: 0.1338\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.0905 - val_loss: 0.1321\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.0839 - val_loss: 0.1230\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.0790 - val_loss: 0.1197\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.0739 - val_loss: 0.1193\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.0706 - val_loss: 0.1143\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.0673 - val_loss: 0.1171\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.0639 - val_loss: 0.1127\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.0622 - val_loss: 0.1100\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.0589 - val_loss: 0.1103\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.0567 - val_loss: 0.1083\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0538 - val_loss: 0.1063\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0533 - val_loss: 0.1060\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0516 - val_loss: 0.1045\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0490 - val_loss: 0.1069\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0471 - val_loss: 0.1063\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0456 - val_loss: 0.1065\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0478 - val_loss: 0.1065\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0450 - val_loss: 0.1049\n",
            "Epoch 00029: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a70wFgkoSwT7",
        "colab_type": "code",
        "outputId": "9aee5ace-ab80-46a5-8370-a03ca74fc184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "model_regression.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_regression.hdf5')\n",
        "pred = model_regression.predict(x_test_lin)\n",
        "score = np.sqrt(metrics.mean_squared_error(y_test_lin,pred)) \n",
        "\n",
        "print(\"Score (RMSE):   {}\".format(score))\n",
        "print(\"R2 score       \",metrics.r2_score(y_test_lin,pred))\n",
        "print(\"MSE:           \", metrics.mean_squared_error(y_test_lin, pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score (RMSE):   0.3055426061211678\n",
            "R2 score        0.7941978555882595\n",
            "MSE:            0.09335628415531509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wJyfxDCkVoLA",
        "colab_type": "code",
        "outputId": "089f4b52-bc13-4bdb-ba63-1b0411aec4af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "chart_regression(pred[:50].flatten(),y_test_lin[:50],sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RkrfTonkXV1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using  SGD and sigmoid"
      ]
    },
    {
      "metadata": {
        "id": "fy_ji8TzW_7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "zZpkg-u8Xq4B",
        "colab_type": "code",
        "outputId": "e377832e-46fc-453e-976b-1ab375d9accb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 22865
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_regression1.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    #build network\n",
        "    model_regression = Sequential()\n",
        "\n",
        "    model_regression.add(Dense(25, input_dim=x_train_lin.shape[1], activation='sigmoid')) # Hidden 1   \n",
        "    model_regression.add(Dense(10, activation='sigmoid')) # Hidden 2\n",
        "    model_regression.add(Dense(1)) # Output\n",
        "\n",
        "    model_regression.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model_regression.fit(x_train_lin,y_train_lin,validation_data=(x_test_lin,y_test_lin),callbacks=[monitor,checkpointer],verbose=2,epochs=100)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.2716 - val_loss: 0.4526\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4892 - val_loss: 0.4512\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4870 - val_loss: 0.4513\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4881 - val_loss: 0.4509\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4873 - val_loss: 0.4521\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4522\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4870 - val_loss: 0.4498\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4496\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4504\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4492\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4488\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4506\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4494\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4486\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4479\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4486\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4475\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4496\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4849 - val_loss: 0.4469\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4840 - val_loss: 0.4481\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4466\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4463\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4461\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4831 - val_loss: 0.4467\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4489\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4459\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4458\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4467\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4450\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4448\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4809 - val_loss: 0.4445\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4439\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4443\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4807 - val_loss: 0.4435\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4433\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4802 - val_loss: 0.4429\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4428\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4437\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4423\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4784 - val_loss: 0.4422\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4787 - val_loss: 0.4422\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4417\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4784 - val_loss: 0.4410\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4775 - val_loss: 0.4413\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4407\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4402\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4768 - val_loss: 0.4403\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4764 - val_loss: 0.4403\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4399\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4416\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4759 - val_loss: 0.4398\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4405\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4386\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4738 - val_loss: 0.4384\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4736 - val_loss: 0.4373\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4369\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4719 - val_loss: 0.4365\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4720 - val_loss: 0.4374\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4715 - val_loss: 0.4358\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4714 - val_loss: 0.4355\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4367\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4708 - val_loss: 0.4360\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4701 - val_loss: 0.4343\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4701 - val_loss: 0.4378\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4703 - val_loss: 0.4341\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4691 - val_loss: 0.4335\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4680 - val_loss: 0.4342\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4680 - val_loss: 0.4322\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4676 - val_loss: 0.4316\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4665 - val_loss: 0.4314\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4663 - val_loss: 0.4306\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4656 - val_loss: 0.4303\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4650 - val_loss: 0.4295\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4644 - val_loss: 0.4297\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4641 - val_loss: 0.4283\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4629 - val_loss: 0.4278\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4615 - val_loss: 0.4272\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4612 - val_loss: 0.4269\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4607 - val_loss: 0.4267\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4604 - val_loss: 0.4254\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4597 - val_loss: 0.4262\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4586 - val_loss: 0.4255\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4580 - val_loss: 0.4261\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4575 - val_loss: 0.4226\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4563 - val_loss: 0.4241\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4561 - val_loss: 0.4221\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4542 - val_loss: 0.4219\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4538 - val_loss: 0.4199\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4529 - val_loss: 0.4189\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4522 - val_loss: 0.4180\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4513 - val_loss: 0.4171\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4497 - val_loss: 0.4162\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4492 - val_loss: 0.4153\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4478 - val_loss: 0.4143\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4461 - val_loss: 0.4139\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4454 - val_loss: 0.4160\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4446 - val_loss: 0.4113\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4441 - val_loss: 0.4116\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4424 - val_loss: 0.4093\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4410 - val_loss: 0.4081\n",
            "1\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.4290 - val_loss: 0.4509\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4510\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4504\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4513\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4500\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4502\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4494\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4501\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4497\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4493\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4505\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4484\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4481\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4480\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4477\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4475\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4473\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4473\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4471\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4469\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4469\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4833 - val_loss: 0.4463\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4463\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4835 - val_loss: 0.4462\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4460\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4833 - val_loss: 0.4461\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4830 - val_loss: 0.4457\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4815 - val_loss: 0.4493\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4824 - val_loss: 0.4451\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4450\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4821 - val_loss: 0.4445\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4483\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4440\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4810 - val_loss: 0.4466\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4787 - val_loss: 0.4479\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4438\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4431\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4430\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4442\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4795 - val_loss: 0.4424\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4423\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4420\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4428\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4419\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4777 - val_loss: 0.4413\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4778 - val_loss: 0.4436\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4773 - val_loss: 0.4427\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4780 - val_loss: 0.4418\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4771 - val_loss: 0.4411\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4399\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4404\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4758 - val_loss: 0.4394\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4392\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4756 - val_loss: 0.4397\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4412\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4383\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4741 - val_loss: 0.4380\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4738 - val_loss: 0.4384\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4744 - val_loss: 0.4385\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4734 - val_loss: 0.4374\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4378\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4729 - val_loss: 0.4374\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4719 - val_loss: 0.4360\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4357\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4712 - val_loss: 0.4354\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4714 - val_loss: 0.4350\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4704 - val_loss: 0.4347\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4706 - val_loss: 0.4347\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4696 - val_loss: 0.4418\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4697 - val_loss: 0.4352\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4683 - val_loss: 0.4335\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4684 - val_loss: 0.4327\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4681 - val_loss: 0.4325\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4676 - val_loss: 0.4325\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4672 - val_loss: 0.4315\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4669 - val_loss: 0.4322\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4661 - val_loss: 0.4305\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4646 - val_loss: 0.4333\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4639 - val_loss: 0.4296\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4635 - val_loss: 0.4302\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4638 - val_loss: 0.4299\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4640 - val_loss: 0.4281\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4620 - val_loss: 0.4281\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4630 - val_loss: 0.4279\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4621 - val_loss: 0.4267\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4611 - val_loss: 0.4301\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4606 - val_loss: 0.4255\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4599 - val_loss: 0.4250\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4592 - val_loss: 0.4254\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4578 - val_loss: 0.4238\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4579 - val_loss: 0.4229\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4554 - val_loss: 0.4308\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4562 - val_loss: 0.4215\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4554 - val_loss: 0.4208\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4541 - val_loss: 0.4215\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4533 - val_loss: 0.4196\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4526 - val_loss: 0.4198\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4514 - val_loss: 0.4208\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4509 - val_loss: 0.4171\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4499 - val_loss: 0.4162\n",
            "2\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 2.0477 - val_loss: 0.4531\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4906 - val_loss: 0.4528\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4896 - val_loss: 0.4527\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4899 - val_loss: 0.4525\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4895 - val_loss: 0.4529\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4894 - val_loss: 0.4529\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4892 - val_loss: 0.4520\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4519\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4523\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4519\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4881 - val_loss: 0.4522\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4512\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4510\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4511\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4518\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4514\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4520\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4515\n",
            "Epoch 00018: early stopping\n",
            "3\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 2.3949 - val_loss: 0.4522\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4896 - val_loss: 0.4516\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4515\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4514\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4561\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4506\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4883 - val_loss: 0.4506\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4503\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4548\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4503\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4528\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4496\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4499\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4855 - val_loss: 0.4502\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4490\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4501\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4503\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4483\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4513\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4505\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4478\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4491\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4487\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4474\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4475\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4476\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4471\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4480\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4465\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4464\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4833 - val_loss: 0.4475\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4830 - val_loss: 0.4460\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4471\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4829 - val_loss: 0.4458\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4456\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4453\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4459\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4450\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4456\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4821 - val_loss: 0.4452\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4451\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4821 - val_loss: 0.4446\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4444\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4447\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4440\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4802 - val_loss: 0.4436\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4436\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4434\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4431\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4798 - val_loss: 0.4447\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4429\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4429\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4428\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4788 - val_loss: 0.4439\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4423\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4420\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4784 - val_loss: 0.4429\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4414\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4414\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4777 - val_loss: 0.4411\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4425\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4407\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4437\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4407\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4767 - val_loss: 0.4410\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4764 - val_loss: 0.4399\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4763 - val_loss: 0.4404\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4395\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4760 - val_loss: 0.4391\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4759 - val_loss: 0.4389\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4752 - val_loss: 0.4397\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4389\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4382\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4754 - val_loss: 0.4379\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4379\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4741 - val_loss: 0.4374\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4375\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4737 - val_loss: 0.4370\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4727 - val_loss: 0.4384\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4737 - val_loss: 0.4364\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4729 - val_loss: 0.4368\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4725 - val_loss: 0.4360\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4718 - val_loss: 0.4361\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4723 - val_loss: 0.4364\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4361\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4346\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4348\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4708 - val_loss: 0.4340\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4696 - val_loss: 0.4339\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4699 - val_loss: 0.4333\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4694 - val_loss: 0.4332\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4683 - val_loss: 0.4350\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4683 - val_loss: 0.4332\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4687 - val_loss: 0.4343\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4686 - val_loss: 0.4325\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4675 - val_loss: 0.4311\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4671 - val_loss: 0.4309\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4669 - val_loss: 0.4304\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4659 - val_loss: 0.4305\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4650 - val_loss: 0.4315\n",
            "4\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.3377 - val_loss: 0.4529\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4899 - val_loss: 0.4542\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4897 - val_loss: 0.4521\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4895 - val_loss: 0.4520\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4888 - val_loss: 0.4524\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4894 - val_loss: 0.4511\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4509\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4518\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4504\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4501\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4873 - val_loss: 0.4499\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4525\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4498\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4854 - val_loss: 0.4504\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4491\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4504\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4485\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4484\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4481\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4849 - val_loss: 0.4479\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4479\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4475\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4473\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4483\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4471\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4840 - val_loss: 0.4486\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4472\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4466\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4829 - val_loss: 0.4463\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4502\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4836 - val_loss: 0.4457\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4456\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4453\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4466\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4452\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4489\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4815 - val_loss: 0.4450\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4450\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4442\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4438\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4504\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4437\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4796 - val_loss: 0.4452\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4442\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4783 - val_loss: 0.4438\n",
            "Epoch 00045: early stopping\n",
            "5\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.1893 - val_loss: 0.4521\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4506\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4505\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4496\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4492\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4856 - val_loss: 0.4485\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4488\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4505\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4491\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4474\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4501\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4477\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4482\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4829 - val_loss: 0.4472\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4472\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4461\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4817 - val_loss: 0.4456\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4455\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4455\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4449\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4451\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4481\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4439\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4797 - val_loss: 0.4438\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4793 - val_loss: 0.4432\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4430\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4796 - val_loss: 0.4440\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4790 - val_loss: 0.4426\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4793 - val_loss: 0.4435\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4781 - val_loss: 0.4421\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4771 - val_loss: 0.4442\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4412\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4410\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4410\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4436\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4398\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4756 - val_loss: 0.4398\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4391\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4409\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4396\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4739 - val_loss: 0.4381\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4379\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4730 - val_loss: 0.4382\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4725 - val_loss: 0.4377\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4711 - val_loss: 0.4408\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4727 - val_loss: 0.4362\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4712 - val_loss: 0.4358\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4354\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4359\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4694 - val_loss: 0.4346\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4698 - val_loss: 0.4346\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4691 - val_loss: 0.4346\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4682 - val_loss: 0.4333\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4670 - val_loss: 0.4348\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4677 - val_loss: 0.4342\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4667 - val_loss: 0.4320\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4667 - val_loss: 0.4313\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4658 - val_loss: 0.4307\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4648 - val_loss: 0.4305\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4645 - val_loss: 0.4298\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4634 - val_loss: 0.4295\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4632 - val_loss: 0.4288\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4628 - val_loss: 0.4371\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4626 - val_loss: 0.4279\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4617 - val_loss: 0.4268\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4591 - val_loss: 0.4274\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4597 - val_loss: 0.4260\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4587 - val_loss: 0.4256\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4582 - val_loss: 0.4247\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4573 - val_loss: 0.4253\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4563 - val_loss: 0.4238\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4554 - val_loss: 0.4221\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4544 - val_loss: 0.4218\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4548 - val_loss: 0.4209\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4533 - val_loss: 0.4198\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4527 - val_loss: 0.4200\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4517 - val_loss: 0.4186\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4494 - val_loss: 0.4178\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4487 - val_loss: 0.4167\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4484 - val_loss: 0.4154\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4467 - val_loss: 0.4151\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4460 - val_loss: 0.4138\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4451 - val_loss: 0.4135\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4446 - val_loss: 0.4125\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4425 - val_loss: 0.4113\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4416 - val_loss: 0.4094\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4386 - val_loss: 0.4115\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4377 - val_loss: 0.4076\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4374 - val_loss: 0.4060\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4363 - val_loss: 0.4051\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4347 - val_loss: 0.4052\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4332 - val_loss: 0.4068\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4322 - val_loss: 0.4009\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4301 - val_loss: 0.3996\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4281 - val_loss: 0.3983\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4270 - val_loss: 0.3968\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4258 - val_loss: 0.3973\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4234 - val_loss: 0.3952\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4218 - val_loss: 0.3924\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4202 - val_loss: 0.3925\n",
            "6\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.1601 - val_loss: 0.4528\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4901 - val_loss: 0.4556\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4900 - val_loss: 0.4524\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4896 - val_loss: 0.4517\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4885 - val_loss: 0.4530\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4515\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4882 - val_loss: 0.4513\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4885 - val_loss: 0.4505\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4883 - val_loss: 0.4504\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4500\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4498\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4500\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4873 - val_loss: 0.4493\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4495\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4507\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4491\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4485\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4483\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4482\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4856 - val_loss: 0.4507\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4477\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4856 - val_loss: 0.4483\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4849 - val_loss: 0.4484\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4849 - val_loss: 0.4476\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4470\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4468\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4466\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4464\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4464\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4835 - val_loss: 0.4468\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4460\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4457\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4835 - val_loss: 0.4457\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4453\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4824 - val_loss: 0.4454\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4492\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4824 - val_loss: 0.4456\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4464\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4448\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4810 - val_loss: 0.4460\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4441\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4444\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4440\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4434\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4434\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4447\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4435\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4798 - val_loss: 0.4446\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4427\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4795 - val_loss: 0.4424\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4419\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4420\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4416\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4413\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4783 - val_loss: 0.4410\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4408\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4780 - val_loss: 0.4405\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4783 - val_loss: 0.4403\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4774 - val_loss: 0.4400\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4770 - val_loss: 0.4400\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4765 - val_loss: 0.4409\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4765 - val_loss: 0.4457\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4762 - val_loss: 0.4393\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4392\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4752 - val_loss: 0.4385\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4748 - val_loss: 0.4395\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4380\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4377\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4419\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4372\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4735 - val_loss: 0.4369\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4373\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4734 - val_loss: 0.4365\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4727 - val_loss: 0.4375\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4726 - val_loss: 0.4376\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4722 - val_loss: 0.4357\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4715 - val_loss: 0.4359\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4708 - val_loss: 0.4391\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4349\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4711 - val_loss: 0.4341\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4699 - val_loss: 0.4353\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4701 - val_loss: 0.4341\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4697 - val_loss: 0.4329\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4687 - val_loss: 0.4327\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4684 - val_loss: 0.4331\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4678 - val_loss: 0.4327\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4670 - val_loss: 0.4327\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4670 - val_loss: 0.4311\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4670 - val_loss: 0.4304\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4660 - val_loss: 0.4303\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4655 - val_loss: 0.4297\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4653 - val_loss: 0.4301\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4648 - val_loss: 0.4286\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4630 - val_loss: 0.4294\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4638 - val_loss: 0.4288\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4632 - val_loss: 0.4272\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4623 - val_loss: 0.4266\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4618 - val_loss: 0.4260\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4608 - val_loss: 0.4262\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4603 - val_loss: 0.4259\n",
            "7\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 3.0177 - val_loss: 0.4515\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4516\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4881 - val_loss: 0.4516\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4522\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4511\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4882 - val_loss: 0.4509\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4508\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4507\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4873 - val_loss: 0.4508\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4510\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4874 - val_loss: 0.4512\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4517\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4499\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4505\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4498\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4518\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4501\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4493\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4501\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4497\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4493\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4510\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4490\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4510\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4485\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4483\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4499\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4486\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4479\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4512\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4483\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4482\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4476\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4495\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4476\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4476\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4471\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4469\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4468\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4490\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4493\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4464\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4476\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4832 - val_loss: 0.4463\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4829 - val_loss: 0.4463\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4830 - val_loss: 0.4462\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4457\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4821 - val_loss: 0.4457\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4827 - val_loss: 0.4456\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4815 - val_loss: 0.4456\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4461\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4451\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4449\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4448\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4446\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4809 - val_loss: 0.4445\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4445\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4442\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4462\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4440\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4437\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4436\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4435\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4798 - val_loss: 0.4449\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4431\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4797 - val_loss: 0.4432\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4430\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4793 - val_loss: 0.4433\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4788 - val_loss: 0.4425\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4797 - val_loss: 0.4423\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4788 - val_loss: 0.4430\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4421\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4784 - val_loss: 0.4421\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4771 - val_loss: 0.4438\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4778 - val_loss: 0.4413\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4413\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4770 - val_loss: 0.4412\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4441\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4407\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4765 - val_loss: 0.4403\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4408\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4418\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4762 - val_loss: 0.4412\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4762 - val_loss: 0.4395\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4752 - val_loss: 0.4395\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4752 - val_loss: 0.4392\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4740 - val_loss: 0.4433\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4754 - val_loss: 0.4386\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4394\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4744 - val_loss: 0.4381\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4738 - val_loss: 0.4395\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4743 - val_loss: 0.4377\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4735 - val_loss: 0.4387\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4375\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4396\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4730 - val_loss: 0.4373\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4722 - val_loss: 0.4363\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4375\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4358\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4715 - val_loss: 0.4358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YWF8qkZAXrJF",
        "colab_type": "code",
        "outputId": "a0aa85ff-0fde-4d01-da28-6ea140e3cae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "model_regression.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_regression1.hdf5')\n",
        "pred1 = model_regression.predict(x_test_lin)\n",
        "score = np.sqrt(metrics.mean_squared_error(y_test_lin,pred1)) \n",
        "\n",
        "print(\"Score (RMSE):   {}\".format(score))\n",
        "print(\"R2 score       \",metrics.r2_score(y_test_lin,pred1))\n",
        "print(\"MSE:           \", metrics.mean_squared_error(y_test_lin, pred1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score (RMSE):   0.6264275314082124\n",
            "R2 score        0.13493645268891774\n",
            "MSE:            0.39241145210618694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qFrEDyIfal4Z",
        "colab_type": "code",
        "outputId": "c169c210-700b-466c-ca23-a0c5bc6d26c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "chart_regression(pred1[:50].flatten(),y_test_lin[:50],sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noD08TKbLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vW0TJ21haeSi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Using SGD & tanh**"
      ]
    },
    {
      "metadata": {
        "id": "76SBhVv3XrGv",
        "colab_type": "code",
        "outputId": "d7c83b26-e238-41ee-ff34-6c47a6c2711f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24820
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_regression1.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    #build network\n",
        "    model_regression = Sequential()\n",
        "\n",
        "    model_regression.add(Dense(25, input_dim=x_train_lin.shape[1], activation='sigmoid')) # Hidden 1   \n",
        "    model_regression.add(Dense(10, activation='sigmoid')) # Hidden 2\n",
        "    model_regression.add(Dense(1)) # Output\n",
        "\n",
        "    model_regression.compile(loss='mean_squared_error', optimizer='sgd')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "    model_regression.fit(x_train_lin,y_train_lin,validation_data=(x_test_lin,y_test_lin),callbacks=[monitor,checkpointer],verbose=2,epochs=100)    # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.6842 - val_loss: 0.4533\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4897 - val_loss: 0.4521\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4894 - val_loss: 0.4527\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4894 - val_loss: 0.4515\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4514\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4516\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4515\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4542\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4505\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4870 - val_loss: 0.4512\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4501\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4498\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4505\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4870 - val_loss: 0.4498\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4870 - val_loss: 0.4498\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4491\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4489\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4489\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4506\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4485\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4491\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4854 - val_loss: 0.4480\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4486\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4848 - val_loss: 0.4493\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4477\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4848 - val_loss: 0.4484\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4474\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4471\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4469\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4469\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4835 - val_loss: 0.4466\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4831 - val_loss: 0.4465\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4835 - val_loss: 0.4462\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4475\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4475\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4832 - val_loss: 0.4458\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4475\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4455\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4452\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4452\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4467\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4826 - val_loss: 0.4446\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4452\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4444\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4442\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4439\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4444\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4435\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4435\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4431\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4797 - val_loss: 0.4443\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4796 - val_loss: 0.4427\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4444\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4423\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4432\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4419\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4788 - val_loss: 0.4417\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4787 - val_loss: 0.4415\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4775 - val_loss: 0.4423\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4434\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4781 - val_loss: 0.4410\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4773 - val_loss: 0.4434\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4767 - val_loss: 0.4406\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4764 - val_loss: 0.4410\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4755 - val_loss: 0.4434\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4770 - val_loss: 0.4400\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4402\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4754 - val_loss: 0.4410\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4758 - val_loss: 0.4395\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4755 - val_loss: 0.4399\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4755 - val_loss: 0.4400\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4381\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4744 - val_loss: 0.4383\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4736 - val_loss: 0.4376\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4738 - val_loss: 0.4373\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4370\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4729 - val_loss: 0.4395\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4735 - val_loss: 0.4377\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4367\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4719 - val_loss: 0.4374\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4355\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4713 - val_loss: 0.4366\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4707 - val_loss: 0.4350\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4708 - val_loss: 0.4345\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4691 - val_loss: 0.4356\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4692 - val_loss: 0.4349\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4688 - val_loss: 0.4385\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4699 - val_loss: 0.4330\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4689 - val_loss: 0.4326\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4680 - val_loss: 0.4322\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4675 - val_loss: 0.4324\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4667 - val_loss: 0.4335\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4664 - val_loss: 0.4311\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4662 - val_loss: 0.4309\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4662 - val_loss: 0.4304\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4645 - val_loss: 0.4328\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4650 - val_loss: 0.4300\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4640 - val_loss: 0.4296\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4631 - val_loss: 0.4304\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4634 - val_loss: 0.4285\n",
            "1\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.5221 - val_loss: 0.4533\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4909 - val_loss: 0.4531\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4898 - val_loss: 0.4530\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4894 - val_loss: 0.4530\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4893 - val_loss: 0.4515\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4512\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4514\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4528\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4503\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4868 - val_loss: 0.4512\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4506\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4498\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4500\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4491\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4488\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4525\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4492\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4483\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4503\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4842 - val_loss: 0.4517\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4475\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4473\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4474\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4471\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4472\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4465\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4463\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4464\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4464\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4824 - val_loss: 0.4457\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4456\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4460\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4451\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4448\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4447\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4444\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4484\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4441\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4438\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4802 - val_loss: 0.4439\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4434\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4430\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4428\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4796 - val_loss: 0.4435\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4423\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4425\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4418\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4420\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4415\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4420\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4409\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4405\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4404\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4400\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4763 - val_loss: 0.4396\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4759 - val_loss: 0.4395\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4391\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4759 - val_loss: 0.4391\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4385\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4751 - val_loss: 0.4381\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4746 - val_loss: 0.4388\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4744 - val_loss: 0.4375\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4729 - val_loss: 0.4414\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4739 - val_loss: 0.4369\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4365\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4730 - val_loss: 0.4362\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4724 - val_loss: 0.4365\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4703 - val_loss: 0.4365\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4713 - val_loss: 0.4359\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4347\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4710 - val_loss: 0.4350\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4698 - val_loss: 0.4340\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4696 - val_loss: 0.4341\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4693 - val_loss: 0.4344\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4690 - val_loss: 0.4331\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4676 - val_loss: 0.4328\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4676 - val_loss: 0.4322\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4677 - val_loss: 0.4314\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4664 - val_loss: 0.4310\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4653 - val_loss: 0.4306\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4657 - val_loss: 0.4299\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4650 - val_loss: 0.4294\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4642 - val_loss: 0.4303\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4638 - val_loss: 0.4301\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4633 - val_loss: 0.4280\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4623 - val_loss: 0.4282\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4615 - val_loss: 0.4312\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4616 - val_loss: 0.4266\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4605 - val_loss: 0.4266\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4612 - val_loss: 0.4249\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4596 - val_loss: 0.4248\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4576 - val_loss: 0.4260\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4584 - val_loss: 0.4230\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4569 - val_loss: 0.4227\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4565 - val_loss: 0.4241\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4563 - val_loss: 0.4218\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4547 - val_loss: 0.4206\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4542 - val_loss: 0.4202\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4525 - val_loss: 0.4192\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4522 - val_loss: 0.4184\n",
            "2\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.5387 - val_loss: 0.4534\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4881 - val_loss: 0.4522\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4534\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4882 - val_loss: 0.4508\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4507\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4506\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4513\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4504\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4531\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4490\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4488\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4488\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4854 - val_loss: 0.4482\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4499\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4490\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4836 - val_loss: 0.4474\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4477\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4836 - val_loss: 0.4469\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4832 - val_loss: 0.4508\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4834 - val_loss: 0.4464\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4497\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4469\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4458\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4454\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4477\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4812 - val_loss: 0.4460\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4450\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4809 - val_loss: 0.4443\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4798 - val_loss: 0.4441\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4457\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4440\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4437\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4433\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4426\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4424\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4429\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4774 - val_loss: 0.4420\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4778 - val_loss: 0.4416\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4416\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4770 - val_loss: 0.4410\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4408\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4764 - val_loss: 0.4408\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4760 - val_loss: 0.4409\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4755 - val_loss: 0.4447\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4394\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4389\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4743 - val_loss: 0.4406\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4393\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4739 - val_loss: 0.4379\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4380\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4408\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4724 - val_loss: 0.4380\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4724 - val_loss: 0.4363\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4718 - val_loss: 0.4383\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4711 - val_loss: 0.4355\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4702 - val_loss: 0.4361\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4705 - val_loss: 0.4347\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4703 - val_loss: 0.4346\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4690 - val_loss: 0.4339\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4686 - val_loss: 0.4334\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4681 - val_loss: 0.4330\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4683 - val_loss: 0.4325\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4676 - val_loss: 0.4320\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4662 - val_loss: 0.4320\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4662 - val_loss: 0.4315\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4659 - val_loss: 0.4304\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4649 - val_loss: 0.4310\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4645 - val_loss: 0.4293\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4626 - val_loss: 0.4297\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4632 - val_loss: 0.4293\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4623 - val_loss: 0.4280\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4610 - val_loss: 0.4270\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4602 - val_loss: 0.4353\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4611 - val_loss: 0.4264\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4586 - val_loss: 0.4298\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4588 - val_loss: 0.4254\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4579 - val_loss: 0.4237\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4567 - val_loss: 0.4229\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4566 - val_loss: 0.4222\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4560 - val_loss: 0.4215\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4541 - val_loss: 0.4227\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4536 - val_loss: 0.4235\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4517 - val_loss: 0.4227\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4521 - val_loss: 0.4189\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4510 - val_loss: 0.4184\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4500 - val_loss: 0.4174\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4486 - val_loss: 0.4173\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4478 - val_loss: 0.4146\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4466 - val_loss: 0.4136\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4454 - val_loss: 0.4131\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4438 - val_loss: 0.4116\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4428 - val_loss: 0.4108\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4417 - val_loss: 0.4094\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4404 - val_loss: 0.4082\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4387 - val_loss: 0.4071\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4376 - val_loss: 0.4062\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4361 - val_loss: 0.4052\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4340 - val_loss: 0.4058\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4336 - val_loss: 0.4023\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4309 - val_loss: 0.4020\n",
            "3\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 0.7710 - val_loss: 0.4535\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4895 - val_loss: 0.4518\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4897 - val_loss: 0.4517\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4888 - val_loss: 0.4519\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4899 - val_loss: 0.4518\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4516\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4888 - val_loss: 0.4509\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4885 - val_loss: 0.4506\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4506\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4510\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4501\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4509\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4509\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4875 - val_loss: 0.4503\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4503\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4496\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4505\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4498\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4503\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4542\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4509\n",
            "Epoch 00021: early stopping\n",
            "4\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.1860 - val_loss: 0.4532\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4909 - val_loss: 0.4533\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4902 - val_loss: 0.4523\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4900 - val_loss: 0.4519\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4890 - val_loss: 0.4523\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4899 - val_loss: 0.4511\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4502\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4878 - val_loss: 0.4502\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4500\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4500\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4489\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4487\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4488\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4483\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4490\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4475\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4476\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4470\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4469\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4848 - val_loss: 0.4473\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4464\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4475\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4836 - val_loss: 0.4464\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4457\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4826 - val_loss: 0.4464\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4831 - val_loss: 0.4451\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4450\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4446\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4445\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4442\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4441\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4438\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4815 - val_loss: 0.4463\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4442\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4431\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4434\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4804 - val_loss: 0.4429\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4796 - val_loss: 0.4445\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4427\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4795 - val_loss: 0.4418\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4791 - val_loss: 0.4419\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4788 - val_loss: 0.4413\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4411\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4778 - val_loss: 0.4414\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4412\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4777 - val_loss: 0.4407\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4401\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4404\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4774 - val_loss: 0.4412\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4391\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4758 - val_loss: 0.4400\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4758 - val_loss: 0.4385\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4755 - val_loss: 0.4382\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4752 - val_loss: 0.4382\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4748 - val_loss: 0.4377\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4396\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4740 - val_loss: 0.4374\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4739 - val_loss: 0.4375\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4415\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4380\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4727 - val_loss: 0.4356\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4720 - val_loss: 0.4373\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4349\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4712 - val_loss: 0.4346\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4716 - val_loss: 0.4367\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4713 - val_loss: 0.4375\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4698 - val_loss: 0.4336\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4703 - val_loss: 0.4333\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4697 - val_loss: 0.4331\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4689 - val_loss: 0.4322\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4681 - val_loss: 0.4318\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4686 - val_loss: 0.4314\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4674 - val_loss: 0.4312\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4667 - val_loss: 0.4305\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4666 - val_loss: 0.4302\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4661 - val_loss: 0.4305\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4656 - val_loss: 0.4291\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4644 - val_loss: 0.4286\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4646 - val_loss: 0.4287\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4630 - val_loss: 0.4277\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4627 - val_loss: 0.4271\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4622 - val_loss: 0.4275\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4628 - val_loss: 0.4261\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4613 - val_loss: 0.4294\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4608 - val_loss: 0.4288\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4610 - val_loss: 0.4246\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4592 - val_loss: 0.4246\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4589 - val_loss: 0.4251\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4585 - val_loss: 0.4224\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4570 - val_loss: 0.4228\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4556 - val_loss: 0.4212\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4563 - val_loss: 0.4204\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4552 - val_loss: 0.4215\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4544 - val_loss: 0.4193\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4531 - val_loss: 0.4218\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4531 - val_loss: 0.4176\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4519 - val_loss: 0.4167\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4513 - val_loss: 0.4161\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4498 - val_loss: 0.4178\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4484 - val_loss: 0.4144\n",
            "5\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 1.2336 - val_loss: 0.4538\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4528\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4513\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4513\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4515\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4885 - val_loss: 0.4505\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4548\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4507\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4501\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4863 - val_loss: 0.4496\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4502\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4531\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4865 - val_loss: 0.4492\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4861 - val_loss: 0.4495\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4489\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4502\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4855 - val_loss: 0.4483\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4480\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4481\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4845 - val_loss: 0.4479\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4474\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4471\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4839 - val_loss: 0.4470\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4476\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4545\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4473\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4831 - val_loss: 0.4461\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4470\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4456\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4459\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4453\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4453\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4448\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4446\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4464\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4810 - val_loss: 0.4463\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4809 - val_loss: 0.4445\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4811 - val_loss: 0.4438\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4443\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4438\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4801 - val_loss: 0.4429\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4790 - val_loss: 0.4428\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4775 - val_loss: 0.4439\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4433\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4787 - val_loss: 0.4425\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4781 - val_loss: 0.4434\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4787 - val_loss: 0.4425\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4774 - val_loss: 0.4421\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4431\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4406\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4761 - val_loss: 0.4428\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4759 - val_loss: 0.4402\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4756 - val_loss: 0.4428\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4758 - val_loss: 0.4394\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4754 - val_loss: 0.4390\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4750 - val_loss: 0.4392\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4748 - val_loss: 0.4387\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4742 - val_loss: 0.4386\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4738 - val_loss: 0.4388\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4375\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4371\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4732 - val_loss: 0.4368\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4368\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4360\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4714 - val_loss: 0.4357\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4708 - val_loss: 0.4356\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4711 - val_loss: 0.4348\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4700 - val_loss: 0.4346\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4700 - val_loss: 0.4342\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4689 - val_loss: 0.4335\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4682 - val_loss: 0.4331\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4677 - val_loss: 0.4328\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4679 - val_loss: 0.4324\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4671 - val_loss: 0.4328\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4666 - val_loss: 0.4314\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4667 - val_loss: 0.4316\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4650 - val_loss: 0.4308\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4652 - val_loss: 0.4299\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4645 - val_loss: 0.4297\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4642 - val_loss: 0.4286\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4629 - val_loss: 0.4284\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4621 - val_loss: 0.4275\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4614 - val_loss: 0.4313\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4611 - val_loss: 0.4286\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4607 - val_loss: 0.4271\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4600 - val_loss: 0.4251\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4583 - val_loss: 0.4246\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4583 - val_loss: 0.4238\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4576 - val_loss: 0.4232\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4566 - val_loss: 0.4225\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4556 - val_loss: 0.4229\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4550 - val_loss: 0.4209\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4538 - val_loss: 0.4206\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4525 - val_loss: 0.4214\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4525 - val_loss: 0.4192\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4508 - val_loss: 0.4176\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4507 - val_loss: 0.4167\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4493 - val_loss: 0.4174\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4489 - val_loss: 0.4153\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4471 - val_loss: 0.4144\n",
            "6\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 7s - loss: 2.1816 - val_loss: 0.4532\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4895 - val_loss: 0.4517\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4523\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4889 - val_loss: 0.4511\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4882 - val_loss: 0.4528\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4881 - val_loss: 0.4520\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4876 - val_loss: 0.4527\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4529\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4499\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4497\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4501\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4864 - val_loss: 0.4501\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4495\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4862 - val_loss: 0.4490\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4495\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4490\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4852 - val_loss: 0.4483\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4857 - val_loss: 0.4486\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4483\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4487\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4488\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4480\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4846 - val_loss: 0.4482\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4514\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4490\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4474\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4837 - val_loss: 0.4479\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4833 - val_loss: 0.4466\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4818 - val_loss: 0.4469\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4838 - val_loss: 0.4460\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4459\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4457\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4497\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4829 - val_loss: 0.4455\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4822 - val_loss: 0.4455\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4821 - val_loss: 0.4458\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4468\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4820 - val_loss: 0.4448\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4804 - val_loss: 0.4446\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4446\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4449\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4810 - val_loss: 0.4444\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4439\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4804 - val_loss: 0.4439\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4800 - val_loss: 0.4435\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4799 - val_loss: 0.4447\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4441\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4798 - val_loss: 0.4429\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4805 - val_loss: 0.4429\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4793 - val_loss: 0.4429\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4426\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4423\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4786 - val_loss: 0.4419\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4418\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4784 - val_loss: 0.4415\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4781 - val_loss: 0.4415\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4776 - val_loss: 0.4411\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4441\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4774 - val_loss: 0.4407\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4769 - val_loss: 0.4405\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4435\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4768 - val_loss: 0.4403\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4763 - val_loss: 0.4415\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4763 - val_loss: 0.4405\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4402\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4760 - val_loss: 0.4396\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4757 - val_loss: 0.4389\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4393\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4747 - val_loss: 0.4394\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4749 - val_loss: 0.4382\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4743 - val_loss: 0.4380\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4376\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4741 - val_loss: 0.4373\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4735 - val_loss: 0.4371\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4379\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4733 - val_loss: 0.4365\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4725 - val_loss: 0.4369\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4718 - val_loss: 0.4374\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4720 - val_loss: 0.4360\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4719 - val_loss: 0.4355\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4714 - val_loss: 0.4353\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4712 - val_loss: 0.4347\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4705 - val_loss: 0.4377\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4704 - val_loss: 0.4349\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4699 - val_loss: 0.4338\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4701 - val_loss: 0.4338\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4689 - val_loss: 0.4334\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4694 - val_loss: 0.4328\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4677 - val_loss: 0.4335\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4677 - val_loss: 0.4327\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4675 - val_loss: 0.4319\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4670 - val_loss: 0.4320\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4663 - val_loss: 0.4308\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4664 - val_loss: 0.4304\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4655 - val_loss: 0.4323\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4656 - val_loss: 0.4299\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4647 - val_loss: 0.4293\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4645 - val_loss: 0.4289\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4634 - val_loss: 0.4283\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4634 - val_loss: 0.4280\n",
            "7\n",
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 7s - loss: 2.3563 - val_loss: 0.4519\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.4896 - val_loss: 0.4515\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.4887 - val_loss: 0.4523\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.4888 - val_loss: 0.4510\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.4886 - val_loss: 0.4507\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.4885 - val_loss: 0.4506\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.4884 - val_loss: 0.4504\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.4880 - val_loss: 0.4512\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.4879 - val_loss: 0.4510\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.4877 - val_loss: 0.4496\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.4869 - val_loss: 0.4508\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.4873 - val_loss: 0.4491\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.4872 - val_loss: 0.4490\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4509\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.4858 - val_loss: 0.4498\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.4853 - val_loss: 0.4495\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.4866 - val_loss: 0.4480\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.4860 - val_loss: 0.4484\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.4850 - val_loss: 0.4479\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.4854 - val_loss: 0.4478\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4481\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.4848 - val_loss: 0.4471\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.4851 - val_loss: 0.4468\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.4844 - val_loss: 0.4471\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.4843 - val_loss: 0.4480\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.4841 - val_loss: 0.4463\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.4840 - val_loss: 0.4460\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.4832 - val_loss: 0.4503\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.4828 - val_loss: 0.4468\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.4826 - val_loss: 0.4463\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.4831 - val_loss: 0.4452\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4448\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.4823 - val_loss: 0.4447\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.4452\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4443\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.4816 - val_loss: 0.4442\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4484\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.4813 - val_loss: 0.4436\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.4803 - val_loss: 0.4457\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.4808 - val_loss: 0.4435\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.4806 - val_loss: 0.4427\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4428\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.4793 - val_loss: 0.4422\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.4797 - val_loss: 0.4419\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.4792 - val_loss: 0.4426\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.4790 - val_loss: 0.4416\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.4785 - val_loss: 0.4427\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.4782 - val_loss: 0.4410\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.4779 - val_loss: 0.4410\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4406\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.4773 - val_loss: 0.4400\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.4766 - val_loss: 0.4404\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.4770 - val_loss: 0.4394\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.4762 - val_loss: 0.4390\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4388\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.4751 - val_loss: 0.4386\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4751 - val_loss: 0.4381\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4745 - val_loss: 0.4381\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4739 - val_loss: 0.4398\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4735 - val_loss: 0.4381\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4366\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4729 - val_loss: 0.4368\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4725 - val_loss: 0.4365\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4720 - val_loss: 0.4355\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.4706 - val_loss: 0.4373\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.4717 - val_loss: 0.4345\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.4703 - val_loss: 0.4341\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.4701 - val_loss: 0.4338\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.4697 - val_loss: 0.4337\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.4700 - val_loss: 0.4328\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.4688 - val_loss: 0.4323\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.4682 - val_loss: 0.4317\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.4671 - val_loss: 0.4333\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.4673 - val_loss: 0.4311\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.4664 - val_loss: 0.4302\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.4659 - val_loss: 0.4304\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.4653 - val_loss: 0.4306\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.4642 - val_loss: 0.4312\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.4641 - val_loss: 0.4295\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.4629 - val_loss: 0.4275\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.4625 - val_loss: 0.4266\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.4612 - val_loss: 0.4287\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.4608 - val_loss: 0.4254\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.4602 - val_loss: 0.4261\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.4598 - val_loss: 0.4237\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.4588 - val_loss: 0.4235\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.4580 - val_loss: 0.4223\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.4565 - val_loss: 0.4214\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.4561 - val_loss: 0.4207\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.4552 - val_loss: 0.4198\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.4550 - val_loss: 0.4189\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.4531 - val_loss: 0.4183\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.4512 - val_loss: 0.4175\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.4509 - val_loss: 0.4166\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.4493 - val_loss: 0.4159\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.4487 - val_loss: 0.4175\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.4477 - val_loss: 0.4134\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.4464 - val_loss: 0.4127\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.4447 - val_loss: 0.4116\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.4441 - val_loss: 0.4101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iq8BsIRLapXt",
        "colab_type": "code",
        "outputId": "507e1fa1-fb0f-40e4-c8a2-8bb772abf226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "model_regression.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_regression1.hdf5')\n",
        "pred2 = model_regression.predict(x_test_lin)\n",
        "score = np.sqrt(metrics.mean_squared_error(y_test_lin,pred2)) \n",
        "\n",
        "print(\"Score (RMSE):   {}\".format(score))\n",
        "print(\"R2 score       \",metrics.r2_score(y_test_lin,pred1))\n",
        "print(\"MSE:           \", metrics.mean_squared_error(y_test_lin, pred2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score (RMSE):   0.6340179578108384\n",
            "R2 score        0.13493645268891774\n",
            "MSE:            0.40197877082662603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iQE67D7W1s92",
        "colab_type": "code",
        "outputId": "17a6fecf-9fee-4b44-9001-b66e36082683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "chart_regression(pred2[:30].flatten(),y_test_lin[:30],sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJFXMEJCkihkCklSxDw31pWmKy339AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "eSf6Gm8qa1wv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Classification**"
      ]
    },
    {
      "metadata": {
        "id": "FIUX5hXhPl7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding "
      ]
    },
    {
      "metadata": {
        "id": "g8q9Fw7ca0ba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMBH__C6eErJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns_new = ['normal_y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WpPUIUZ_eEnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_y_class = pd.DataFrame(normal_y , columns=columns_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2gEhLEhdjvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encode_text_dummy(df_y_class,'normal_y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQkpWGj8eT-M",
        "colab_type": "code",
        "outputId": "8172fe76-2f82-4009-d3b4-885430897c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "df_y_class[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normal_y-0</th>\n",
              "      <th>normal_y-1</th>\n",
              "      <th>normal_y-2</th>\n",
              "      <th>normal_y-3</th>\n",
              "      <th>normal_y-4</th>\n",
              "      <th>normal_y-5</th>\n",
              "      <th>normal_y-6</th>\n",
              "      <th>normal_y-7</th>\n",
              "      <th>normal_y-8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   normal_y-0  normal_y-1  normal_y-2  normal_y-3  normal_y-4  normal_y-5  \\\n",
              "0           0           0           0           1           0           0   \n",
              "1           0           0           0           0           0           0   \n",
              "2           0           0           0           0           0           0   \n",
              "3           0           0           0           0           0           0   \n",
              "4           0           0           0           0           0           0   \n",
              "5           0           0           0           0           1           0   \n",
              "6           0           0           0           0           0           0   \n",
              "7           0           0           0           1           0           0   \n",
              "8           0           0           0           0           0           0   \n",
              "9           0           0           0           0           0           0   \n",
              "\n",
              "   normal_y-6  normal_y-7  normal_y-8  \n",
              "0           0           0           0  \n",
              "1           1           0           0  \n",
              "2           1           0           0  \n",
              "3           0           1           0  \n",
              "4           0           1           0  \n",
              "5           0           0           0  \n",
              "6           0           1           0  \n",
              "7           0           0           0  \n",
              "8           1           0           0  \n",
              "9           0           1           0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "metadata": {
        "id": "9aIV1hDaea2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_encoded_y = df_y_class.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJuafr-Qekov",
        "colab_type": "code",
        "outputId": "fe73875b-b398-4c4e-c672-b226def1907f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "df_encoded_y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "metadata": {
        "id": "LvVvq654foAZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Label Smoothing (Additional)**"
      ]
    },
    {
      "metadata": {
        "id": "sOq3napOfsjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_smooth =  df_encoded_y *0.9  # label smoothing "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s4WCPVdFftBA",
        "colab_type": "code",
        "outputId": "91e13a79-0a78-42c1-dbd5-d2913df51fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "df_smooth[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
              "       [0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "metadata": {
        "id": "5GRASpA-f7qT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Splitting Dataset**"
      ]
    },
    {
      "metadata": {
        "id": "Xx27ABQlftHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_trainNN,x_testNN,y_trainNN,y_testNN = train_test_split(x,df_encoded_y,test_size=0.25,random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9p2M1UVqgWsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_trainNNS,x_testNNS,y_trainNNS,y_testNNS = train_test_split(x,df_smooth,test_size=0.25,random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTrUKzClhK8p",
        "colab_type": "code",
        "outputId": "f6e6c691-8899-47d3-9e39-98c363c7e670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_trainNN.shape\n",
        "y_trainNN.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1937, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "metadata": {
        "id": "lRX5qFcFhWgI",
        "colab_type": "code",
        "outputId": "647426b7-723e-413d-a2b6-587808071c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_encoded_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2583, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "0UVe3cjkhdDw",
        "colab_type": "code",
        "outputId": "e0ccf214-8d06-469e-d61f-30a9f2b30f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_smooth.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2583, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "metadata": {
        "id": "Hah424bqJFjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tensorflow classification neural network model "
      ]
    },
    {
      "metadata": {
        "id": "283eOxFwhfZI",
        "colab_type": "code",
        "outputId": "5a95967f-1d99-49dc-9dc3-f75ecba7bae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5644
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NN.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNN.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='relu')) # Hidden 2\n",
        "    \n",
        "    model_classification.add(Dense(y_trainNN.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNN, y_trainNN,validation_data=(x_testNN,y_testNN),callbacks=[monitor,checkpointer],verbose=2,epochs=500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.8741 - val_loss: 1.7080\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.5625 - val_loss: 1.5221\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.3813 - val_loss: 1.3333\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.2160 - val_loss: 1.2063\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.0840 - val_loss: 1.0783\n",
            "Epoch 6/500\n",
            " - 0s - loss: 0.9806 - val_loss: 0.9989\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.8976 - val_loss: 0.9477\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8214 - val_loss: 0.8999\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7700 - val_loss: 0.8648\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7168 - val_loss: 0.8397\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.6751 - val_loss: 0.8148\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6419 - val_loss: 0.8042\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6062 - val_loss: 0.7861\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.5699 - val_loss: 0.7850\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5438 - val_loss: 0.7724\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5272 - val_loss: 0.7749\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.4964 - val_loss: 0.7760\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4709 - val_loss: 0.7751\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4526 - val_loss: 0.7803\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4348 - val_loss: 0.8003\n",
            "Epoch 00020: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.8569 - val_loss: 1.7358\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6344 - val_loss: 1.6093\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.4927 - val_loss: 1.4498\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3150 - val_loss: 1.2847\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1609 - val_loss: 1.1575\n",
            "Epoch 6/500\n",
            " - 0s - loss: 1.0376 - val_loss: 1.0652\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.9467 - val_loss: 0.9911\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8633 - val_loss: 0.9305\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7965 - val_loss: 0.8878\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7453 - val_loss: 0.8569\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.6913 - val_loss: 0.8427\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6561 - val_loss: 0.8103\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6239 - val_loss: 0.8029\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.5893 - val_loss: 0.7956\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5628 - val_loss: 0.7790\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5289 - val_loss: 0.7770\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.5038 - val_loss: 0.7998\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4855 - val_loss: 0.7908\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4658 - val_loss: 0.7903\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4400 - val_loss: 0.7949\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.4251 - val_loss: 0.7944\n",
            "Epoch 00021: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.9660 - val_loss: 1.7895\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6769 - val_loss: 1.6503\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.5257 - val_loss: 1.4644\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3189 - val_loss: 1.2670\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1247 - val_loss: 1.0946\n",
            "Epoch 6/500\n",
            " - 0s - loss: 0.9879 - val_loss: 0.9934\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.8862 - val_loss: 0.9171\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8095 - val_loss: 0.8753\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7583 - val_loss: 0.8313\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7017 - val_loss: 0.8136\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.6631 - val_loss: 0.8045\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6257 - val_loss: 0.7803\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.5877 - val_loss: 0.7663\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.5608 - val_loss: 0.7667\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5361 - val_loss: 0.7697\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5135 - val_loss: 0.7672\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.4847 - val_loss: 0.7676\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4632 - val_loss: 0.7635\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4444 - val_loss: 0.7831\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4172 - val_loss: 0.7745\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.3986 - val_loss: 0.7753\n",
            "Epoch 22/500\n",
            " - 0s - loss: 0.3847 - val_loss: 0.7869\n",
            "Epoch 23/500\n",
            " - 0s - loss: 0.3619 - val_loss: 0.7962\n",
            "Epoch 00023: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.9575 - val_loss: 1.7680\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6676 - val_loss: 1.6694\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.5468 - val_loss: 1.5194\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3712 - val_loss: 1.3298\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1916 - val_loss: 1.1626\n",
            "Epoch 6/500\n",
            " - 0s - loss: 1.0449 - val_loss: 1.0692\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.9402 - val_loss: 0.9754\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8612 - val_loss: 0.9228\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7996 - val_loss: 0.8735\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7452 - val_loss: 0.8645\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.7043 - val_loss: 0.8282\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6752 - val_loss: 0.8051\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6313 - val_loss: 0.8002\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.6024 - val_loss: 0.7900\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5707 - val_loss: 0.7846\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5414 - val_loss: 0.7884\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.5276 - val_loss: 0.8023\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4946 - val_loss: 0.7743\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4746 - val_loss: 0.7800\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4554 - val_loss: 0.7810\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.4336 - val_loss: 0.7802\n",
            "Epoch 22/500\n",
            " - 0s - loss: 0.4118 - val_loss: 0.7929\n",
            "Epoch 23/500\n",
            " - 0s - loss: 0.3967 - val_loss: 0.7933\n",
            "Epoch 00023: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.9178 - val_loss: 1.7493\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6461 - val_loss: 1.6283\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.5150 - val_loss: 1.4645\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3315 - val_loss: 1.2690\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1477 - val_loss: 1.1197\n",
            "Epoch 6/500\n",
            " - 0s - loss: 1.0095 - val_loss: 1.0172\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.9129 - val_loss: 0.9489\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8320 - val_loss: 0.9106\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7687 - val_loss: 0.8810\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7222 - val_loss: 0.8442\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.6772 - val_loss: 0.8502\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6405 - val_loss: 0.8108\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6054 - val_loss: 0.7849\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.5711 - val_loss: 0.7900\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5494 - val_loss: 0.7752\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5221 - val_loss: 0.7722\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.4910 - val_loss: 0.7842\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4697 - val_loss: 0.8478\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4528 - val_loss: 0.7890\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4210 - val_loss: 0.7938\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.4050 - val_loss: 0.7961\n",
            "Epoch 00021: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.9203 - val_loss: 1.7512\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6393 - val_loss: 1.6256\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.5033 - val_loss: 1.4584\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3180 - val_loss: 1.2860\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1459 - val_loss: 1.1461\n",
            "Epoch 6/500\n",
            " - 0s - loss: 1.0232 - val_loss: 1.0470\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.9232 - val_loss: 0.9760\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8406 - val_loss: 0.9361\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.7839 - val_loss: 0.8808\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7295 - val_loss: 0.8603\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.6786 - val_loss: 0.8359\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6414 - val_loss: 0.8430\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6068 - val_loss: 0.7985\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.5763 - val_loss: 0.7991\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5459 - val_loss: 0.7934\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5126 - val_loss: 0.7934\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.4975 - val_loss: 0.7851\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.4674 - val_loss: 0.7754\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.4415 - val_loss: 0.7890\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4242 - val_loss: 0.7952\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.4050 - val_loss: 0.7989\n",
            "Epoch 22/500\n",
            " - 0s - loss: 0.3821 - val_loss: 0.7981\n",
            "Epoch 23/500\n",
            " - 0s - loss: 0.3661 - val_loss: 0.8070\n",
            "Epoch 00023: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/500\n",
            " - 7s - loss: 1.9488 - val_loss: 1.7643\n",
            "Epoch 2/500\n",
            " - 0s - loss: 1.6471 - val_loss: 1.6244\n",
            "Epoch 3/500\n",
            " - 0s - loss: 1.4980 - val_loss: 1.4410\n",
            "Epoch 4/500\n",
            " - 0s - loss: 1.3159 - val_loss: 1.2694\n",
            "Epoch 5/500\n",
            " - 0s - loss: 1.1639 - val_loss: 1.1411\n",
            "Epoch 6/500\n",
            " - 0s - loss: 1.0473 - val_loss: 1.0478\n",
            "Epoch 7/500\n",
            " - 0s - loss: 0.9544 - val_loss: 0.9840\n",
            "Epoch 8/500\n",
            " - 0s - loss: 0.8814 - val_loss: 0.9217\n",
            "Epoch 9/500\n",
            " - 0s - loss: 0.8232 - val_loss: 0.8772\n",
            "Epoch 10/500\n",
            " - 0s - loss: 0.7680 - val_loss: 0.8435\n",
            "Epoch 11/500\n",
            " - 0s - loss: 0.7288 - val_loss: 0.8209\n",
            "Epoch 12/500\n",
            " - 0s - loss: 0.6962 - val_loss: 0.8020\n",
            "Epoch 13/500\n",
            " - 0s - loss: 0.6606 - val_loss: 0.7907\n",
            "Epoch 14/500\n",
            " - 0s - loss: 0.6320 - val_loss: 0.7834\n",
            "Epoch 15/500\n",
            " - 0s - loss: 0.5992 - val_loss: 0.7718\n",
            "Epoch 16/500\n",
            " - 0s - loss: 0.5779 - val_loss: 0.7756\n",
            "Epoch 17/500\n",
            " - 0s - loss: 0.5556 - val_loss: 0.7660\n",
            "Epoch 18/500\n",
            " - 0s - loss: 0.5362 - val_loss: 0.7708\n",
            "Epoch 19/500\n",
            " - 0s - loss: 0.5138 - val_loss: 0.7573\n",
            "Epoch 20/500\n",
            " - 0s - loss: 0.4974 - val_loss: 0.7669\n",
            "Epoch 21/500\n",
            " - 0s - loss: 0.4807 - val_loss: 0.7699\n",
            "Epoch 22/500\n",
            " - 0s - loss: 0.4561 - val_loss: 0.7601\n",
            "Epoch 23/500\n",
            " - 0s - loss: 0.4377 - val_loss: 0.7678\n",
            "Epoch 24/500\n",
            " - 0s - loss: 0.4244 - val_loss: 0.7667\n",
            "Epoch 00024: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SuibQFhfiDTl",
        "colab_type": "code",
        "outputId": "b1aec313-476c-4118-f0b3-c20be2e83ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NN.hdf5')\n",
        "pred_NN = model_classification.predict(x_testNN)\n",
        "\n",
        "pred_NN = np.argmax(pred_NN,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNN,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.6885408551575445\n",
            "Recall score:    0.695046439628483\n",
            "F1 score:        0.6870719550000636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-hdrc4Phm39m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Parameter Tuning for Classification**"
      ]
    },
    {
      "metadata": {
        "id": "yTCLJK_0i7MM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **With Label smoothed y**"
      ]
    },
    {
      "metadata": {
        "id": "SnYu8jvBijSq",
        "colab_type": "code",
        "outputId": "4088116a-c9d3-46f8-9f7b-0371d21fdd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5576
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNNS.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='relu')) # Hidden 2\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 7s - loss: 1.7484 - val_loss: 1.5896\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5086 - val_loss: 1.5219\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.4358 - val_loss: 1.4322\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.3205 - val_loss: 1.2926\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.1575 - val_loss: 1.1127\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.0110 - val_loss: 1.0036\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9028 - val_loss: 0.9161\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8201 - val_loss: 0.8674\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7538 - val_loss: 0.8273\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7028 - val_loss: 0.7819\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6528 - val_loss: 0.7458\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6130 - val_loss: 0.7323\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5730 - val_loss: 0.7409\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5430 - val_loss: 0.7146\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5173 - val_loss: 0.7126\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4869 - val_loss: 0.7036\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4600 - val_loss: 0.6910\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4379 - val_loss: 0.6937\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4218 - val_loss: 0.6981\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4012 - val_loss: 0.7014\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3842 - val_loss: 0.7225\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.3669 - val_loss: 0.7057\n",
            "Epoch 00022: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.7005 - val_loss: 1.5725\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4439 - val_loss: 1.3962\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.2657 - val_loss: 1.2071\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0887 - val_loss: 1.0559\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9587 - val_loss: 0.9444\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.8695 - val_loss: 0.8967\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.7909 - val_loss: 0.8333\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7340 - val_loss: 0.7884\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.6918 - val_loss: 0.7617\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6464 - val_loss: 0.7411\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6114 - val_loss: 0.7287\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.5783 - val_loss: 0.7184\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5556 - val_loss: 0.7163\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5375 - val_loss: 0.7099\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5060 - val_loss: 0.6949\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4794 - val_loss: 0.7010\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4628 - val_loss: 0.6978\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4433 - val_loss: 0.6981\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4263 - val_loss: 0.6892\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4068 - val_loss: 0.6984\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3912 - val_loss: 0.7007\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.3728 - val_loss: 0.7073\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.3591 - val_loss: 0.7073\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.3457 - val_loss: 0.7154\n",
            "Epoch 00024: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.6800 - val_loss: 1.5534\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4589 - val_loss: 1.4616\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3314 - val_loss: 1.2944\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1715 - val_loss: 1.1540\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0299 - val_loss: 1.0245\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9141 - val_loss: 0.9467\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8220 - val_loss: 0.8627\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7481 - val_loss: 0.8229\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.6951 - val_loss: 0.7847\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6529 - val_loss: 0.7606\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6082 - val_loss: 0.7444\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.5802 - val_loss: 0.7750\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5524 - val_loss: 0.7246\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5167 - val_loss: 0.7332\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.4966 - val_loss: 0.7257\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4745 - val_loss: 0.7268\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4459 - val_loss: 0.7187\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4255 - val_loss: 0.7130\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4070 - val_loss: 0.7273\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.3851 - val_loss: 0.7327\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3679 - val_loss: 0.7277\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.3564 - val_loss: 0.7388\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.3375 - val_loss: 0.7373\n",
            "Epoch 00023: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.7455 - val_loss: 1.5618\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4578 - val_loss: 1.4288\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3113 - val_loss: 1.2579\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1445 - val_loss: 1.1157\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0192 - val_loss: 1.0108\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9154 - val_loss: 0.9313\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8357 - val_loss: 0.8773\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7741 - val_loss: 0.8344\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7101 - val_loss: 0.7934\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6662 - val_loss: 0.7713\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6300 - val_loss: 0.7550\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.5925 - val_loss: 0.7410\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5541 - val_loss: 0.7388\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5282 - val_loss: 0.7224\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5026 - val_loss: 0.7149\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4764 - val_loss: 0.7159\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4589 - val_loss: 0.7348\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4364 - val_loss: 0.7182\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4118 - val_loss: 0.7171\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.3896 - val_loss: 0.7263\n",
            "Epoch 00020: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.7884 - val_loss: 1.6018\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4723 - val_loss: 1.4352\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3084 - val_loss: 1.2530\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1289 - val_loss: 1.1055\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9829 - val_loss: 0.9748\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.8750 - val_loss: 0.8863\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.7946 - val_loss: 0.8350\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7295 - val_loss: 0.7949\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.6723 - val_loss: 0.7702\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6316 - val_loss: 0.7427\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.5880 - val_loss: 0.7567\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.5536 - val_loss: 0.7193\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5198 - val_loss: 0.7255\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.4943 - val_loss: 0.7272\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.4725 - val_loss: 0.7009\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4407 - val_loss: 0.7190\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4224 - val_loss: 0.7047\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.3985 - val_loss: 0.7204\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.3818 - val_loss: 0.7115\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.3591 - val_loss: 0.7139\n",
            "Epoch 00020: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.7131 - val_loss: 1.5613\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4514 - val_loss: 1.4469\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3203 - val_loss: 1.2837\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1759 - val_loss: 1.1618\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0472 - val_loss: 1.0458\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9460 - val_loss: 0.9658\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8653 - val_loss: 0.8904\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7907 - val_loss: 0.8397\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7321 - val_loss: 0.8110\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6804 - val_loss: 0.7837\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6384 - val_loss: 0.7542\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6023 - val_loss: 0.7483\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5710 - val_loss: 0.7276\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5396 - val_loss: 0.7178\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5107 - val_loss: 0.7203\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4855 - val_loss: 0.7130\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4603 - val_loss: 0.7167\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4362 - val_loss: 0.7060\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4150 - val_loss: 0.7083\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.3943 - val_loss: 0.7161\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3829 - val_loss: 0.7169\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.3618 - val_loss: 0.7281\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.3395 - val_loss: 0.7293\n",
            "Epoch 00023: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 1.7295 - val_loss: 1.5815\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4684 - val_loss: 1.4384\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3214 - val_loss: 1.2733\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1585 - val_loss: 1.1204\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0167 - val_loss: 1.0020\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9054 - val_loss: 0.9224\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8209 - val_loss: 0.8610\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.7571 - val_loss: 0.8206\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7078 - val_loss: 0.7856\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.6577 - val_loss: 0.7829\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6218 - val_loss: 0.7473\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.5876 - val_loss: 0.7393\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.5540 - val_loss: 0.7367\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5338 - val_loss: 0.7193\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5075 - val_loss: 0.7227\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4825 - val_loss: 0.6980\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4585 - val_loss: 0.7025\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4404 - val_loss: 0.7035\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4194 - val_loss: 0.7023\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.3968 - val_loss: 0.6994\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3820 - val_loss: 0.7046\n",
            "Epoch 00021: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bMJMB7BSi5RT",
        "colab_type": "code",
        "outputId": "197b1f1c-d115-43fd-9860-e8b90544d490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NNS = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NNS = np.argmax(pred_NNS,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NNS, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NNS, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NNS, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.6654608316252482\n",
            "Recall score:    0.6749226006191951\n",
            "F1 score:        0.6686543907438568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "THja7hmBnFd8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **With 4 hidden layers and different # of neurons**\n",
        "\n",
        "Source: http://snap.stanford.edu/class/cs224w-2017/projects/cs224w-33-final.pdf"
      ]
    },
    {
      "metadata": {
        "id": "gxQlWn5DnEqg",
        "colab_type": "code",
        "outputId": "ec11a84f-9858-4346-877d-e80b16aa612c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10642
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NN.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(200, input_dim=x_trainNN.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(40, activation='relu')) # Hidden 2\n",
        "    model_classification.add(Dense(8, activation='relu')) # Hidden 3\n",
        "    model_classification.add(Dense(2, activation='relu')) # Hidden 4\n",
        "    model_classification.add(Dense(y_trainNN.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNN, y_trainNN,validation_data=(x_testNN,y_testNN),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 2.1751 - val_loss: 2.1545\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 2.1310 - val_loss: 2.1158\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 2.0911 - val_loss: 2.0806\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 2.0550 - val_loss: 2.0488\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 2.0222 - val_loss: 2.0201\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.9928 - val_loss: 1.9947\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.9660 - val_loss: 1.9716\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.9420 - val_loss: 1.9508\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.9202 - val_loss: 1.9325\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.9005 - val_loss: 1.9159\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.8830 - val_loss: 1.9010\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.8671 - val_loss: 1.8878\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.8529 - val_loss: 1.8761\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.8400 - val_loss: 1.8655\n",
            "Epoch 15/1000\n",
            " - 1s - loss: 1.8283 - val_loss: 1.8561\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.8179 - val_loss: 1.8476\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.8086 - val_loss: 1.8401\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.8001 - val_loss: 1.8335\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.7924 - val_loss: 1.8274\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.7855 - val_loss: 1.8219\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.7793 - val_loss: 1.8171\n",
            "Epoch 22/1000\n",
            " - 1s - loss: 1.7738 - val_loss: 1.8127\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 1.7687 - val_loss: 1.8090\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 1.7642 - val_loss: 1.8055\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 1.7600 - val_loss: 1.8023\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 1.7562 - val_loss: 1.7994\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 1.7528 - val_loss: 1.7967\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 1.7496 - val_loss: 1.7943\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 1.7467 - val_loss: 1.7923\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 1.7441 - val_loss: 1.7903\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 1.7417 - val_loss: 1.7885\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 1.7396 - val_loss: 1.7868\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 1.7374 - val_loss: 1.7855\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 1.7356 - val_loss: 1.7841\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 1.7339 - val_loss: 1.7828\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 1.7322 - val_loss: 1.7815\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 1.7308 - val_loss: 1.7804\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 1.7294 - val_loss: 1.7794\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 1.7281 - val_loss: 1.7785\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 1.7269 - val_loss: 1.7776\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 1.7258 - val_loss: 1.7769\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 1.7248 - val_loss: 1.7760\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 1.7238 - val_loss: 1.7752\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 1.7229 - val_loss: 1.7746\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 1.7221 - val_loss: 1.7739\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 1.7212 - val_loss: 1.7733\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 1.7204 - val_loss: 1.7726\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 1.7197 - val_loss: 1.7721\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 1.7190 - val_loss: 1.7714\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 1.7184 - val_loss: 1.7710\n",
            "Epoch 51/1000\n",
            " - 1s - loss: 1.7178 - val_loss: 1.7707\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 1.7172 - val_loss: 1.7702\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 1.7166 - val_loss: 1.7698\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 1.7162 - val_loss: 1.7692\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 1.7156 - val_loss: 1.7688\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 1.7152 - val_loss: 1.7684\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 1.7147 - val_loss: 1.7682\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 1.7144 - val_loss: 1.7678\n",
            "Epoch 59/1000\n",
            " - 1s - loss: 1.7139 - val_loss: 1.7672\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 1.7135 - val_loss: 1.7671\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 1.7131 - val_loss: 1.7669\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 1.7129 - val_loss: 1.7665\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 1.7125 - val_loss: 1.7664\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 1.7121 - val_loss: 1.7661\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 1.7119 - val_loss: 1.7657\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 1.7116 - val_loss: 1.7657\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 1.7113 - val_loss: 1.7654\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 1.7111 - val_loss: 1.7652\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 1.7108 - val_loss: 1.7649\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 1.7106 - val_loss: 1.7649\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 1.7104 - val_loss: 1.7648\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 1.7102 - val_loss: 1.7646\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 1.7100 - val_loss: 1.7644\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 1.7098 - val_loss: 1.7644\n",
            "Epoch 00074: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 8s - loss: 2.1758 - val_loss: 2.1559\n",
            "Epoch 2/1000\n",
            " - 1s - loss: 2.1325 - val_loss: 2.1175\n",
            "Epoch 3/1000\n",
            " - 1s - loss: 2.0928 - val_loss: 2.0820\n",
            "Epoch 4/1000\n",
            " - 1s - loss: 2.0567 - val_loss: 2.0505\n",
            "Epoch 5/1000\n",
            " - 1s - loss: 2.0240 - val_loss: 2.0221\n",
            "Epoch 6/1000\n",
            " - 1s - loss: 1.9946 - val_loss: 1.9962\n",
            "Epoch 7/1000\n",
            " - 1s - loss: 1.9679 - val_loss: 1.9732\n",
            "Epoch 8/1000\n",
            " - 1s - loss: 1.9438 - val_loss: 1.9525\n",
            "Epoch 9/1000\n",
            " - 1s - loss: 1.9221 - val_loss: 1.9340\n",
            "Epoch 10/1000\n",
            " - 1s - loss: 1.9023 - val_loss: 1.9177\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.8845 - val_loss: 1.9027\n",
            "Epoch 12/1000\n",
            " - 1s - loss: 1.8685 - val_loss: 1.8893\n",
            "Epoch 13/1000\n",
            " - 1s - loss: 1.8540 - val_loss: 1.8771\n",
            "Epoch 14/1000\n",
            " - 1s - loss: 1.8409 - val_loss: 1.8665\n",
            "Epoch 15/1000\n",
            " - 1s - loss: 1.8293 - val_loss: 1.8569\n",
            "Epoch 16/1000\n",
            " - 1s - loss: 1.8187 - val_loss: 1.8484\n",
            "Epoch 17/1000\n",
            " - 1s - loss: 1.8093 - val_loss: 1.8407\n",
            "Epoch 18/1000\n",
            " - 1s - loss: 1.8008 - val_loss: 1.8340\n",
            "Epoch 19/1000\n",
            " - 1s - loss: 1.7931 - val_loss: 1.8280\n",
            "Epoch 20/1000\n",
            " - 1s - loss: 1.7862 - val_loss: 1.8225\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.7800 - val_loss: 1.8177\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 1.7743 - val_loss: 1.8132\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 1.7692 - val_loss: 1.8092\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 1.7646 - val_loss: 1.8056\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 1.7603 - val_loss: 1.8024\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 1.7566 - val_loss: 1.7995\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 1.7531 - val_loss: 1.7969\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 1.7499 - val_loss: 1.7946\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 1.7470 - val_loss: 1.7924\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 1.7444 - val_loss: 1.7904\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 1.7420 - val_loss: 1.7886\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 1.7397 - val_loss: 1.7870\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 1.7377 - val_loss: 1.7854\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 1.7358 - val_loss: 1.7841\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 1.7340 - val_loss: 1.7828\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 1.7324 - val_loss: 1.7817\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 1.7309 - val_loss: 1.7806\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 1.7296 - val_loss: 1.7796\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 1.7283 - val_loss: 1.7787\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 1.7270 - val_loss: 1.7777\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 1.7259 - val_loss: 1.7769\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 1.7248 - val_loss: 1.7761\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 1.7239 - val_loss: 1.7754\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 1.7230 - val_loss: 1.7747\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 1.7221 - val_loss: 1.7739\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 1.7212 - val_loss: 1.7733\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 1.7205 - val_loss: 1.7727\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 1.7199 - val_loss: 1.7723\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 1.7191 - val_loss: 1.7717\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 1.7184 - val_loss: 1.7710\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 1.7178 - val_loss: 1.7705\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 1.7172 - val_loss: 1.7700\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 1.7167 - val_loss: 1.7696\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 1.7161 - val_loss: 1.7692\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 1.7157 - val_loss: 1.7688\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 1.7152 - val_loss: 1.7684\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 1.7147 - val_loss: 1.7682\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 1.7143 - val_loss: 1.7678\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 1.7139 - val_loss: 1.7675\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 1.7135 - val_loss: 1.7672\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 1.7132 - val_loss: 1.7669\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 1.7128 - val_loss: 1.7666\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 1.7125 - val_loss: 1.7662\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 1.7122 - val_loss: 1.7661\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 1.7119 - val_loss: 1.7658\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 1.7116 - val_loss: 1.7657\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 1.7113 - val_loss: 1.7654\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 1.7112 - val_loss: 1.7650\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 1.7108 - val_loss: 1.7650\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 1.7106 - val_loss: 1.7646\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 1.7104 - val_loss: 1.7646\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 1.7101 - val_loss: 1.7644\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 1.7100 - val_loss: 1.7644\n",
            "Epoch 00073: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 2.1153 - val_loss: 1.9854\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.8842 - val_loss: 1.7879\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.7276 - val_loss: 1.6937\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.6190 - val_loss: 1.6107\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5212 - val_loss: 1.5312\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4246 - val_loss: 1.4719\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.3448 - val_loss: 1.4382\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.2513 - val_loss: 1.3968\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.1135 - val_loss: 1.3024\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.9825 - val_loss: 1.2405\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.9031 - val_loss: 1.3186\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.8517 - val_loss: 1.2423\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7886 - val_loss: 1.2008\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.7384 - val_loss: 1.1364\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6521 - val_loss: 1.0803\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6035 - val_loss: 1.0234\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5212 - val_loss: 1.0286\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4951 - val_loss: 1.0128\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4532 - val_loss: 1.0598\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4172 - val_loss: 1.0346\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.3781 - val_loss: 1.0502\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.3414 - val_loss: 1.1373\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.3277 - val_loss: 1.0943\n",
            "Epoch 00023: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 2.0427 - val_loss: 1.9565\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.8247 - val_loss: 1.7845\n",
            "Epoch 3/1000\n",
            " - 1s - loss: 1.6433 - val_loss: 1.6237\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.4530 - val_loss: 1.4665\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.2683 - val_loss: 1.3065\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.1028 - val_loss: 1.2016\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9739 - val_loss: 1.1393\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8888 - val_loss: 1.1310\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7933 - val_loss: 1.1224\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7291 - val_loss: 1.1005\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6796 - val_loss: 1.0593\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6378 - val_loss: 1.0625\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6004 - val_loss: 1.1299\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5582 - val_loss: 1.1163\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5241 - val_loss: 1.1158\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.4831 - val_loss: 1.1058\n",
            "Epoch 00016: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 2.1750 - val_loss: 2.1546\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 2.1311 - val_loss: 2.1160\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 2.0915 - val_loss: 2.0812\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 2.0557 - val_loss: 2.0497\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 2.0231 - val_loss: 2.0212\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.9933 - val_loss: 1.9953\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.9666 - val_loss: 1.9722\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.9425 - val_loss: 1.9519\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.9208 - val_loss: 1.9333\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.9012 - val_loss: 1.9166\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.8834 - val_loss: 1.9020\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.8675 - val_loss: 1.8885\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.8531 - val_loss: 1.8766\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.8403 - val_loss: 1.8660\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.8288 - val_loss: 1.8564\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.8183 - val_loss: 1.8482\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.8089 - val_loss: 1.8404\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.8004 - val_loss: 1.8339\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.7928 - val_loss: 1.8277\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.7859 - val_loss: 1.8222\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.7797 - val_loss: 1.8174\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 1.7741 - val_loss: 1.8132\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 1.7690 - val_loss: 1.8091\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 1.7644 - val_loss: 1.8058\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 1.7602 - val_loss: 1.8024\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 1.7564 - val_loss: 1.7996\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 1.7530 - val_loss: 1.7971\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 1.7498 - val_loss: 1.7947\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 1.7470 - val_loss: 1.7926\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 1.7443 - val_loss: 1.7907\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 1.7419 - val_loss: 1.7888\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 1.7397 - val_loss: 1.7871\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 1.7377 - val_loss: 1.7858\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 1.7358 - val_loss: 1.7844\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 1.7341 - val_loss: 1.7829\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 1.7324 - val_loss: 1.7817\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 1.7310 - val_loss: 1.7807\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 1.7296 - val_loss: 1.7796\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 1.7283 - val_loss: 1.7787\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 1.7271 - val_loss: 1.7778\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 1.7260 - val_loss: 1.7768\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 1.7249 - val_loss: 1.7760\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 1.7240 - val_loss: 1.7753\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 1.7230 - val_loss: 1.7747\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 1.7221 - val_loss: 1.7740\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 1.7213 - val_loss: 1.7735\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 1.7206 - val_loss: 1.7726\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 1.7198 - val_loss: 1.7722\n",
            "Epoch 49/1000\n",
            " - 1s - loss: 1.7191 - val_loss: 1.7716\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 1.7185 - val_loss: 1.7712\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 1.7178 - val_loss: 1.7706\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 1.7173 - val_loss: 1.7704\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 1.7167 - val_loss: 1.7697\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 1.7162 - val_loss: 1.7693\n",
            "Epoch 55/1000\n",
            " - 1s - loss: 1.7156 - val_loss: 1.7688\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 1.7152 - val_loss: 1.7686\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 1.7147 - val_loss: 1.7682\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 1.7144 - val_loss: 1.7679\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 1.7140 - val_loss: 1.7676\n",
            "Epoch 60/1000\n",
            " - 1s - loss: 1.7135 - val_loss: 1.7672\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 1.7132 - val_loss: 1.7668\n",
            "Epoch 62/1000\n",
            " - 1s - loss: 1.7129 - val_loss: 1.7664\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 1.7125 - val_loss: 1.7663\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 1.7122 - val_loss: 1.7660\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 1.7119 - val_loss: 1.7656\n",
            "Epoch 66/1000\n",
            " - 1s - loss: 1.7116 - val_loss: 1.7654\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 1.7113 - val_loss: 1.7654\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 1.7111 - val_loss: 1.7650\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 1.7109 - val_loss: 1.7650\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 1.7106 - val_loss: 1.7647\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 1.7104 - val_loss: 1.7646\n",
            "Epoch 00071: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 2.1674 - val_loss: 2.1257\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 2.0808 - val_loss: 2.0148\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.9142 - val_loss: 1.8193\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.6980 - val_loss: 1.6118\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5168 - val_loss: 1.4941\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4147 - val_loss: 1.4171\n",
            "Epoch 7/1000\n",
            " - 1s - loss: 1.3209 - val_loss: 1.3761\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.2569 - val_loss: 1.3526\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.1960 - val_loss: 1.3239\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.1312 - val_loss: 1.3308\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.0434 - val_loss: 1.2867\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.8569 - val_loss: 1.1242\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7451 - val_loss: 1.0437\n",
            "Epoch 14/1000\n",
            " - 1s - loss: 0.6394 - val_loss: 1.0754\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5956 - val_loss: 1.0314\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5245 - val_loss: 1.0544\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.4967 - val_loss: 1.0370\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.4151 - val_loss: 1.0762\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.3767 - val_loss: 1.0780\n",
            "Epoch 20/1000\n",
            " - 1s - loss: 0.3264 - val_loss: 1.1382\n",
            "Epoch 00020: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 2.1539 - val_loss: 2.1102\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 2.0224 - val_loss: 2.0072\n",
            "Epoch 3/1000\n",
            " - 1s - loss: 1.9046 - val_loss: 1.9158\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.7995 - val_loss: 1.8362\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.6917 - val_loss: 1.7513\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5897 - val_loss: 1.7196\n",
            "Epoch 7/1000\n",
            " - 1s - loss: 1.4877 - val_loss: 1.6174\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.3795 - val_loss: 1.5399\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.2894 - val_loss: 1.4901\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.2047 - val_loss: 1.4301\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.1309 - val_loss: 1.5619\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.0570 - val_loss: 1.4272\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.0144 - val_loss: 1.5152\n",
            "Epoch 14/1000\n",
            " - 1s - loss: 0.9635 - val_loss: 1.3684\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.9165 - val_loss: 1.3742\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.8836 - val_loss: 1.3218\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.8569 - val_loss: 1.3802\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.8240 - val_loss: 1.3229\n",
            "Epoch 19/1000\n",
            " - 1s - loss: 0.7850 - val_loss: 1.3399\n",
            "Epoch 20/1000\n",
            " - 1s - loss: 0.7357 - val_loss: 1.2695\n",
            "Epoch 21/1000\n",
            " - 1s - loss: 0.7384 - val_loss: 1.2819\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.7110 - val_loss: 1.3315\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.6611 - val_loss: 1.2938\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.6744 - val_loss: 1.2857\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.6358 - val_loss: 1.4301\n",
            "Epoch 00025: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kWtesx9Bjwio",
        "colab_type": "code",
        "outputId": "0c7f4f47-8270-41c5-e340-f86dc7ddb1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NN.hdf5')\n",
        "pred_NN1 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN1 = np.argmax(pred_NN1,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN1, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN1, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN1, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.5387058952622946\n",
            "Recall score:    0.6006191950464397\n",
            "F1 score:        0.5559963686767957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "speBuVynsSua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **RMSPROP** with RELU"
      ]
    },
    {
      "metadata": {
        "id": "6auCrsEUr19N",
        "colab_type": "code",
        "outputId": "df3d6f1f-0366-4a76-c233-35f9cd1fa780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6426
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNNS.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='relu')) # Hidden 2\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.6541 - val_loss: 1.5551\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4584 - val_loss: 1.4553\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3488 - val_loss: 1.3317\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.2252 - val_loss: 1.2099\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.1118 - val_loss: 1.1193\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.0106 - val_loss: 1.0259\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9297 - val_loss: 0.9608\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8556 - val_loss: 0.9278\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.8009 - val_loss: 0.8589\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7514 - val_loss: 0.8311\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7067 - val_loss: 0.7900\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6699 - val_loss: 0.7837\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6403 - val_loss: 0.7483\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6093 - val_loss: 0.7426\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5839 - val_loss: 0.7314\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5576 - val_loss: 0.7123\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5371 - val_loss: 0.7064\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5160 - val_loss: 0.7321\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.5008 - val_loss: 0.7033\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4772 - val_loss: 0.7004\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4638 - val_loss: 0.7296\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4452 - val_loss: 0.7097\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4298 - val_loss: 0.7223\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4148 - val_loss: 0.7061\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.4012 - val_loss: 0.7117\n",
            "Epoch 00025: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.6356 - val_loss: 1.5350\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4478 - val_loss: 1.4340\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3325 - val_loss: 1.3095\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1954 - val_loss: 1.1941\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0751 - val_loss: 1.0616\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9771 - val_loss: 0.9874\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8973 - val_loss: 0.9196\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8292 - val_loss: 0.8586\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7741 - val_loss: 0.8169\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7271 - val_loss: 0.7942\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6861 - val_loss: 0.7801\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6532 - val_loss: 0.7396\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6203 - val_loss: 0.7292\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5945 - val_loss: 0.7082\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5714 - val_loss: 0.8018\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5505 - val_loss: 0.6927\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5266 - val_loss: 0.6873\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5072 - val_loss: 0.6868\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4894 - val_loss: 0.6799\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4753 - val_loss: 0.6895\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4540 - val_loss: 0.7162\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4389 - val_loss: 0.6868\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4269 - val_loss: 0.7013\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4153 - val_loss: 0.6911\n",
            "Epoch 00024: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.7156 - val_loss: 1.5366\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4388 - val_loss: 1.4143\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3049 - val_loss: 1.2814\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1747 - val_loss: 1.1571\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0556 - val_loss: 1.0489\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9612 - val_loss: 0.9838\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8883 - val_loss: 0.9189\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8258 - val_loss: 0.8631\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7740 - val_loss: 0.8268\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7288 - val_loss: 0.8009\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6894 - val_loss: 0.7616\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6617 - val_loss: 0.7689\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6336 - val_loss: 0.7337\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6045 - val_loss: 0.7468\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5841 - val_loss: 0.7120\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5601 - val_loss: 0.6978\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5389 - val_loss: 0.7029\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5195 - val_loss: 0.7346\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.5032 - val_loss: 0.7030\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4872 - val_loss: 0.7010\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4667 - val_loss: 0.6896\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4524 - val_loss: 0.7143\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4400 - val_loss: 0.6965\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4257 - val_loss: 0.7275\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.4130 - val_loss: 0.6975\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.4008 - val_loss: 0.7093\n",
            "Epoch 00026: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.6231 - val_loss: 1.5323\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4257 - val_loss: 1.3972\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.2924 - val_loss: 1.2615\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1556 - val_loss: 1.1414\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0442 - val_loss: 1.0428\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9576 - val_loss: 0.9700\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8820 - val_loss: 0.9172\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8208 - val_loss: 0.8650\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7666 - val_loss: 0.8231\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7203 - val_loss: 0.7970\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.6841 - val_loss: 0.7946\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6506 - val_loss: 0.7548\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6181 - val_loss: 0.7469\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.5934 - val_loss: 0.7379\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5694 - val_loss: 0.7220\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5420 - val_loss: 0.7305\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5295 - val_loss: 0.7066\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5072 - val_loss: 0.7070\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.4886 - val_loss: 0.7060\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4671 - val_loss: 0.6968\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4540 - val_loss: 0.7120\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4382 - val_loss: 0.7280\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4211 - val_loss: 0.7425\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4071 - val_loss: 0.7140\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.3901 - val_loss: 0.7497\n",
            "Epoch 00025: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.6295 - val_loss: 1.5319\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4402 - val_loss: 1.4338\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3271 - val_loss: 1.3179\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.2157 - val_loss: 1.2041\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.1030 - val_loss: 1.1061\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.0078 - val_loss: 1.0227\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9253 - val_loss: 0.9534\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8632 - val_loss: 0.9034\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.8062 - val_loss: 0.8621\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7599 - val_loss: 0.8263\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7172 - val_loss: 0.7926\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6843 - val_loss: 0.7695\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6515 - val_loss: 0.7635\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6228 - val_loss: 0.7398\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5993 - val_loss: 0.7296\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5815 - val_loss: 0.7162\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5564 - val_loss: 0.7178\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5391 - val_loss: 0.7090\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.5206 - val_loss: 0.7029\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.5029 - val_loss: 0.7290\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4888 - val_loss: 0.7080\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4739 - val_loss: 0.6968\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4565 - val_loss: 0.7002\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4417 - val_loss: 0.6969\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.4296 - val_loss: 0.7216\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.4190 - val_loss: 0.7056\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.4045 - val_loss: 0.7103\n",
            "Epoch 00027: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.6553 - val_loss: 1.5449\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4492 - val_loss: 1.4266\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.3252 - val_loss: 1.3029\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.2024 - val_loss: 1.1965\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0965 - val_loss: 1.1018\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.0040 - val_loss: 1.0462\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9257 - val_loss: 0.9509\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8574 - val_loss: 0.8998\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7996 - val_loss: 0.8685\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7530 - val_loss: 0.8199\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7072 - val_loss: 0.7991\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6761 - val_loss: 0.7687\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6418 - val_loss: 0.7646\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6160 - val_loss: 0.7376\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5866 - val_loss: 0.7489\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5646 - val_loss: 0.7581\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5420 - val_loss: 0.7184\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5196 - val_loss: 0.7206\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.5043 - val_loss: 0.7026\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4829 - val_loss: 0.7464\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4670 - val_loss: 0.7124\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4478 - val_loss: 0.7307\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4357 - val_loss: 0.7118\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4168 - val_loss: 0.7206\n",
            "Epoch 00024: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.7018 - val_loss: 1.5503\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.4424 - val_loss: 1.4068\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.2994 - val_loss: 1.2677\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1722 - val_loss: 1.1499\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0649 - val_loss: 1.0492\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9755 - val_loss: 0.9855\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8972 - val_loss: 0.9235\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8346 - val_loss: 0.8711\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7839 - val_loss: 0.8481\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7411 - val_loss: 0.8099\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7011 - val_loss: 0.7701\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.6676 - val_loss: 0.7603\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6371 - val_loss: 0.7349\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6065 - val_loss: 0.7380\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.5862 - val_loss: 0.7608\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.5654 - val_loss: 0.7055\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.5431 - val_loss: 0.6991\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.5205 - val_loss: 0.6926\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.5015 - val_loss: 0.7126\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.4860 - val_loss: 0.7068\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.4701 - val_loss: 0.7191\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.4557 - val_loss: 0.6843\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.4390 - val_loss: 0.6947\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.4298 - val_loss: 0.7066\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.4140 - val_loss: 0.7202\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.4003 - val_loss: 0.7037\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.3867 - val_loss: 0.6948\n",
            "Epoch 00027: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sr_LYRC0sERq",
        "colab_type": "code",
        "outputId": "216e6a0e-ba73-4197-e62c-874e93d04423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NN2 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN2 = np.argmax(pred_NN2,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN2, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN2, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN2, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.6704390928596999\n",
            "Recall score:    0.6857585139318886\n",
            "F1 score:        0.674374103456599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ep7MyfN-sW_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **RMSPROP with Sigmoid**"
      ]
    },
    {
      "metadata": {
        "id": "QubacaoVsXX_",
        "colab_type": "code",
        "outputId": "9bb0f0b0-535c-441f-c127-381bdc3e2de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19380
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNNS.shape[1], activation='sigmoid')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='sigmoid')) # Hidden 2\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.7238 - val_loss: 1.6128\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5449 - val_loss: 1.5830\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5263 - val_loss: 1.5739\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5178 - val_loss: 1.5636\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5075 - val_loss: 1.5574\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4940 - val_loss: 1.5352\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4775 - val_loss: 1.5284\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4576 - val_loss: 1.5018\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4326 - val_loss: 1.4746\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.4057 - val_loss: 1.4476\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3729 - val_loss: 1.4128\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.3366 - val_loss: 1.3745\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2991 - val_loss: 1.3438\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2639 - val_loss: 1.2972\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.2272 - val_loss: 1.2659\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1928 - val_loss: 1.2275\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1562 - val_loss: 1.1986\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.1259 - val_loss: 1.1685\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0952 - val_loss: 1.1312\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0664 - val_loss: 1.1080\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.0377 - val_loss: 1.0851\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 1.0139 - val_loss: 1.0552\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9899 - val_loss: 1.0326\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9681 - val_loss: 1.0127\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9459 - val_loss: 0.9931\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.9274 - val_loss: 0.9772\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.9086 - val_loss: 0.9619\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8903 - val_loss: 0.9478\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8728 - val_loss: 0.9282\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8568 - val_loss: 0.9158\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8407 - val_loss: 0.9036\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.8270 - val_loss: 0.8913\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.8136 - val_loss: 0.8794\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7996 - val_loss: 0.8698\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7877 - val_loss: 0.8610\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7753 - val_loss: 0.8516\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7633 - val_loss: 0.8441\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7522 - val_loss: 0.8337\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7412 - val_loss: 0.8277\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7301 - val_loss: 0.8236\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.7200 - val_loss: 0.8184\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.7101 - val_loss: 0.8030\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.7013 - val_loss: 0.8056\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6926 - val_loss: 0.7900\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6827 - val_loss: 0.7885\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6735 - val_loss: 0.7796\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6642 - val_loss: 0.7740\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6572 - val_loss: 0.7692\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6493 - val_loss: 0.7688\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6413 - val_loss: 0.7646\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6344 - val_loss: 0.7543\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6258 - val_loss: 0.7543\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6172 - val_loss: 0.7477\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.6119 - val_loss: 0.7468\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.6040 - val_loss: 0.7426\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5981 - val_loss: 0.7382\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5910 - val_loss: 0.7359\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5855 - val_loss: 0.7349\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5775 - val_loss: 0.7384\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5724 - val_loss: 0.7297\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5657 - val_loss: 0.7314\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5608 - val_loss: 0.7218\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5563 - val_loss: 0.7244\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5491 - val_loss: 0.7193\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5447 - val_loss: 0.7171\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5384 - val_loss: 0.7129\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5331 - val_loss: 0.7219\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5269 - val_loss: 0.7169\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5231 - val_loss: 0.7089\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5171 - val_loss: 0.7091\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5123 - val_loss: 0.7076\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.5080 - val_loss: 0.7108\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.5028 - val_loss: 0.7070\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4981 - val_loss: 0.7183\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4929 - val_loss: 0.7097\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4880 - val_loss: 0.7051\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.4827 - val_loss: 0.7055\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.4783 - val_loss: 0.7026\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.4750 - val_loss: 0.7034\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 0.4695 - val_loss: 0.7041\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 0.4644 - val_loss: 0.7118\n",
            "Epoch 82/1000\n",
            " - 0s - loss: 0.4617 - val_loss: 0.7058\n",
            "Epoch 83/1000\n",
            " - 0s - loss: 0.4580 - val_loss: 0.7108\n",
            "Epoch 00083: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 9s - loss: 1.9352 - val_loss: 1.6560\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5737 - val_loss: 1.5838\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5321 - val_loss: 1.5764\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5206 - val_loss: 1.5598\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5081 - val_loss: 1.5445\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4933 - val_loss: 1.5275\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4723 - val_loss: 1.5025\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4455 - val_loss: 1.4787\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4150 - val_loss: 1.4386\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3760 - val_loss: 1.3993\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3367 - val_loss: 1.3593\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.2960 - val_loss: 1.3196\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2547 - val_loss: 1.2781\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2168 - val_loss: 1.2486\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.1802 - val_loss: 1.2050\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1449 - val_loss: 1.1688\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1112 - val_loss: 1.1450\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.0812 - val_loss: 1.1141\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0541 - val_loss: 1.0850\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0282 - val_loss: 1.0552\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.0041 - val_loss: 1.0327\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9783 - val_loss: 1.0202\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9596 - val_loss: 0.9958\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9396 - val_loss: 0.9796\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9199 - val_loss: 0.9679\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.9008 - val_loss: 0.9444\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8841 - val_loss: 0.9260\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8683 - val_loss: 0.9136\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8517 - val_loss: 0.8981\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8359 - val_loss: 0.8885\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8238 - val_loss: 0.8754\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.8095 - val_loss: 0.8693\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.7972 - val_loss: 0.8589\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7848 - val_loss: 0.8468\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7725 - val_loss: 0.8373\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7611 - val_loss: 0.8278\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7501 - val_loss: 0.8182\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7397 - val_loss: 0.8097\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7283 - val_loss: 0.8025\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7204 - val_loss: 0.7988\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.7097 - val_loss: 0.7926\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.6996 - val_loss: 0.7912\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6923 - val_loss: 0.7907\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6838 - val_loss: 0.7733\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6753 - val_loss: 0.7678\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6673 - val_loss: 0.7667\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6584 - val_loss: 0.7611\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6506 - val_loss: 0.7548\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6440 - val_loss: 0.7525\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6382 - val_loss: 0.7463\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6297 - val_loss: 0.7424\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6235 - val_loss: 0.7381\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6156 - val_loss: 0.7372\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.6096 - val_loss: 0.7340\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.6020 - val_loss: 0.7300\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5957 - val_loss: 0.7353\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5908 - val_loss: 0.7284\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5840 - val_loss: 0.7208\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5770 - val_loss: 0.7233\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5725 - val_loss: 0.7158\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5657 - val_loss: 0.7124\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5592 - val_loss: 0.7153\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5553 - val_loss: 0.7100\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5498 - val_loss: 0.7076\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5444 - val_loss: 0.7023\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5369 - val_loss: 0.7121\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5336 - val_loss: 0.6996\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5273 - val_loss: 0.7041\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5231 - val_loss: 0.7012\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5181 - val_loss: 0.6977\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5134 - val_loss: 0.6988\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.5079 - val_loss: 0.7035\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.5026 - val_loss: 0.7009\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4982 - val_loss: 0.6913\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4948 - val_loss: 0.6962\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4905 - val_loss: 0.6941\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.4851 - val_loss: 0.6977\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.4825 - val_loss: 0.6936\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.4768 - val_loss: 0.6959\n",
            "Epoch 00079: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.6476 - val_loss: 1.5900\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5321 - val_loss: 1.5829\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5241 - val_loss: 1.5660\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5140 - val_loss: 1.5588\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5017 - val_loss: 1.5386\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4851 - val_loss: 1.5270\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4638 - val_loss: 1.4945\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4357 - val_loss: 1.4682\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4036 - val_loss: 1.4305\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3603 - val_loss: 1.3841\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3167 - val_loss: 1.3459\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.2707 - val_loss: 1.2896\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2265 - val_loss: 1.2455\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.1853 - val_loss: 1.2085\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.1485 - val_loss: 1.1700\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1108 - val_loss: 1.1426\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.0777 - val_loss: 1.1032\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.0461 - val_loss: 1.0727\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0181 - val_loss: 1.0441\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.9940 - val_loss: 1.0305\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.9688 - val_loss: 1.0026\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9472 - val_loss: 0.9839\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9254 - val_loss: 0.9645\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9064 - val_loss: 0.9460\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.8881 - val_loss: 0.9337\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.8707 - val_loss: 0.9198\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8544 - val_loss: 0.9028\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8397 - val_loss: 0.8868\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8260 - val_loss: 0.8850\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8120 - val_loss: 0.8669\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.7990 - val_loss: 0.8551\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.7850 - val_loss: 0.8490\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.7727 - val_loss: 0.8375\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7615 - val_loss: 0.8346\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7518 - val_loss: 0.8224\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7410 - val_loss: 0.8188\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7310 - val_loss: 0.8074\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7191 - val_loss: 0.8024\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7113 - val_loss: 0.7912\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7034 - val_loss: 0.7899\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.6928 - val_loss: 0.7778\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.6845 - val_loss: 0.7765\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6768 - val_loss: 0.7709\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6679 - val_loss: 0.7654\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6590 - val_loss: 0.7574\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6519 - val_loss: 0.7559\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6438 - val_loss: 0.7514\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6377 - val_loss: 0.7513\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6299 - val_loss: 0.7477\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6225 - val_loss: 0.7493\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6156 - val_loss: 0.7384\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6076 - val_loss: 0.7359\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6026 - val_loss: 0.7270\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.5960 - val_loss: 0.7310\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.5921 - val_loss: 0.7259\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5859 - val_loss: 0.7191\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5788 - val_loss: 0.7196\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5729 - val_loss: 0.7143\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5670 - val_loss: 0.7174\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5613 - val_loss: 0.7128\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5560 - val_loss: 0.7099\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5499 - val_loss: 0.7061\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5450 - val_loss: 0.7068\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5389 - val_loss: 0.7047\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5343 - val_loss: 0.7056\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5296 - val_loss: 0.6995\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5235 - val_loss: 0.6981\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5186 - val_loss: 0.6970\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5140 - val_loss: 0.7030\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5091 - val_loss: 0.6971\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5037 - val_loss: 0.7008\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.4994 - val_loss: 0.7001\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.4949 - val_loss: 0.6976\n",
            "Epoch 00073: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.7303 - val_loss: 1.5963\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5421 - val_loss: 1.5820\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5282 - val_loss: 1.5808\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5193 - val_loss: 1.5637\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5085 - val_loss: 1.5518\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4951 - val_loss: 1.5395\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4773 - val_loss: 1.5121\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4557 - val_loss: 1.4892\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4262 - val_loss: 1.4564\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3952 - val_loss: 1.4264\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3592 - val_loss: 1.3841\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.3215 - val_loss: 1.3515\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2828 - val_loss: 1.3166\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2456 - val_loss: 1.2675\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.2075 - val_loss: 1.2327\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1709 - val_loss: 1.1976\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1372 - val_loss: 1.1706\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.1058 - val_loss: 1.1370\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0765 - val_loss: 1.1197\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0506 - val_loss: 1.0819\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.0242 - val_loss: 1.0604\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9998 - val_loss: 1.0383\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9775 - val_loss: 1.0125\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9546 - val_loss: 0.9971\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9357 - val_loss: 0.9717\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.9156 - val_loss: 0.9552\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8979 - val_loss: 0.9448\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8808 - val_loss: 0.9276\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8648 - val_loss: 0.9145\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8479 - val_loss: 0.9029\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8326 - val_loss: 0.8926\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.8189 - val_loss: 0.8808\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.8066 - val_loss: 0.8647\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7940 - val_loss: 0.8626\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7824 - val_loss: 0.8496\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7701 - val_loss: 0.8389\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7565 - val_loss: 0.8323\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7471 - val_loss: 0.8168\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7361 - val_loss: 0.8097\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7246 - val_loss: 0.8052\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.7150 - val_loss: 0.8004\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.7047 - val_loss: 0.7960\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6960 - val_loss: 0.7838\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6859 - val_loss: 0.7801\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6780 - val_loss: 0.7793\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6686 - val_loss: 0.7665\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6614 - val_loss: 0.7608\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6520 - val_loss: 0.7569\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6452 - val_loss: 0.7522\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6367 - val_loss: 0.7469\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6301 - val_loss: 0.7439\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6224 - val_loss: 0.7387\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6135 - val_loss: 0.7543\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.6084 - val_loss: 0.7363\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.6024 - val_loss: 0.7316\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5966 - val_loss: 0.7293\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5893 - val_loss: 0.7254\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5836 - val_loss: 0.7241\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5769 - val_loss: 0.7200\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5716 - val_loss: 0.7248\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5660 - val_loss: 0.7154\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5596 - val_loss: 0.7123\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5528 - val_loss: 0.7134\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5482 - val_loss: 0.7155\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5424 - val_loss: 0.7089\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5370 - val_loss: 0.7085\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5312 - val_loss: 0.7021\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5272 - val_loss: 0.7031\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5223 - val_loss: 0.7007\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5161 - val_loss: 0.7050\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5123 - val_loss: 0.7010\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.5068 - val_loss: 0.6993\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.5008 - val_loss: 0.6975\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4959 - val_loss: 0.6948\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4929 - val_loss: 0.6941\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4857 - val_loss: 0.7207\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.4821 - val_loss: 0.6918\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.4776 - val_loss: 0.6938\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.4740 - val_loss: 0.7052\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 0.4706 - val_loss: 0.6931\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 0.4650 - val_loss: 0.6971\n",
            "Epoch 82/1000\n",
            " - 0s - loss: 0.4621 - val_loss: 0.6956\n",
            "Epoch 00082: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.6605 - val_loss: 1.5957\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5364 - val_loss: 1.5738\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5220 - val_loss: 1.5690\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5108 - val_loss: 1.5461\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.4973 - val_loss: 1.5356\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4804 - val_loss: 1.5201\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4581 - val_loss: 1.4873\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4301 - val_loss: 1.4598\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.3972 - val_loss: 1.4208\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3604 - val_loss: 1.3832\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3201 - val_loss: 1.3514\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.2808 - val_loss: 1.2991\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2379 - val_loss: 1.2585\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2004 - val_loss: 1.2313\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.1663 - val_loss: 1.1985\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1323 - val_loss: 1.1628\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1016 - val_loss: 1.1267\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.0717 - val_loss: 1.0983\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0437 - val_loss: 1.0763\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0173 - val_loss: 1.0477\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.9912 - val_loss: 1.0286\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9687 - val_loss: 1.0080\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9473 - val_loss: 0.9852\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9271 - val_loss: 0.9691\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9079 - val_loss: 0.9560\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.8895 - val_loss: 0.9345\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8728 - val_loss: 0.9211\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8568 - val_loss: 0.9106\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8417 - val_loss: 0.8961\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8259 - val_loss: 0.8824\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8120 - val_loss: 0.8694\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.7998 - val_loss: 0.8606\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.7870 - val_loss: 0.8535\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7756 - val_loss: 0.8413\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7628 - val_loss: 0.8315\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7524 - val_loss: 0.8236\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7401 - val_loss: 0.8197\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7305 - val_loss: 0.8099\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7197 - val_loss: 0.7999\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7092 - val_loss: 0.7962\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.6993 - val_loss: 0.7864\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.6901 - val_loss: 0.7786\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6813 - val_loss: 0.7726\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6727 - val_loss: 0.7725\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6641 - val_loss: 0.7635\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6554 - val_loss: 0.7628\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6482 - val_loss: 0.7531\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6388 - val_loss: 0.7510\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6315 - val_loss: 0.7448\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6228 - val_loss: 0.7400\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6176 - val_loss: 0.7424\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6093 - val_loss: 0.7314\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6022 - val_loss: 0.7289\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.5968 - val_loss: 0.7263\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.5899 - val_loss: 0.7233\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5829 - val_loss: 0.7202\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5773 - val_loss: 0.7243\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5702 - val_loss: 0.7158\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5642 - val_loss: 0.7126\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5584 - val_loss: 0.7081\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5532 - val_loss: 0.7029\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5485 - val_loss: 0.7028\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5409 - val_loss: 0.7104\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5370 - val_loss: 0.6991\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5316 - val_loss: 0.6962\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5251 - val_loss: 0.6996\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5187 - val_loss: 0.7131\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5151 - val_loss: 0.6932\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5094 - val_loss: 0.6933\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5043 - val_loss: 0.6918\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.4987 - val_loss: 0.6926\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.4939 - val_loss: 0.6896\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.4904 - val_loss: 0.6888\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4864 - val_loss: 0.6995\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4792 - val_loss: 0.6971\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4776 - val_loss: 0.6869\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.4674 - val_loss: 0.6934\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.4674 - val_loss: 0.6876\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.4623 - val_loss: 0.6886\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 0.4575 - val_loss: 0.6854\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 0.4538 - val_loss: 0.6898\n",
            "Epoch 82/1000\n",
            " - 0s - loss: 0.4502 - val_loss: 0.6907\n",
            "Epoch 83/1000\n",
            " - 0s - loss: 0.4455 - val_loss: 0.6902\n",
            "Epoch 84/1000\n",
            " - 0s - loss: 0.4416 - val_loss: 0.6869\n",
            "Epoch 85/1000\n",
            " - 0s - loss: 0.4384 - val_loss: 0.6879\n",
            "Epoch 00085: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.6559 - val_loss: 1.5976\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5421 - val_loss: 1.5745\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5247 - val_loss: 1.5626\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5162 - val_loss: 1.5529\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5035 - val_loss: 1.5435\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4860 - val_loss: 1.5218\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4625 - val_loss: 1.4967\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4356 - val_loss: 1.4698\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4051 - val_loss: 1.4323\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3686 - val_loss: 1.3919\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3271 - val_loss: 1.3573\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.2861 - val_loss: 1.3094\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2435 - val_loss: 1.2733\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2044 - val_loss: 1.2294\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.1659 - val_loss: 1.2020\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1314 - val_loss: 1.1658\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1006 - val_loss: 1.1336\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.0691 - val_loss: 1.1012\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0404 - val_loss: 1.0742\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0137 - val_loss: 1.0442\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.9887 - val_loss: 1.0271\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9648 - val_loss: 1.0007\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9415 - val_loss: 0.9825\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9230 - val_loss: 0.9614\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9035 - val_loss: 0.9447\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.8841 - val_loss: 0.9333\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8663 - val_loss: 0.9103\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8494 - val_loss: 0.8960\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8340 - val_loss: 0.8855\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8196 - val_loss: 0.8791\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8055 - val_loss: 0.8645\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.7915 - val_loss: 0.8498\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.7776 - val_loss: 0.8405\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7669 - val_loss: 0.8346\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7559 - val_loss: 0.8238\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7429 - val_loss: 0.8140\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7331 - val_loss: 0.8081\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7238 - val_loss: 0.8065\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7148 - val_loss: 0.7937\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7047 - val_loss: 0.7853\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.6943 - val_loss: 0.7847\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.6861 - val_loss: 0.7753\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6770 - val_loss: 0.7692\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6684 - val_loss: 0.7652\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6597 - val_loss: 0.7580\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6524 - val_loss: 0.7516\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6439 - val_loss: 0.7476\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6360 - val_loss: 0.7468\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6282 - val_loss: 0.7384\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6211 - val_loss: 0.7334\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6142 - val_loss: 0.7299\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6078 - val_loss: 0.7258\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6007 - val_loss: 0.7292\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.5938 - val_loss: 0.7264\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.5875 - val_loss: 0.7258\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5824 - val_loss: 0.7176\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5768 - val_loss: 0.7115\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5698 - val_loss: 0.7137\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5620 - val_loss: 0.7071\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5573 - val_loss: 0.7122\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5530 - val_loss: 0.7035\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5462 - val_loss: 0.7012\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5404 - val_loss: 0.7074\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5360 - val_loss: 0.6994\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5292 - val_loss: 0.7033\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5254 - val_loss: 0.6944\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5186 - val_loss: 0.7026\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5162 - val_loss: 0.6935\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5098 - val_loss: 0.6923\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5052 - val_loss: 0.6976\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.4995 - val_loss: 0.6942\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.4950 - val_loss: 0.7056\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.4917 - val_loss: 0.7020\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4861 - val_loss: 0.6889\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4816 - val_loss: 0.6883\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4762 - val_loss: 0.6877\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.4718 - val_loss: 0.6943\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.4672 - val_loss: 0.6889\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.4654 - val_loss: 0.6879\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 0.4593 - val_loss: 0.6910\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 0.4562 - val_loss: 0.6879\n",
            "Epoch 00081: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.6753 - val_loss: 1.5880\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5363 - val_loss: 1.5724\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5244 - val_loss: 1.5732\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5131 - val_loss: 1.5545\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5015 - val_loss: 1.5394\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.4834 - val_loss: 1.5250\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.4629 - val_loss: 1.4977\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.4391 - val_loss: 1.4700\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.4123 - val_loss: 1.4447\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.3803 - val_loss: 1.4067\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.3468 - val_loss: 1.3750\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.3087 - val_loss: 1.3510\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.2722 - val_loss: 1.3077\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.2342 - val_loss: 1.2618\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.1979 - val_loss: 1.2315\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.1649 - val_loss: 1.1962\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.1303 - val_loss: 1.1734\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.1012 - val_loss: 1.1330\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.0738 - val_loss: 1.1041\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.0455 - val_loss: 1.0834\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.0221 - val_loss: 1.0615\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9988 - val_loss: 1.0396\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9769 - val_loss: 1.0189\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9551 - val_loss: 0.9968\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9354 - val_loss: 0.9809\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.9151 - val_loss: 0.9627\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8973 - val_loss: 0.9528\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8803 - val_loss: 0.9336\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8643 - val_loss: 0.9202\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8478 - val_loss: 0.9085\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8345 - val_loss: 0.8919\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.8189 - val_loss: 0.8784\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.8044 - val_loss: 0.8724\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.7927 - val_loss: 0.8638\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.7796 - val_loss: 0.8500\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7672 - val_loss: 0.8409\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7552 - val_loss: 0.8336\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7443 - val_loss: 0.8286\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7338 - val_loss: 0.8154\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7242 - val_loss: 0.8090\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.7140 - val_loss: 0.8028\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.7044 - val_loss: 0.7953\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.6954 - val_loss: 0.7907\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.6863 - val_loss: 0.7836\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.6772 - val_loss: 0.7743\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.6682 - val_loss: 0.7726\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6606 - val_loss: 0.7645\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6516 - val_loss: 0.7647\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6432 - val_loss: 0.7588\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6356 - val_loss: 0.7527\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6293 - val_loss: 0.7498\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6209 - val_loss: 0.7459\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6152 - val_loss: 0.7388\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.6069 - val_loss: 0.7365\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.6011 - val_loss: 0.7380\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.5924 - val_loss: 0.7417\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.5872 - val_loss: 0.7313\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.5809 - val_loss: 0.7227\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.5745 - val_loss: 0.7279\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.5685 - val_loss: 0.7195\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.5624 - val_loss: 0.7146\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.5572 - val_loss: 0.7115\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.5514 - val_loss: 0.7152\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5453 - val_loss: 0.7087\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5402 - val_loss: 0.7098\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5341 - val_loss: 0.7073\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5302 - val_loss: 0.7040\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5240 - val_loss: 0.7060\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5199 - val_loss: 0.7034\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5152 - val_loss: 0.7022\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5088 - val_loss: 0.7002\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.5037 - val_loss: 0.7012\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.4988 - val_loss: 0.7039\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.4940 - val_loss: 0.7021\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.4897 - val_loss: 0.7024\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.4864 - val_loss: 0.7069\n",
            "Epoch 00076: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwH8Bm_gsXUM",
        "colab_type": "code",
        "outputId": "9f190d7a-b809-47ba-d3c8-f9a7d74bb75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NN3 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN3 = np.argmax(pred_NN3,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN3, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN3, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN3, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.6744224679646678\n",
            "Recall score:    0.6888544891640866\n",
            "F1 score:        0.679048856611632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "B6pNsxOJtdrv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **SGD with Sigmoid**"
      ]
    },
    {
      "metadata": {
        "id": "iEjwpVSltjbC",
        "colab_type": "code",
        "outputId": "cac97921-7ced-4c21-c83b-6cf77f0b03d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5270
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNNS.shape[1], activation='sigmoid')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='sigmoid')) # Hidden 2\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.8172 - val_loss: 1.6889\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6308 - val_loss: 1.6251\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5806 - val_loss: 1.6065\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5623 - val_loss: 1.5988\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5534 - val_loss: 1.5935\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5492 - val_loss: 1.5935\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5465 - val_loss: 1.5922\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5443 - val_loss: 1.5904\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5431 - val_loss: 1.5898\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5417 - val_loss: 1.5889\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5412 - val_loss: 1.5887\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5404 - val_loss: 1.5875\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5396 - val_loss: 1.5866\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5393 - val_loss: 1.5867\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5385 - val_loss: 1.5862\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5381 - val_loss: 1.5866\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5381 - val_loss: 1.5873\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5377 - val_loss: 1.5875\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5870\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5858\n",
            "Epoch 00020: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.8575 - val_loss: 1.7210\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6267 - val_loss: 1.6387\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5802 - val_loss: 1.6165\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5638 - val_loss: 1.6056\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5557 - val_loss: 1.6003\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5508 - val_loss: 1.5966\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5476 - val_loss: 1.5947\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5452 - val_loss: 1.5947\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5433 - val_loss: 1.5937\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5424 - val_loss: 1.5923\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5414 - val_loss: 1.5910\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5406 - val_loss: 1.5897\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5400 - val_loss: 1.5884\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5391 - val_loss: 1.5872\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5391 - val_loss: 1.5870\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5384 - val_loss: 1.5876\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5382 - val_loss: 1.5877\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5378 - val_loss: 1.5875\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5377 - val_loss: 1.5872\n",
            "Epoch 00019: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.8008 - val_loss: 1.6929\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6213 - val_loss: 1.6355\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5801 - val_loss: 1.6164\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5636 - val_loss: 1.6089\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5553 - val_loss: 1.6040\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5501 - val_loss: 1.5988\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5469 - val_loss: 1.5941\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5446 - val_loss: 1.5957\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5430 - val_loss: 1.5927\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5419 - val_loss: 1.5908\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5405 - val_loss: 1.5903\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5396 - val_loss: 1.5896\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5391 - val_loss: 1.5883\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5382 - val_loss: 1.5893\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5380 - val_loss: 1.5872\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5861\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5847\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5367 - val_loss: 1.5868\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5366 - val_loss: 1.5865\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.5363 - val_loss: 1.5863\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.5362 - val_loss: 1.5850\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 1.5360 - val_loss: 1.5855\n",
            "Epoch 00022: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.7519 - val_loss: 1.6813\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6177 - val_loss: 1.6280\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5792 - val_loss: 1.6104\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5632 - val_loss: 1.6026\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5549 - val_loss: 1.5982\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5502 - val_loss: 1.5961\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5469 - val_loss: 1.5937\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5453 - val_loss: 1.5915\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5433 - val_loss: 1.5900\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5424 - val_loss: 1.5898\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5416 - val_loss: 1.5905\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5407 - val_loss: 1.5886\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5398 - val_loss: 1.5882\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5394 - val_loss: 1.5875\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5388 - val_loss: 1.5865\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5389 - val_loss: 1.5860\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5381 - val_loss: 1.5873\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5378 - val_loss: 1.5864\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5376 - val_loss: 1.5880\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5860\n",
            "Epoch 00020: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.6905 - val_loss: 1.6453\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.5858 - val_loss: 1.6088\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5606 - val_loss: 1.5998\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5514 - val_loss: 1.5958\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5469 - val_loss: 1.5928\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5446 - val_loss: 1.5912\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5427 - val_loss: 1.5904\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5416 - val_loss: 1.5901\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5409 - val_loss: 1.5895\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5400 - val_loss: 1.5893\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5389 - val_loss: 1.5873\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5389 - val_loss: 1.5880\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5382 - val_loss: 1.5871\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5378 - val_loss: 1.5880\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5378 - val_loss: 1.5865\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5373 - val_loss: 1.5870\n",
            "Epoch 00016: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.8013 - val_loss: 1.6762\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6008 - val_loss: 1.6232\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5665 - val_loss: 1.6073\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5546 - val_loss: 1.6010\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5484 - val_loss: 1.5964\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5452 - val_loss: 1.5933\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5424 - val_loss: 1.5924\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5410 - val_loss: 1.5912\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5402 - val_loss: 1.5902\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5390 - val_loss: 1.5881\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5385 - val_loss: 1.5888\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5380 - val_loss: 1.5874\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5371 - val_loss: 1.5880\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5369 - val_loss: 1.5853\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5367 - val_loss: 1.5865\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5363 - val_loss: 1.5864\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5361 - val_loss: 1.5863\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5358 - val_loss: 1.5851\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5357 - val_loss: 1.5853\n",
            "Epoch 00019: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.7559 - val_loss: 1.6736\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.6085 - val_loss: 1.6302\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.5763 - val_loss: 1.6150\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.5633 - val_loss: 1.6077\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.5558 - val_loss: 1.6024\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.5512 - val_loss: 1.6006\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.5477 - val_loss: 1.5948\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.5458 - val_loss: 1.5944\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.5439 - val_loss: 1.5919\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.5429 - val_loss: 1.5912\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.5416 - val_loss: 1.5920\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.5411 - val_loss: 1.5902\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.5403 - val_loss: 1.5909\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.5396 - val_loss: 1.5905\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.5390 - val_loss: 1.5870\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.5388 - val_loss: 1.5882\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.5385 - val_loss: 1.5875\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 1.5380 - val_loss: 1.5863\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 1.5375 - val_loss: 1.5854\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 1.5375 - val_loss: 1.5858\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 1.5371 - val_loss: 1.5873\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 1.5369 - val_loss: 1.5855\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 1.5368 - val_loss: 1.5843\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 1.5365 - val_loss: 1.5857\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 1.5365 - val_loss: 1.5872\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 1.5364 - val_loss: 1.5860\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 1.5360 - val_loss: 1.5866\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 1.5360 - val_loss: 1.5852\n",
            "Epoch 00028: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z0zILYnBtkK7",
        "colab_type": "code",
        "outputId": "be1c0e34-ba4c-4de1-f935-06e0700116bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NN4 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN4 = np.argmax(pred_NN4,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN4, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN4, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN4, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.09299667398326449\n",
            "Recall score:    0.30495356037151705\n",
            "F1 score:        0.14252871030412542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xZRqKYiWvd2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **ADAGRAD with RELU**"
      ]
    },
    {
      "metadata": {
        "id": "0ocOAkq5vcco",
        "colab_type": "code",
        "outputId": "b0ddaecb-0c41-4696-ee0a-a8b76f25365e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11118
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(30, input_dim=x_trainNNS.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(45, activation='relu')) # Hidden 2\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.5442 - val_loss: 1.4515\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.3376 - val_loss: 1.2912\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.1881 - val_loss: 1.1804\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0816 - val_loss: 1.0845\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0062 - val_loss: 1.0391\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9505 - val_loss: 0.9844\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9033 - val_loss: 0.9417\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8652 - val_loss: 0.9150\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.8315 - val_loss: 0.8879\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.8018 - val_loss: 0.8746\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7748 - val_loss: 0.8523\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7534 - val_loss: 0.8427\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7313 - val_loss: 0.8215\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.7127 - val_loss: 0.8101\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6939 - val_loss: 0.8028\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6785 - val_loss: 0.7935\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6626 - val_loss: 0.7843\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6473 - val_loss: 0.7756\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6326 - val_loss: 0.7825\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6205 - val_loss: 0.7687\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.6076 - val_loss: 0.7633\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5988 - val_loss: 0.7504\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5877 - val_loss: 0.7467\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5757 - val_loss: 0.7413\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5662 - val_loss: 0.7398\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5565 - val_loss: 0.7336\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5477 - val_loss: 0.7325\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5391 - val_loss: 0.7287\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5310 - val_loss: 0.7250\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5230 - val_loss: 0.7262\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5146 - val_loss: 0.7235\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5075 - val_loss: 0.7201\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.4995 - val_loss: 0.7213\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4937 - val_loss: 0.7200\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4861 - val_loss: 0.7143\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4798 - val_loss: 0.7159\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4736 - val_loss: 0.7138\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4673 - val_loss: 0.7068\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4614 - val_loss: 0.7141\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4557 - val_loss: 0.7159\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4507 - val_loss: 0.7094\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4451 - val_loss: 0.7074\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4386 - val_loss: 0.7055\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4348 - val_loss: 0.7044\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.4291 - val_loss: 0.7096\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.4240 - val_loss: 0.7095\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.4191 - val_loss: 0.7073\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.4148 - val_loss: 0.7062\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.4096 - val_loss: 0.7050\n",
            "Epoch 00049: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.4632 - val_loss: 1.3190\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.2074 - val_loss: 1.1616\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.0820 - val_loss: 1.0697\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0048 - val_loss: 1.0202\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9460 - val_loss: 0.9705\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.8998 - val_loss: 0.9389\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8611 - val_loss: 0.9040\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8287 - val_loss: 0.8767\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7982 - val_loss: 0.8650\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7714 - val_loss: 0.8458\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7470 - val_loss: 0.8205\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7240 - val_loss: 0.8065\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7051 - val_loss: 0.8014\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6876 - val_loss: 0.7841\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6709 - val_loss: 0.7734\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6540 - val_loss: 0.7641\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6419 - val_loss: 0.7545\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6275 - val_loss: 0.7475\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6166 - val_loss: 0.7419\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6051 - val_loss: 0.7344\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.5930 - val_loss: 0.7309\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5838 - val_loss: 0.7258\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5732 - val_loss: 0.7288\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5641 - val_loss: 0.7215\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5553 - val_loss: 0.7158\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5468 - val_loss: 0.7152\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5373 - val_loss: 0.7124\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5314 - val_loss: 0.7052\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5232 - val_loss: 0.7071\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5164 - val_loss: 0.7052\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5110 - val_loss: 0.7004\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5027 - val_loss: 0.6980\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.4968 - val_loss: 0.6970\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4915 - val_loss: 0.6976\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4843 - val_loss: 0.6964\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4794 - val_loss: 0.6945\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4739 - val_loss: 0.6940\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4691 - val_loss: 0.6928\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4626 - val_loss: 0.6973\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4585 - val_loss: 0.6906\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4531 - val_loss: 0.6941\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4484 - val_loss: 0.6901\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4438 - val_loss: 0.6906\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4395 - val_loss: 0.6928\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.4356 - val_loss: 0.6931\n",
            "Epoch 00045: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.5352 - val_loss: 1.4712\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.3657 - val_loss: 1.3424\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.2329 - val_loss: 1.2205\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.1260 - val_loss: 1.1376\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0456 - val_loss: 1.0620\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9758 - val_loss: 1.0075\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.9213 - val_loss: 0.9592\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8730 - val_loss: 0.9333\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.8334 - val_loss: 0.8957\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.8006 - val_loss: 0.8734\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7697 - val_loss: 0.8535\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7449 - val_loss: 0.8381\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7225 - val_loss: 0.8131\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.7038 - val_loss: 0.8003\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6838 - val_loss: 0.7992\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6689 - val_loss: 0.7822\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6525 - val_loss: 0.7719\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6397 - val_loss: 0.7641\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6258 - val_loss: 0.7550\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6137 - val_loss: 0.7529\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.6030 - val_loss: 0.7443\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5928 - val_loss: 0.7379\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5814 - val_loss: 0.7388\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5734 - val_loss: 0.7317\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5636 - val_loss: 0.7280\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5544 - val_loss: 0.7224\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5467 - val_loss: 0.7211\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5380 - val_loss: 0.7238\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5321 - val_loss: 0.7144\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5239 - val_loss: 0.7137\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5177 - val_loss: 0.7138\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5101 - val_loss: 0.7104\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.5039 - val_loss: 0.7160\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4986 - val_loss: 0.7043\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4919 - val_loss: 0.7084\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4851 - val_loss: 0.7089\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4799 - val_loss: 0.7026\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4741 - val_loss: 0.7046\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4693 - val_loss: 0.7006\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4639 - val_loss: 0.7031\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4589 - val_loss: 0.6991\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4547 - val_loss: 0.7004\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4489 - val_loss: 0.7001\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4444 - val_loss: 0.6984\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.4405 - val_loss: 0.7001\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.4352 - val_loss: 0.7039\n",
            "Epoch 00046: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 10s - loss: 1.5040 - val_loss: 1.3881\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.2557 - val_loss: 1.2260\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.1152 - val_loss: 1.1045\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0227 - val_loss: 1.0326\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9542 - val_loss: 0.9879\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9023 - val_loss: 0.9413\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8588 - val_loss: 0.9116\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8230 - val_loss: 0.8828\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7935 - val_loss: 0.8636\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7647 - val_loss: 0.8419\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7423 - val_loss: 0.8291\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7195 - val_loss: 0.8082\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7010 - val_loss: 0.7966\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6834 - val_loss: 0.7889\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6683 - val_loss: 0.7881\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6534 - val_loss: 0.7689\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6398 - val_loss: 0.7599\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6272 - val_loss: 0.7561\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6139 - val_loss: 0.7499\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6037 - val_loss: 0.7415\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.5925 - val_loss: 0.7495\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5829 - val_loss: 0.7347\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5733 - val_loss: 0.7291\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5638 - val_loss: 0.7261\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5550 - val_loss: 0.7214\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5469 - val_loss: 0.7201\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5386 - val_loss: 0.7155\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5312 - val_loss: 0.7170\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5246 - val_loss: 0.7126\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5165 - val_loss: 0.7099\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5091 - val_loss: 0.7081\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5023 - val_loss: 0.7056\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.4950 - val_loss: 0.7096\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4898 - val_loss: 0.7041\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4830 - val_loss: 0.7059\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4771 - val_loss: 0.7000\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4711 - val_loss: 0.7003\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4663 - val_loss: 0.7002\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4602 - val_loss: 0.6994\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4538 - val_loss: 0.6988\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4494 - val_loss: 0.6995\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4437 - val_loss: 0.6963\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4385 - val_loss: 0.6968\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4342 - val_loss: 0.6974\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.4281 - val_loss: 0.6990\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.4227 - val_loss: 0.7027\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.4201 - val_loss: 0.6973\n",
            "Epoch 00047: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 11s - loss: 1.5421 - val_loss: 1.4108\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.2792 - val_loss: 1.2157\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.1263 - val_loss: 1.1060\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0297 - val_loss: 1.0284\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9589 - val_loss: 0.9742\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9047 - val_loss: 0.9312\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8595 - val_loss: 0.8947\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8217 - val_loss: 0.8688\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7874 - val_loss: 0.8418\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7584 - val_loss: 0.8294\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7334 - val_loss: 0.8091\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7134 - val_loss: 0.7920\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6940 - val_loss: 0.7862\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6767 - val_loss: 0.7736\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6606 - val_loss: 0.7581\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6469 - val_loss: 0.7517\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6335 - val_loss: 0.7432\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6194 - val_loss: 0.7378\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6091 - val_loss: 0.7321\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.5987 - val_loss: 0.7252\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.5881 - val_loss: 0.7228\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5784 - val_loss: 0.7161\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5698 - val_loss: 0.7137\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5621 - val_loss: 0.7097\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5517 - val_loss: 0.7082\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5466 - val_loss: 0.7041\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5373 - val_loss: 0.7010\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5315 - val_loss: 0.6980\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5246 - val_loss: 0.6993\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5182 - val_loss: 0.6967\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5112 - val_loss: 0.6928\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5056 - val_loss: 0.6897\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.5000 - val_loss: 0.6913\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4949 - val_loss: 0.6902\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4895 - val_loss: 0.6897\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4833 - val_loss: 0.6867\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4780 - val_loss: 0.6843\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4748 - val_loss: 0.6834\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4688 - val_loss: 0.6833\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4639 - val_loss: 0.6860\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4600 - val_loss: 0.6830\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4558 - val_loss: 0.6809\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4513 - val_loss: 0.6850\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4474 - val_loss: 0.6810\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.4436 - val_loss: 0.6842\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.4399 - val_loss: 0.6861\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.4355 - val_loss: 0.6918\n",
            "Epoch 00047: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 11s - loss: 1.5060 - val_loss: 1.3906\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.2746 - val_loss: 1.2348\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.1301 - val_loss: 1.1136\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0349 - val_loss: 1.0358\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9655 - val_loss: 0.9779\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.9105 - val_loss: 0.9350\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8672 - val_loss: 0.9047\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8299 - val_loss: 0.8821\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7988 - val_loss: 0.8502\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7688 - val_loss: 0.8345\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7439 - val_loss: 0.8140\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7210 - val_loss: 0.8028\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.7026 - val_loss: 0.7894\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6829 - val_loss: 0.7811\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6666 - val_loss: 0.7776\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6510 - val_loss: 0.7606\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6369 - val_loss: 0.7546\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6249 - val_loss: 0.7522\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6122 - val_loss: 0.7437\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6004 - val_loss: 0.7365\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.5894 - val_loss: 0.7340\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5788 - val_loss: 0.7267\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5696 - val_loss: 0.7242\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5604 - val_loss: 0.7217\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5517 - val_loss: 0.7171\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5437 - val_loss: 0.7173\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5342 - val_loss: 0.7125\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5265 - val_loss: 0.7136\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5185 - val_loss: 0.7090\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5119 - val_loss: 0.7062\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5050 - val_loss: 0.7054\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.4983 - val_loss: 0.7018\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.4910 - val_loss: 0.7007\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4860 - val_loss: 0.7003\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4790 - val_loss: 0.7008\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4720 - val_loss: 0.7001\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4669 - val_loss: 0.7000\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4611 - val_loss: 0.7012\n",
            "Epoch 00038: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/1000\n",
            " - 11s - loss: 1.4905 - val_loss: 1.3752\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.2263 - val_loss: 1.1884\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.0853 - val_loss: 1.0808\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 0.9962 - val_loss: 1.0064\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.9292 - val_loss: 0.9667\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.8793 - val_loss: 0.9218\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.8418 - val_loss: 0.9021\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.8080 - val_loss: 0.8663\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.7780 - val_loss: 0.8548\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.7533 - val_loss: 0.8316\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.7328 - val_loss: 0.8157\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.7116 - val_loss: 0.8003\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.6930 - val_loss: 0.7943\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.6770 - val_loss: 0.7834\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.6630 - val_loss: 0.7705\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.6484 - val_loss: 0.7671\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.6354 - val_loss: 0.7541\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.6223 - val_loss: 0.7524\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.6112 - val_loss: 0.7416\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.6007 - val_loss: 0.7355\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.5895 - val_loss: 0.7345\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.5796 - val_loss: 0.7307\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.5725 - val_loss: 0.7254\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.5629 - val_loss: 0.7273\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.5553 - val_loss: 0.7166\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.5466 - val_loss: 0.7134\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.5395 - val_loss: 0.7144\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.5318 - val_loss: 0.7105\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.5238 - val_loss: 0.7079\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.5180 - val_loss: 0.7062\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.5118 - val_loss: 0.7016\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.5048 - val_loss: 0.7020\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.4997 - val_loss: 0.7013\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.4944 - val_loss: 0.6973\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.4868 - val_loss: 0.6936\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.4832 - val_loss: 0.6967\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.4777 - val_loss: 0.6928\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.4732 - val_loss: 0.6943\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.4677 - val_loss: 0.6917\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.4629 - val_loss: 0.6946\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.4579 - val_loss: 0.6936\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.4526 - val_loss: 0.7020\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.4495 - val_loss: 0.6924\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.4446 - val_loss: 0.6915\n",
            "Epoch 00044: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "60R7Gt3NvuC5",
        "colab_type": "code",
        "outputId": "22e8d8ce-0687-407d-ecff-cd9a201f96f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NN5 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN5 = np.argmax(pred_NN5,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN5, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN5, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN5, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.7034636266312974\n",
            "Recall score:    0.6965944272445821\n",
            "F1 score:        0.6893118079803466\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9RsPnLjLwdx9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Hidden layers = 4 **\n",
        "### **RELU and ADAGRAD**"
      ]
    },
    {
      "metadata": {
        "id": "slxhl8XPvr6Q",
        "colab_type": "code",
        "outputId": "48b13b3b-4035-41e3-87f3-7ad34e494530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3264
        }
      },
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5\", verbose=0, save_best_only=True) # save best model\n",
        "\n",
        "for i in range(7):\n",
        "    print(i)\n",
        "    model_classification = Sequential()\n",
        "    model_classification.add(Dense(100, input_dim=x_trainNNS.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
        "    model_classification.add(Dense(200, activation='relu')) # Hidden 2\n",
        "    model_classification.add(Dense(300, activation='relu'))\n",
        "    model_classification.add(Dense(500, activation='relu'))\n",
        "    model_classification.add(Dense(y_trainNNS.shape[1],activation='softmax')) # Output\n",
        "    model_classification.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=7, verbose=1, mode='auto')\n",
        "    model_classification.fit(x_trainNNS, y_trainNNS,validation_data=(x_testNNS,y_testNNS),callbacks=[monitor,checkpointer],verbose=2,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5452 - val_loss: 1.3536\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0532 - val_loss: 0.9096\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7696 - val_loss: 0.8750\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6361 - val_loss: 0.8478\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.5670 - val_loss: 0.7829\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.5070 - val_loss: 0.7607\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4532 - val_loss: 0.9148\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.4329 - val_loss: 0.8790\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3266 - val_loss: 0.8882\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.2739 - val_loss: 0.9739\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2240 - val_loss: 1.0557\n",
            "Epoch 12/100\n",
            " - 1s - loss: 0.2387 - val_loss: 1.0601\n",
            "Epoch 13/100\n",
            " - 1s - loss: 0.1661 - val_loss: 1.2102\n",
            "Epoch 00013: early stopping\n",
            "1\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5201 - val_loss: 1.3176\n",
            "Epoch 2/100\n",
            " - 1s - loss: 0.9994 - val_loss: 0.8694\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.8078 - val_loss: 0.7907\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6484 - val_loss: 0.7658\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.5562 - val_loss: 0.8650\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.5665 - val_loss: 0.7481\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4439 - val_loss: 0.8522\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.4209 - val_loss: 0.8069\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3614 - val_loss: 0.8442\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.3152 - val_loss: 0.9810\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2736 - val_loss: 0.9831\n",
            "Epoch 12/100\n",
            " - 1s - loss: 0.2503 - val_loss: 1.0262\n",
            "Epoch 13/100\n",
            " - 1s - loss: 0.2242 - val_loss: 1.2479\n",
            "Epoch 00013: early stopping\n",
            "2\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5491 - val_loss: 1.3482\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0777 - val_loss: 0.9068\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7754 - val_loss: 0.8730\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6635 - val_loss: 0.9404\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.6061 - val_loss: 0.8024\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.5182 - val_loss: 0.8720\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4755 - val_loss: 0.9552\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.4011 - val_loss: 0.8323\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3432 - val_loss: 0.8817\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.3002 - val_loss: 0.9017\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2357 - val_loss: 1.0312\n",
            "Epoch 12/100\n",
            " - 1s - loss: 0.2276 - val_loss: 0.9748\n",
            "Epoch 00012: early stopping\n",
            "3\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5105 - val_loss: 1.3329\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0056 - val_loss: 0.8654\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7996 - val_loss: 0.8324\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6344 - val_loss: 0.8397\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.5480 - val_loss: 0.8823\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.4976 - val_loss: 0.8082\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4400 - val_loss: 0.9953\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.4614 - val_loss: 0.9284\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3462 - val_loss: 0.8626\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.2934 - val_loss: 0.9479\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2490 - val_loss: 1.0657\n",
            "Epoch 12/100\n",
            " - 1s - loss: 0.2013 - val_loss: 1.1961\n",
            "Epoch 13/100\n",
            " - 1s - loss: 0.1795 - val_loss: 1.1619\n",
            "Epoch 00013: early stopping\n",
            "4\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5553 - val_loss: 1.3966\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0899 - val_loss: 0.9931\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7892 - val_loss: 0.8289\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6458 - val_loss: 0.7651\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.5654 - val_loss: 0.9699\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.5172 - val_loss: 0.7668\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4136 - val_loss: 0.9534\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.3664 - val_loss: 0.9337\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3496 - val_loss: 1.0819\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.3392 - val_loss: 0.9343\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2671 - val_loss: 1.1401\n",
            "Epoch 00011: early stopping\n",
            "5\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5602 - val_loss: 1.3769\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0512 - val_loss: 0.8647\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7361 - val_loss: 0.7983\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6414 - val_loss: 0.9467\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.6061 - val_loss: 0.7212\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.5301 - val_loss: 0.8629\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4441 - val_loss: 0.9458\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.3977 - val_loss: 0.9411\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3654 - val_loss: 0.9050\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.2972 - val_loss: 0.9702\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2400 - val_loss: 1.0195\n",
            "Epoch 12/100\n",
            " - 1s - loss: 0.1928 - val_loss: 1.0437\n",
            "Epoch 00012: early stopping\n",
            "6\n",
            "Train on 1937 samples, validate on 646 samples\n",
            "Epoch 1/100\n",
            " - 12s - loss: 1.5472 - val_loss: 1.3761\n",
            "Epoch 2/100\n",
            " - 1s - loss: 1.0738 - val_loss: 0.9250\n",
            "Epoch 3/100\n",
            " - 1s - loss: 0.7699 - val_loss: 0.7973\n",
            "Epoch 4/100\n",
            " - 1s - loss: 0.6553 - val_loss: 0.7547\n",
            "Epoch 5/100\n",
            " - 1s - loss: 0.5345 - val_loss: 0.8742\n",
            "Epoch 6/100\n",
            " - 1s - loss: 0.4989 - val_loss: 0.8529\n",
            "Epoch 7/100\n",
            " - 1s - loss: 0.4592 - val_loss: 0.8309\n",
            "Epoch 8/100\n",
            " - 1s - loss: 0.3840 - val_loss: 0.9222\n",
            "Epoch 9/100\n",
            " - 1s - loss: 0.3398 - val_loss: 0.9009\n",
            "Epoch 10/100\n",
            " - 1s - loss: 0.3083 - val_loss: 0.9599\n",
            "Epoch 11/100\n",
            " - 1s - loss: 0.2463 - val_loss: 1.0650\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jxMEFwiLweUO",
        "colab_type": "code",
        "outputId": "b4c20042-c332-4016-f185-90bb19e197ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "model_classification.load_weights('/content/drive/My Drive/Colab Notebooks/best_weights_NNSmooth.hdf5')\n",
        "pred_NN6 = model_classification.predict(x_testNNS)\n",
        "\n",
        "pred_NN6 = np.argmax(pred_NN6,axis=1) # raw probabilities to chosen class (highest probability)\n",
        "\n",
        "y_true= np.argmax(y_testNNS,axis=1) \n",
        "\n",
        "pr_score = metrics.precision_score(y_true, pred_NN6, average= \"weighted\")\n",
        "print(\"Precision score: {}\".format(pr_score))\n",
        "\n",
        "re_score = metrics.recall_score(y_true, pred_NN6, average= \"weighted\")\n",
        "print(\"Recall score:    {}\".format(re_score))\n",
        "\n",
        "f1_score = metrics.f1_score(y_true, pred_NN6, average= \"weighted\")\n",
        "print(\"F1 score:        {}\".format(f1_score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.6483510771340206\n",
            "Recall score:    0.6609907120743034\n",
            "F1 score:        0.6523040291928616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y_1z4m6a8sfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Additional Features:\n",
        "\n",
        "Regression: \n",
        "\n",
        "*   L1 & L2 Regularizer\n",
        "*    Dropout Layer\n",
        "*    Parameter Tuning\n",
        "\n",
        "Classification:\n",
        "\n",
        "*   Label Smoothing\n",
        "*   Parameter Tuning\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uNpxoV6ku-oo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **L1 & L2 with Tensorflow  (Additional)**"
      ]
    },
    {
      "metadata": {
        "id": "BNgDDd3k8re4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure, show\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1JVbEU6aMI5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
        "model.add(Dense(25, activation='relu'))\n",
        "\n",
        "model.add(Dense(10, \n",
        "                kernel_regularizer=regularizers.l1(0.01),\n",
        "                activity_regularizer=regularizers.l2(0.01), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eazACbQ3MS3E",
        "colab_type": "code",
        "outputId": "a4a0ffb2-1e6a-46b9-e0fc-2c39c63e4c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3682
        }
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(1))   #  output layer\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=100)\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Final score (RMSE): {}\".format(score))\n",
        "\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "chart_regression(pred2[:50].flatten(),y_test_lin[:50],sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/100\n",
            " - 15s - loss: 28.2269 - val_loss: 11.2671\n",
            "Epoch 2/100\n",
            " - 0s - loss: 8.8127 - val_loss: 7.2732\n",
            "Epoch 3/100\n",
            " - 0s - loss: 6.6952 - val_loss: 6.1355\n",
            "Epoch 4/100\n",
            " - 0s - loss: 5.7610 - val_loss: 5.4260\n",
            "Epoch 5/100\n",
            " - 0s - loss: 5.0973 - val_loss: 4.8711\n",
            "Epoch 6/100\n",
            " - 0s - loss: 4.4946 - val_loss: 4.1852\n",
            "Epoch 7/100\n",
            " - 0s - loss: 3.8761 - val_loss: 3.7152\n",
            "Epoch 8/100\n",
            " - 0s - loss: 3.4599 - val_loss: 3.3977\n",
            "Epoch 9/100\n",
            " - 0s - loss: 3.1370 - val_loss: 3.1058\n",
            "Epoch 10/100\n",
            " - 0s - loss: 2.8896 - val_loss: 2.8649\n",
            "Epoch 11/100\n",
            " - 0s - loss: 2.6560 - val_loss: 2.6692\n",
            "Epoch 12/100\n",
            " - 0s - loss: 2.4785 - val_loss: 2.5120\n",
            "Epoch 13/100\n",
            " - 0s - loss: 2.3174 - val_loss: 2.3796\n",
            "Epoch 14/100\n",
            " - 0s - loss: 2.1827 - val_loss: 2.2524\n",
            "Epoch 15/100\n",
            " - 0s - loss: 2.0537 - val_loss: 2.1378\n",
            "Epoch 16/100\n",
            " - 0s - loss: 1.9461 - val_loss: 2.0508\n",
            "Epoch 17/100\n",
            " - 0s - loss: 1.8477 - val_loss: 1.9642\n",
            "Epoch 18/100\n",
            " - 0s - loss: 1.7599 - val_loss: 1.8859\n",
            "Epoch 19/100\n",
            " - 0s - loss: 1.6825 - val_loss: 1.8053\n",
            "Epoch 20/100\n",
            " - 0s - loss: 1.6053 - val_loss: 1.7331\n",
            "Epoch 21/100\n",
            " - 0s - loss: 1.5338 - val_loss: 1.6858\n",
            "Epoch 22/100\n",
            " - 0s - loss: 1.4684 - val_loss: 1.6222\n",
            "Epoch 23/100\n",
            " - 0s - loss: 1.4137 - val_loss: 1.5748\n",
            "Epoch 24/100\n",
            " - 0s - loss: 1.3508 - val_loss: 1.5164\n",
            "Epoch 25/100\n",
            " - 0s - loss: 1.3013 - val_loss: 1.4686\n",
            "Epoch 26/100\n",
            " - 0s - loss: 1.2524 - val_loss: 1.4162\n",
            "Epoch 27/100\n",
            " - 0s - loss: 1.2031 - val_loss: 1.3907\n",
            "Epoch 28/100\n",
            " - 0s - loss: 1.1636 - val_loss: 1.3385\n",
            "Epoch 29/100\n",
            " - 0s - loss: 1.1170 - val_loss: 1.3223\n",
            "Epoch 30/100\n",
            " - 0s - loss: 1.0779 - val_loss: 1.2649\n",
            "Epoch 31/100\n",
            " - 0s - loss: 1.0405 - val_loss: 1.2443\n",
            "Epoch 32/100\n",
            " - 0s - loss: 1.0022 - val_loss: 1.2087\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.9756 - val_loss: 1.1781\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.9368 - val_loss: 1.1508\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.9063 - val_loss: 1.1293\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.8782 - val_loss: 1.1031\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.8465 - val_loss: 1.0863\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.8204 - val_loss: 1.0591\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.7978 - val_loss: 1.0355\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.7736 - val_loss: 1.0140\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.7502 - val_loss: 1.0033\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.7273 - val_loss: 0.9775\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.7070 - val_loss: 0.9661\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.6897 - val_loss: 0.9548\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.6698 - val_loss: 0.9414\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.6542 - val_loss: 0.9390\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.6340 - val_loss: 0.9059\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.6131 - val_loss: 0.9043\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.6003 - val_loss: 0.8733\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.5778 - val_loss: 0.8720\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.5633 - val_loss: 0.8769\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.5503 - val_loss: 0.8450\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.5317 - val_loss: 0.8305\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.5233 - val_loss: 0.8199\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.5057 - val_loss: 0.8286\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.5004 - val_loss: 0.8128\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.4819 - val_loss: 0.7972\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.4718 - val_loss: 0.8121\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.4593 - val_loss: 0.7917\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.4491 - val_loss: 0.7661\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.4342 - val_loss: 0.7622\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.4233 - val_loss: 0.7508\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.4140 - val_loss: 0.7421\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.4019 - val_loss: 0.7339\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.3943 - val_loss: 0.7468\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.3869 - val_loss: 0.7354\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.3727 - val_loss: 0.7222\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.3643 - val_loss: 0.7180\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.3567 - val_loss: 0.7194\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.3484 - val_loss: 0.7021\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.3452 - val_loss: 0.7097\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.3333 - val_loss: 0.7296\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.3258 - val_loss: 0.6900\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.3174 - val_loss: 0.6922\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.3081 - val_loss: 0.6940\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.3044 - val_loss: 0.6831\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.2998 - val_loss: 0.6656\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.2909 - val_loss: 0.6622\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.2833 - val_loss: 0.6615\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.2764 - val_loss: 0.6617\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.2731 - val_loss: 0.6783\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.2671 - val_loss: 0.6493\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.2607 - val_loss: 0.6569\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.2543 - val_loss: 0.6432\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.2485 - val_loss: 0.6504\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.2425 - val_loss: 0.6504\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.2401 - val_loss: 0.6298\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.2350 - val_loss: 0.6357\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.2307 - val_loss: 0.6364\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.2272 - val_loss: 0.6271\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.2224 - val_loss: 0.6326\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.2172 - val_loss: 0.6274\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.2132 - val_loss: 0.6306\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.2095 - val_loss: 0.6100\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.2050 - val_loss: 0.6187\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.2019 - val_loss: 0.6211\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.2000 - val_loss: 0.6093\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.1951 - val_loss: 0.6107\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.1911 - val_loss: 0.6129\n",
            "Epoch 00099: early stopping\n",
            "Final score (RMSE): 0.6784269589786283\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "cuUAMVDIMa9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Dropout Layer**"
      ]
    },
    {
      "metadata": {
        "id": "HroAyQF0MbFO",
        "colab_type": "code",
        "outputId": "7b22b1c5-bdeb-45a3-a032-84bac05bd253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1472
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1]))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "model.fit(x_train_lin,y_train_lin,validation_data=(x_test_lin,y_test_lin),callbacks=[monitor],verbose=2,epochs=1000)\n",
        "\n",
        "pred = model.predict(x_test_lin)\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test_lin))\n",
        "print(\"Final score (RMSE): {}\".format(score))\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "chart_regression(pred[:50].flatten(),y_test_lin[:50],sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2195 samples, validate on 388 samples\n",
            "Epoch 1/1000\n",
            " - 13s - loss: 3.8549 - val_loss: 0.8357\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 0.7030 - val_loss: 0.4631\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 0.3742 - val_loss: 0.2711\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 0.2394 - val_loss: 0.2243\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 0.1911 - val_loss: 0.1767\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 0.1623 - val_loss: 0.1586\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 0.1390 - val_loss: 0.1464\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 0.1303 - val_loss: 0.1336\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 0.1192 - val_loss: 0.1315\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 0.1151 - val_loss: 0.1229\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 0.1038 - val_loss: 0.1266\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 0.1043 - val_loss: 0.1365\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 0.0973 - val_loss: 0.1124\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 0.0937 - val_loss: 0.1116\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 0.0873 - val_loss: 0.1084\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 0.0864 - val_loss: 0.1082\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 0.0818 - val_loss: 0.1125\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.0780 - val_loss: 0.1059\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.0836 - val_loss: 0.1046\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.0765 - val_loss: 0.1037\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.0764 - val_loss: 0.1035\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.0737 - val_loss: 0.1022\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.0709 - val_loss: 0.1007\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.0727 - val_loss: 0.0994\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.0726 - val_loss: 0.1042\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.0676 - val_loss: 0.1038\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.0666 - val_loss: 0.1045\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.0655 - val_loss: 0.0983\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.0612 - val_loss: 0.0965\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.0593 - val_loss: 0.1035\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.0612 - val_loss: 0.1052\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.0567 - val_loss: 0.0980\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.0639 - val_loss: 0.1025\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.0574 - val_loss: 0.1000\n",
            "Epoch 00034: early stopping\n",
            "Final score (RMSE): 0.3162169855237524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2zynyXVdWjBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}